{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "2192a88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_DIRECTORY = \"../..\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "filepath = os.path.join(WORKING_DIRECTORY, \"data/freud/interpretation-of-dreams.txt\")\n",
    "input_file = open(filepath, 'r', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "7162407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = input_file.read()\n",
    "input_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a56b3546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n !&(),-.0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz·ÀÂÆÉÜàäæçèéêîïóôûüŒœ̓Ψένςυχ–—‘’“”\\ufeff'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_characters = set(raw_text)\n",
    "''.join(sorted(list(raw_characters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "146f819f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = raw_text.replace('\\ufeff', '')\n",
    "import re\n",
    "cleaned_text = re.sub(r'^[ \\t]+|[ \\t]+$', '', cleaned_text, flags=re.MULTILINE)\n",
    "cleaned_text = re.sub(r'\\n{2,}', '\\n\\n', cleaned_text)\n",
    "cleaned_text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', cleaned_text)\n",
    "cleaned_text = re.sub(r' {2,}', ' ', cleaned_text)\n",
    "cleaned_text = re.sub(r'\\n\\n', '\\n', cleaned_text)\n",
    "text = cleaned_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "5fde6f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({' ': 185886, 'e': 112697, 't': 84033, 'a': 66289, 'o': 65011, 'i': 63896, 'n': 61272, 's': 59113, 'h': 53439, 'r': 52082, 'd': 30739, 'l': 30165, 'c': 29084, 'm': 25062, 'f': 22664, 'u': 22638, 'p': 16956, 'w': 15128, 'y': 14864, 'g': 13554, 'b': 11353, ',': 10012, '.': 7341, 'v': 7260, 'k': 3805, 'I': 3725, 'x': 2267, 'T': 2091, '_': 1872, '\\n': 1237, ';': 1076, '“': 989, 'j': 987, '”': 983, 'A': 904, '-': 897, 'q': 826, '—': 676, '(': 646, ')': 646, 'S': 576, 'W': 560, '’': 547, 'B': 537, ':': 516, 'H': 514, 'F': 441, 'O': 432, 'M': 421, '[': 393, ']': 393, 'D': 344, 'E': 294, 'N': 294, 'R': 275, 'P': 269, '1': 249, 'L': 230, 'C': 223, '?': 203, 'G': 188, 'z': 185, '5': 155, 'é': 153, '3': 152, '6': 148, '2': 145, '8': 142, '4': 128, 'U': 115, 'J': 107, 'V': 97, '7': 79, 'K': 78, 'Y': 77, '0': 67, '9': 63, '&': 56, 'ü': 50, '!': 49, 'æ': 32, 'ê': 29, '‘': 26, 'œ': 22, 'Z': 21, 'à': 21, 'ô': 18, 'X': 16, 'Q': 14, 'è': 12, '=': 12, 'ä': 9, 'ç': 9, 'Ψ': 6, '–': 5, 'Ü': 3, '·': 3, 'ï': 2, 'ó': 2, 'î': 2, 'É': 2, 'Æ': 1, 'Â': 1, 'û': 1, 'À': 1, '̓': 1, 'Œ': 1, 'ν': 1, 'έ': 1, 'χ': 1, 'υ': 1, 'ς': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(text))\n",
    "characters = sorted(list(set(text)))\n",
    "vocab_size = len(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "e59b4c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_token = dict(enumerate(characters))\n",
    "token_to_idx = {t: i for i, t in enumerate(characters)}\n",
    "encode = lambda s: list(map(token_to_idx.__getitem__, s))\n",
    "decode = lambda s: list(map(idx_to_token.__getitem__, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e87eb335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "fe4f2eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train / val\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "cade346b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[73, 60, 70, 60, 71,  1, 71, 66],\n",
      "        [69, 58, 52, 65, 70,  8,  1, 31],\n",
      "        [53, 56, 55, 70,  1, 52, 69, 56],\n",
      "        [58,  1, 74, 60, 71, 59,  1, 57]])\n",
      "tensor([[60, 70, 60, 71,  1, 71, 66,  1],\n",
      "        [58, 52, 65, 70,  8,  1, 31, 57],\n",
      "        [56, 55, 70,  1, 52, 69, 56,  1],\n",
      "        [ 1, 74, 60, 71, 59,  1, 57, 60]])\n",
      "torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "block_size = 8 # maximum context length\n",
    "train_data[:block_size+1]\n",
    "batch_size = 4 # how many we want to process in parallel\n",
    "\n",
    "torch.manual_seed(42)\n",
    "def get_batch(split):\n",
    "\t\tdata = train_data if split == 'train' else val_data\n",
    "\t\tidxs = torch.randint(len(data) - block_size, (batch_size,))\n",
    "\t\tx = torch.stack([data[i:i+block_size] for i in idxs])\n",
    "\t\ty = torch.stack([data[i+1:i+block_size+1] for i in idxs])\n",
    "\t\treturn x, y\n",
    "# each block contains block_size examples of increasingly longer strings to provide recurrent training\n",
    "# y is the next single character to predict\n",
    "# block of 'ABCDEFG' -> [('A', 'B'), ('AB', 'C'), ('ABC', 'D')]\n",
    "# batches for GPU parallelization\n",
    "batch_x, batch_y = get_batch('train')\n",
    "print(batch_x)\n",
    "print(batch_y)\n",
    "print(batch_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32efdb93",
   "metadata": {},
   "source": [
    "# BIGRAM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "c32845f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 112])\n",
      "tensor(5.1489, grad_fn=<NllLossBackward0>)\n",
      "['\\n', 'W', 'œ', 'Ü', '8', 'R', 'î', 'S', '8', '“', 't', 'h', 'Y', 'û', 'Œ', 'A', 'ï', 'Â', 'D', 'χ', 'Æ', ')', 'Ψ', '–', 'T', 'I', 'h', 'U', 'ô', 'e', 'è', 'X', 'Æ', 'k', ' ', '—', 'w', '!', '“', '_', 'H', 'ï', 'c', 'I', 'b', 'û', 'ü', '“', '_', '5', 'S', '6', '5', 'h', '9', '6', 'v', 'j', 'ç', 'Y', '‘', 'Ψ', 'h', 'j', 'έ', 'î', '.', '!', '&', ',', 'C', '.', 'D', 'χ', '\\n', 'y', 'T', 'A', 'Œ', '7', '1', 'D', 'ô', ',', 'H', 'w', ' ', 'Ψ', 'P', 'r', 't', 'b', 'ó', 'w', 'o', 'u', 'C', '5', 'R', 'R', 'Â']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# note: both x and y are of shape (batch size, block size)\n",
    "# B: batch size\n",
    "# T: block size\n",
    "# V: vocab size (channel size)\n",
    "\n",
    "# B -> batch\n",
    "# T -> time dimension\n",
    "# C -> channel\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\tdef __init__(self, vocab_size):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# nn.Embedding works as a lookup table from token -> token\n",
    "\t\tself.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\tdef forward(self, idx, targets=None):\n",
    "\t\t# looks up each idx in the embedding table\n",
    "\t\t# since input idx (x) is of size (B,T), returns (B,T,V) tensor\n",
    "\t\t# (since each word corresponds with a vector size vocab_size)\n",
    "\t\tlogits = self.token_embedding_table(idx)\n",
    "\n",
    "\t\tif targets is None:\n",
    "\t\t\treturn logits, None\n",
    "\n",
    "\t\t# use negative log likelihood (cross entropy)\n",
    "\t\t# F.cross_entropy expects logits to be 2D and targets 1D, so collapse dimensions\n",
    "\t\tB, T, C = logits.shape\n",
    "\t\tlogits_shrink = logits.view(B*T, C)\n",
    "\t\ttargets_shrink = targets.view(B*T)\n",
    "\t\tloss = F.cross_entropy(logits_shrink, targets_shrink)\n",
    "\n",
    "\t\treturn logits_shrink, loss\n",
    "\t\n",
    "\tdef generate(self, idx, max_new_tokens):\n",
    "\t\tfor _ in range(max_new_tokens):\n",
    "\t\t\tlogits, _ = self(idx)\n",
    "\t\t\tlogits = logits[:, -1, :] # specifically only get the LAST timestep\n",
    "\t\t\tprobs = F.softmax(logits, dim = -1) # over the channel dimension, size (B, 1)\n",
    "\t\t\tidx_next = torch.multinomial(probs, num_samples = 1) # sample, size (B, 1)\n",
    "\t\t\tidx = torch.cat((idx, idx_next), dim=1) # append to given indices (B, T + 1)\n",
    "\t\treturn idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(batch_x, batch_y)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "print(decode(\n",
    "\tm.generate(\n",
    "\t\ttorch.zeros((1, 1), dtype=torch.long),\n",
    "\t\t100\n",
    "\t)[0].tolist()\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "42aa7a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "4ee3647b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.037664890289307\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(1000):\n",
    "\txb, yb = get_batch('train')\n",
    "\tlogits, loss = m(xb, yb)\n",
    "\toptimizer.zero_grad(set_to_none=True)\n",
    "\tloss.backward()\n",
    "\toptimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "cee9ab0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "hdF”ô2d=Lç07;stqν——PûLBRHsnÜ8dY_νD:0ûÉ–m6y qÀ—4B(YNêlàςυ·éqoôô2küXEov[fVun’0”BoûN3kak)έ7LiΨ–?MäkέóÂô_45 w89x4—k3êΨ7A‘ s-vvéXiîa3·nυNÆqNWOÜ!wàMÂM.!.ijïcæ?vQMέ\n",
      "p. akFdWg—YP3χ[xB7ûŒ-whvôîæÆuςBl19î0ïURΨFêΨυ(_&äRΨJCPh,_SXÉ;t0ŒH0Ü!OvkæD7eqn(4M4‘χrk3;\n",
      "ê12TcuOnp\n",
      "OÉX–u—?,CCXTνûÉ“IvSaυ_Â&Mg0LXæ—wc3D.v&P7Lubïc\n",
      "9,QNéW0CtXχ6v)f:JeyDPêχu—R3UÆΨυJea3feSüzûT8g-siC—wiv“iïUP:hdCEHûN’bw—9RïÀ7LFGc‘ tυp-oυRyÉGQOYoυRazDa—&,. P[ZYY612VC_æ=;lFAiî5?M62“SpUx6lun a99,8_üCKΨυ‘Æ3.υ·PN\n",
      "5à?SuZadÆÂ?cu·ô)χ̓ÉP):ükQ&,ÜJMxVXccÂUcee\n"
     ]
    }
   ],
   "source": [
    "print(\"\".join(decode(\n",
    "\tm.generate(\n",
    "\t\ttorch.zeros((1, 1), dtype=torch.long),\n",
    "\t\t500\n",
    "\t)[0].tolist()\n",
    ")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19b09b2",
   "metadata": {},
   "source": [
    "# Attention testing\n",
    "note that to couple tokens with previous tokens in the context, it may be valuable to use the 'average' of the previous tokens, named a 'bag of words'\n",
    "\n",
    "the trick to doing this extremely fast (without having to loop over B and T and averaging over time) is to use matrix multiplication\n",
    "\n",
    "multiplying by a ones matrix returns the sum of each row / column\n",
    "\n",
    "=> use a lower triangular matrix of ones to find these values\n",
    "\n",
    "```python\n",
    "weights = torch.tril(torch.ones(T, T))\n",
    "weights = weights / weights.sum(1, keepdim=True)\n",
    "xbow2 = weights @ x # note this is (T, T) x (B, T, C), but torch will cast weights into (B, T, T) resulting in (B, T, C)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "0f9e8564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 112])\n"
     ]
    }
   ],
   "source": [
    "n_embed = 32\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# this one doesn't work i think but you can see what we're doing here, adding a linear layer inbetween\n",
    "\n",
    "class BigramLanguageModelWithPositionalEncoding(nn.Module):\n",
    "\tdef __init__(self, vocab_size=vocab_size, n_embed=n_embed, block_size=block_size):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "\t\tself.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "\t\tself.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "\tdef forward(self, idx, targets=None):\n",
    "\t\ttok_embed = self.token_embedding_table(idx)\n",
    "\t\t_, T, _ = tok_embed.shape\n",
    "\t\tpos_embed = self.position_embedding_table(torch.arange(T))\n",
    "\t\tx = tok_embed + pos_embed\n",
    "\t\tlogits = self.lm_head(x)\n",
    "\n",
    "\t\tif targets is None:\n",
    "\t\t\treturn logits, None\n",
    "\t\tB, T, C = logits.shape\n",
    "\t\tlogits_shrink = logits.view(B*T, C)\n",
    "\t\ttargets_shrink = targets.view(B*T)\n",
    "\t\tloss = F.cross_entropy(logits_shrink, targets_shrink)\n",
    "\t\treturn logits_shrink, loss\n",
    "\t\n",
    "\tdef generate(self, idx, max_new_tokens):\n",
    "\t\tfor _ in range(max_new_tokens):\n",
    "\t\t\tlogits, _ = self(idx)\n",
    "\t\t\tlogits = logits[:, -1, :] # specifically only get the LAST timestep\n",
    "\t\t\tprobs = F.softmax(logits, dim = -1) # over the channel dimension, size (B, 1)\n",
    "\t\t\tidx_next = torch.multinomial(probs, num_samples = 1) # sample, size (B, 1)\n",
    "\t\t\tidx = torch.cat((idx, idx_next), dim=1) # append to given indices (B, T + 1)\n",
    "\t\treturn idx\n",
    "\t\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(batch_x, batch_y)\n",
    "print(logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "a59b2cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# head test\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "head_size=16\n",
    "key=nn.Linear(C, head_size, bias=False)\n",
    "query=nn.Linear(C, head_size, bias=False)\n",
    "value=nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "weights = q @ k.transpose(-2, -1) # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# weights = torch.zeros((T, T))\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = weights @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "dd910246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2239, 0.7761, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.8172, 0.1650, 0.0179, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0411, 0.9134, 0.0296, 0.0159, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0024, 0.0866, 0.6325, 0.0204, 0.2582, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4754, 0.2132, 0.0254, 0.2568, 0.0079, 0.0212, 0.0000, 0.0000],\n",
       "        [0.1016, 0.3475, 0.0438, 0.3245, 0.0951, 0.0521, 0.0354, 0.0000],\n",
       "        [0.4936, 0.1528, 0.0470, 0.0935, 0.0416, 0.0253, 0.1003, 0.0459]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b6b882",
   "metadata": {},
   "source": [
    "notes:\n",
    "\n",
    "q \"what am i looking for?\"\n",
    "k \"what can i offer?\"\n",
    "v \"what i actually offer during attention\"\n",
    "\n",
    "attention is the KVQ mechanism here. attention is a communication mechanism and works in directed graphs. technically it doesn't have to be a DAG and can work with any directed graph. the graphs we use in this model are where each token points to all tokens after it in the batch, and the last token points to itself.\n",
    "\n",
    "note that also in attention there isn't a notion of space here, it just works on a group of vectors. we have to add the positional encoding for the model to care about it\n",
    "\n",
    "each example in a batch does not care about anything about the other batches; completely independent\n",
    "\n",
    "in an encoder it does not matter that tokens can communicate with tokens later in a sequence (meaning no masking above). sequences where that does matter is called a 'decoder', like so, and is commonly used in similarly autoregressive settings. attention does not matter\n",
    "(auto-regressive: statistical models that predicts future values in a time series using previous values)\n",
    "\n",
    "\"self-attention\" -> the keys and values are produced from the same source as the queries. \n",
    "\n",
    "\"cross-attention\" -> keys and values are produced from a difference source as the queries. seems to be used when we want to condition off of some other context or use a different modality. eg. for translation from languages A to B, k/v pairs are generated for tokens in A, then are queried using query vectors from B.\n",
    "\n",
    "in the original \"attention is all you need\" paper, before softmaxing the weights are multiplied by $\\frac{1}{\\sqrt{\\texttt{head\\_size}}}$. this is because when tensor multiplying k and v, the variance of the output function gets amplified tremendously. and since softmax favors the largest values, with greater variance this will increase the disparity between the largest value and the rest of the values, essentially creating one-hot vectors; which we do not want since we want to maximize the usage of our embedding dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b67c49b",
   "metadata": {},
   "source": [
    "# Inserting a head into our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "cc770aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\tdef __init__(self, head_size=head_size, n_embed=n_embed, block_size=block_size):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "\t\tself.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "\t\tself.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "\t\tself.register_buffer('tril', \n",
    "\t\t\ttorch.tril(torch.ones(block_size, block_size))\n",
    "\t\t)\n",
    "\t\tself.attention_scalar = pow(head_size, -0.5)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\t_, T, _ = x.shape\n",
    "\t\tk = self.key(x)\n",
    "\t\tv = self.value(x)\n",
    "\t\tq = self.query(x)\n",
    "\n",
    "\t\t# doing attn. formula -> softmax(qK^T / sqrt(d_k)) * V\n",
    "\t\tweights = q @ k.transpose(-2, -1) * self.attention_scalar\n",
    "\t\tweights = F.softmax(\n",
    "\t\t\tweights.masked_fill(self.tril[:T, :T] == 0, float('-inf')),\n",
    "\t\t\tdim=-1\n",
    "\t\t)\n",
    "\t\tout = weights @ v\n",
    "\t\treturn out\n",
    "\n",
    "class SingleHeadAttentionLanguageModel(nn.Module):\n",
    "\tdef __init__(self, vocab_size=vocab_size, n_embed=n_embed, block_size=block_size, head_size=head_size):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "\t\tself.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "\t\tself.sa_head = Head(head_size=n_embed, n_embed=n_embed, block_size=block_size)\n",
    "\t\tself.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "\tdef forward(self, idx, targets=None):\n",
    "\t\tB, T = idx.shape\n",
    "\t\t\n",
    "\t\ttok_embed = self.token_embedding_table(idx)\n",
    "\t\tpos_embed = self.position_embedding_table(torch.arange(T, device=device))\n",
    "\t\tx = self.sa_head(tok_embed + pos_embed)\n",
    "\t\tlogits = self.lm_head(x)\n",
    "\n",
    "\t\tif targets is None:\n",
    "\t\t\treturn logits, None\n",
    "\t\t\n",
    "\t\tB, T, C = logits.shape\n",
    "\t\tlogits_shrink = logits.view(B*T, C)\n",
    "\t\ttargets_shrink = targets.view(B*T)\n",
    "\t\tloss = F.cross_entropy(logits_shrink, targets_shrink)\n",
    "\t\treturn logits_shrink, loss\n",
    "\t\n",
    "\tdef generate(self, idx, max_new_tokens):\n",
    "\t\tfor _ in range(max_new_tokens):\n",
    "\t\t\tidx_cond = idx[:, -block_size:] # we can only use context length of block_size otherwise positional embeddings will fail\n",
    "\t\t\tlogits, _ = self(idx_cond)\n",
    "\t\t\tlogits = logits[:, -1, :]\n",
    "\t\t\tprobs = F.softmax(logits, dim = -1)\n",
    "\t\t\tidx_next = torch.multinomial(probs, num_samples = 1)\n",
    "\t\t\tidx = torch.cat((idx, idx_next), dim=1)\n",
    "\t\treturn idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "72e3a378",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = SingleHeadAttentionLanguageModel().to(device)\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "17d8581a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8626787662506104\n"
     ]
    }
   ],
   "source": [
    "for steps in range(100):\n",
    "\txb, yb = get_batch('train')\n",
    "\txb = xb.to(device)\n",
    "\tyb = yb.to(device)\n",
    "\tlogits, loss = m(xb, yb)\n",
    "\toptimizer.zero_grad(set_to_none=True)\n",
    "\tloss.backward()\n",
    "\toptimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "819ab013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-e lw n e‘ypchdpPgdyh shrsn n n vd ur gberuhrrnddif.cttsoo  uoexanna hae2hsos 6mheoaeceseaerd9 ncf rbrsnhibenyntl ynispmltiabnptt i msaIyto or]rchao iwoilslorse_d n nrshramil kyaisi  ss tooiJnwh 6miNttnaettœm f idoholoo  tft oece ierr  yse.m eahruasth yi tihyerouv daerfäiowwuu gfemoon nf aoffb eytaosatevntttd dya nal i iwatlyeIrh maenehn  tnpti hέtgiiteνoo6ta asef ain reOoswtan nnhi=e toot ha tine’pn s rtf, co B‘2g rw n.eaerie, e t w ioahfsnn iy tumidiarhiioa e7foeaarrbtraa t dg tisn  iotm=eeke \n"
     ]
    }
   ],
   "source": [
    "print(\"\".join(decode(\n",
    "\tm.generate(\n",
    "\t\ttorch.zeros((1, 1), dtype=torch.long, device=device),\n",
    "\t\t500\n",
    "\t)[0].tolist()\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec256988",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\tdef __init__(self, num_heads, head_size):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "\tdef forward(self, x):\n",
    "\t\treturn torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\tdef __init__(self, n_embed):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.net = nn.Sequential(\n",
    "\t\t\tnn.Linear(n_embed, n_embed),\n",
    "\t\t\tnn.ReLU()\n",
    "\t\t)\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.net(x)\n",
    "\n",
    "class MultiHeadAttentionLanguageModel(nn.Module):\n",
    "\tdef __init__(self, vocab_size=vocab_size, n_embed=n_embed, block_size=block_size, head_size=head_size):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "\t\tself.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "\t\tself.sa_heads = MultiHeadAttention(num_heads=4, head_size=n_embed//4)\n",
    "\t\tself.ffwd = FeedForward(n_embed)\n",
    "\t\tself.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "\tdef forward(self, idx, targets=None):\n",
    "\t\tB, T = idx.shape\n",
    "\t\t\n",
    "\t\ttok_embed = self.token_embedding_table(idx)\n",
    "\t\tpos_embed = self.position_embedding_table(torch.arange(T, device=device))\n",
    "\t\tx = self.sa_heads(tok_embed + pos_embed)\n",
    "\t\tx = self.ffwd(x)\n",
    "\t\tlogits = self.lm_head(x)\n",
    "\n",
    "\t\tif targets is None:\n",
    "\t\t\treturn logits, None\n",
    "\t\t\n",
    "\t\tB, T, C = logits.shape\n",
    "\t\tlogits_shrink = logits.view(B*T, C)\n",
    "\t\ttargets_shrink = targets.view(B*T)\n",
    "\t\tloss = F.cross_entropy(logits_shrink, targets_shrink)\n",
    "\t\treturn logits_shrink, loss\n",
    "\t\n",
    "\tdef generate(self, idx, max_new_tokens):\n",
    "\t\tfor _ in range(max_new_tokens):\n",
    "\t\t\tidx_cond = idx[:, -block_size:] # we can only use context length of block_size otherwise positional embeddings will fail\n",
    "\t\t\tlogits, _ = self(idx_cond)\n",
    "\t\t\tlogits = logits[:, -1, :]\n",
    "\t\t\tprobs = F.softmax(logits, dim = -1)\n",
    "\t\t\tidx_next = torch.multinomial(probs, num_samples = 1)\n",
    "\t\t\tidx = torch.cat((idx, idx_next), dim=1)\n",
    "\t\treturn idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "8302bf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MultiHeadAttentionLanguageModel().to(device)\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "b4a057d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1136281490325928\n"
     ]
    }
   ],
   "source": [
    "for steps in range(1000):\n",
    "\txb, yb = get_batch('train')\n",
    "\txb = xb.to(device)\n",
    "\tyb = yb.to(device)\n",
    "\tlogits, loss = m(xb, yb)\n",
    "\toptimizer.zero_grad(set_to_none=True)\n",
    "\tloss.backward()\n",
    "\toptimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "c977c0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "46Gf”ran at moil amctiwalt Faly sinengle the bare, is feree prof heres_to noes. paor wis fe worek am\n"
     ]
    }
   ],
   "source": [
    "print(\"\".join(decode(\n",
    "\tm.generate(\n",
    "\t\ttorch.zeros((1, 1), dtype=torch.long, device=device),\n",
    "\t\t100\n",
    "\t)[0].tolist()\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "d4c87026",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\tdef __init__(self, num_heads, head_size):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "\t\tself.proj = nn.Linear(n_embed, n_embed)\n",
    "\tdef forward(self, x):\n",
    "\t\tout = torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "\t\tout = self.proj(out)\n",
    "\t\treturn out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\tdef __init__(self, n_embed):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.net = nn.Sequential(\n",
    "\t\t\tnn.Linear(n_embed, 4 * n_embed),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(4 * n_embed, n_embed),\n",
    "\t\t)\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "\tdef __init__(self, n_embed, n_head):\n",
    "\t\tsuper().__init__()\n",
    "\t\thead_size = n_embed // n_head\n",
    "\t\tself.sa = MultiHeadAttention(n_head, head_size)\n",
    "\t\tself.ffwd = FeedForward(n_embed)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = x + self.sa(x)\n",
    "\t\tx = x + self.ffwd(x)\n",
    "\t\treturn x\n",
    "\n",
    "# add blocks + residual connections\n",
    "class AttentionBlockLanguageModel(nn.Module):\n",
    "\tdef __init__(self, vocab_size=vocab_size, n_embed=n_embed, block_size=block_size, head_size=head_size):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "\t\tself.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "\t\tself.blocks = nn.Sequential(\n",
    "\t\t\tBlock(n_embed, n_head=4),\n",
    "\t\t\tBlock(n_embed, n_head=4),\n",
    "\t\t\tBlock(n_embed, n_head=4),\n",
    "\t\t)\n",
    "\t\tself.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "\tdef forward(self, idx, targets=None):\n",
    "\t\tB, T = idx.shape\n",
    "\t\t\n",
    "\t\ttok_embed = self.token_embedding_table(idx)\n",
    "\t\tpos_embed = self.position_embedding_table(torch.arange(T, device=device))\n",
    "\t\tx = self.blocks(tok_embed + pos_embed)\n",
    "\t\tlogits = self.lm_head(x)\n",
    "\n",
    "\t\tif targets is None:\n",
    "\t\t\treturn logits, None\n",
    "\t\t\n",
    "\t\tB, T, C = logits.shape\n",
    "\t\tlogits_shrink = logits.view(B*T, C)\n",
    "\t\ttargets_shrink = targets.view(B*T)\n",
    "\t\tloss = F.cross_entropy(logits_shrink, targets_shrink)\n",
    "\t\treturn logits_shrink, loss\n",
    "\t\n",
    "\tdef generate(self, idx, max_new_tokens):\n",
    "\t\tfor _ in range(max_new_tokens):\n",
    "\t\t\tidx_cond = idx[:, -block_size:] # we can only use context length of block_size otherwise positional embeddings will fail\n",
    "\t\t\tlogits, _ = self(idx_cond)\n",
    "\t\t\tlogits = logits[:, -1, :]\n",
    "\t\t\tprobs = F.softmax(logits, dim = -1)\n",
    "\t\t\tidx_next = torch.multinomial(probs, num_samples = 1)\n",
    "\t\t\tidx = torch.cat((idx, idx_next), dim=1)\n",
    "\t\treturn idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "67dd3efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MultiHeadAttentionLanguageModel().to(device)\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "cfc6b8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9682185649871826\n"
     ]
    }
   ],
   "source": [
    "for steps in range(1000):\n",
    "\txb, yb = get_batch('train')\n",
    "\txb = xb.to(device)\n",
    "\tyb = yb.to(device)\n",
    "\tlogits, loss = m(xb, yb)\n",
    "\toptimizer.zero_grad(set_to_none=True)\n",
    "\tloss.backward()\n",
    "\toptimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "7e6351b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ges t em 9 Éommeemfteere,r en tsof a iinr dudictlite tenfa edm earmre wes am in id of oa simûa yrute\n"
     ]
    }
   ],
   "source": [
    "print(\"\".join(decode(\n",
    "\tm.generate(\n",
    "\t\ttorch.zeros((1, 1), dtype=torch.long, device=device),\n",
    "\t\t100\n",
    "\t)[0].tolist()\n",
    ")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f01909",
   "metadata": {},
   "source": [
    "# Adding Layernorm + Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5894693",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=96\n",
    "block_size=64\n",
    "n_embed=64\n",
    "\n",
    "dropout=0.2\n",
    "\n",
    "class Head(nn.Module):\n",
    "\tdef __init__(self, head_size=head_size, n_embed=n_embed, block_size=block_size):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "\t\tself.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "\t\tself.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\t\tself.register_buffer('tril', \n",
    "\t\t\ttorch.tril(torch.ones(block_size, block_size))\n",
    "\t\t)\n",
    "\t\tself.attention_scalar = pow(head_size, -0.5)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\t_, T, _ = x.shape\n",
    "\t\tk = self.key(x)\n",
    "\t\tv = self.value(x)\n",
    "\t\tq = self.query(x)\n",
    "\n",
    "\t\t# doing attn. formula -> softmax(qK^T / sqrt(d_k)) * V\n",
    "\t\tweights = q @ k.transpose(-2, -1) * self.attention_scalar\n",
    "\t\tweights = F.softmax(\n",
    "\t\t\tweights.masked_fill(self.tril[:T, :T] == 0, float('-inf')),\n",
    "\t\t\tdim=-1\n",
    "\t\t)\n",
    "\t\tweights = self.dropout(weights)\n",
    "\t\tout = weights @ v\n",
    "\t\treturn out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\tdef __init__(self, num_heads, head_size):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "\t\tself.proj = nn.Linear(n_embed, n_embed)\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tout = torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "\t\tout = self.dropout(self.proj(out))\n",
    "\t\treturn out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\tdef __init__(self, n_embed):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.net = nn.Sequential(\n",
    "\t\t\tnn.Linear(n_embed, 4 * n_embed),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(4 * n_embed, n_embed),\n",
    "\t\t\tnn.Dropout(dropout)\n",
    "\t\t)\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "\tdef __init__(self, n_embed, n_head):\n",
    "\t\tsuper().__init__()\n",
    "\t\thead_size = n_embed // n_head\n",
    "\t\tself.sa = MultiHeadAttention(n_head, head_size)\n",
    "\t\tself.ffwd = FeedForward(n_embed)\n",
    "\t\tself.ln1 = nn.LayerNorm(n_embed)\n",
    "\t\tself.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = x + self.sa(self.ln1(x))\n",
    "\t\tx = x + self.ffwd(self.ln2(x))\n",
    "\t\treturn x\n",
    "\n",
    "# add blocks + residual connections\n",
    "class LayernormLanguageModel(nn.Module):\n",
    "\tdef __init__(self, vocab_size=vocab_size, n_embed=n_embed, block_size=block_size, head_size=head_size):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "\t\tself.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "\t\tself.blocks = nn.Sequential(\n",
    "\t\t\tBlock(n_embed, n_head=4),\n",
    "\t\t\tBlock(n_embed, n_head=4),\n",
    "\t\t\tBlock(n_embed, n_head=4),\n",
    "\t\t\tBlock(n_embed, n_head=4),\n",
    "\t\t\tnn.LayerNorm(n_embed),\n",
    "\t\t)\n",
    "\t\tself.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "\tdef forward(self, idx, targets=None):\n",
    "\t\tB, T = idx.shape\n",
    "\t\t\n",
    "\t\ttok_embed = self.token_embedding_table(idx)\n",
    "\t\tpos_embed = self.position_embedding_table(torch.arange(T, device=device))\n",
    "\t\tx = self.blocks(tok_embed + pos_embed)\n",
    "\t\tlogits = self.lm_head(x)\n",
    "\n",
    "\t\tif targets is None:\n",
    "\t\t\treturn logits, None\n",
    "\t\t\n",
    "\t\tB, T, C = logits.shape\n",
    "\t\tlogits_shrink = logits.view(B*T, C)\n",
    "\t\ttargets_shrink = targets.view(B*T)\n",
    "\t\tloss = F.cross_entropy(logits_shrink, targets_shrink)\n",
    "\t\treturn logits_shrink, loss\n",
    "\t\n",
    "\tdef generate(self, idx, max_new_tokens):\n",
    "\t\tfor _ in range(max_new_tokens):\n",
    "\t\t\tidx_cond = idx[:, -block_size:] # we can only use context length of block_size otherwise positional embeddings will fail\n",
    "\t\t\tlogits, _ = self(idx_cond)\n",
    "\t\t\tlogits = logits[:, -1, :]\n",
    "\t\t\tprobs = F.softmax(logits, dim = -1)\n",
    "\t\t\tidx_next = torch.multinomial(probs, num_samples = 1)\n",
    "\t\t\tidx = torch.cat((idx, idx_next), dim=1)\n",
    "\t\treturn idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "2ecbec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = LayernormLanguageModel().to(device)\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "885db15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 4.942709922790527\n",
      "100: 2.858935832977295\n",
      "200: 2.6107945442199707\n",
      "300: 2.4785642623901367\n",
      "400: 2.5089380741119385\n",
      "500: 2.4150643348693848\n",
      "600: 2.3734350204467773\n",
      "700: 2.3243725299835205\n",
      "800: 2.3161206245422363\n",
      "900: 2.388336181640625\n",
      "1000: 2.27097225189209\n",
      "1100: 2.2216544151306152\n",
      "1200: 2.191196918487549\n",
      "1300: 2.163285970687866\n",
      "1400: 2.122673511505127\n",
      "1500: 2.0306336879730225\n",
      "1600: 2.0130319595336914\n",
      "1700: 2.065638303756714\n",
      "1800: 1.9985215663909912\n",
      "1900: 1.9963879585266113\n",
      "2000: 1.937434196472168\n",
      "2100: 1.9453988075256348\n",
      "2200: 1.9944026470184326\n",
      "2300: 2.0154900550842285\n",
      "2400: 1.9585778713226318\n",
      "2500: 1.9265706539154053\n",
      "2600: 1.934171438217163\n",
      "2700: 1.845777988433838\n",
      "2800: 1.8982651233673096\n",
      "2900: 1.8758985996246338\n",
      "3000: 1.8523091077804565\n",
      "3100: 1.787508249282837\n",
      "3200: 1.8449921607971191\n",
      "3300: 1.8031344413757324\n",
      "3400: 1.809058666229248\n",
      "3500: 1.7857574224472046\n",
      "3600: 1.8346235752105713\n",
      "3700: 1.8288640975952148\n",
      "3800: 1.8219435214996338\n",
      "3900: 1.813114881515503\n",
      "4000: 1.7757937908172607\n",
      "4100: 1.7941405773162842\n",
      "4200: 1.8212943077087402\n",
      "4300: 1.7345166206359863\n",
      "4400: 1.8159606456756592\n",
      "4500: 1.728001356124878\n",
      "4600: 1.7414963245391846\n",
      "4700: 1.866267204284668\n",
      "4800: 1.755868673324585\n",
      "4900: 1.7928136587142944\n",
      "5000: 1.6952401399612427\n",
      "5100: 1.7126270532608032\n",
      "5200: 1.7251386642456055\n",
      "5300: 1.6703972816467285\n",
      "5400: 1.7605111598968506\n",
      "5500: 1.6914974451065063\n",
      "5600: 1.7102415561676025\n",
      "5700: 1.6674379110336304\n",
      "5800: 1.7437217235565186\n",
      "5900: 1.657997488975525\n"
     ]
    }
   ],
   "source": [
    "for steps in range(6000):\n",
    "\txb, yb = get_batch('train')\n",
    "\txb = xb.to(device)\n",
    "\tyb = yb.to(device)\n",
    "\tlogits, loss = m(xb, yb)\n",
    "\toptimizer.zero_grad(set_to_none=True)\n",
    "\tloss.backward()\n",
    "\toptimizer.step()\n",
    "\tif steps % 100 == 0:\n",
    "\t\tprint(f\"{steps}: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "03a2021c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minde inctrande—Xausering of rech ascentrent of my the oftent postillow liffeins revely of the sail fanc\n"
     ]
    }
   ],
   "source": [
    "print(\"\".join(decode(\n",
    "\tm.generate(\n",
    "\t\ttorch.tensor([encode(\"The mind\")], dtype=torch.long, device=device),\n",
    "\t\t100\n",
    "\t)[0].tolist()\n",
    ")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c63009",
   "metadata": {},
   "source": [
    "# QUESTIONS\n",
    "- What is the reason for the k / v / q vectors?\n",
    "- What is the reason for multiple heads? Why does this improve gains, over a single head?\n",
    "- Why do we have blocks? What is the optimal choice in the number of blocks?\n",
    "- Why 'feed-forward'? Is there a reason it is this simple?\n",
    "- How to determine an optimal C / channel size?\n",
    "- Why do residual block networks gain speedups?\n",
    "- Why do you need those projection layers if it is the same dim going in and out?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
