{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6afa1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTS\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from torch.amp import autocast\n",
    "\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "\n",
    "sys.path.append(os.path.join(ROOT_DIR, 'src'))\n",
    "sys.path.append(os.path.join(ROOT_DIR, 'src', 'utilities'))\n",
    "\n",
    "from models import gptv2 as transformer\n",
    "from utilities import tokenizer\n",
    "from utilities import tokenization\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ca0317c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.212m\n"
     ]
    }
   ],
   "source": [
    "## CONFIGS\n",
    "TRAIN_BIN = os.path.join(ROOT_DIR, \"data/processed/simplebooks-train.bin\")\n",
    "VAL_BIN = os.path.join(ROOT_DIR, \"data/processed/simplebooks-val.bin\")\n",
    "VOCAB_JSON = os.path.join(ROOT_DIR, \"artifacts/simplebooks_4097.json\")\n",
    "\n",
    "td = tokenization.read_tokenization(VOCAB_JSON)\n",
    "\n",
    "vocab_size = len(td.token_set)\n",
    "device=\"mps\"\n",
    "\n",
    "config = transformer.GPTv2Config(\n",
    "\tvocab_size=vocab_size,\n",
    "\tdevice=device,\n",
    ")\n",
    "m = transformer.LanguageModel(config)\n",
    "print(m.get_num_parameters(as_str=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d529b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD DATA\n",
    "train_data: torch.Tensor = torch.load(TRAIN_BIN, map_location='cpu')\n",
    "val_data: torch.Tensor = torch.load(VAL_BIN, map_location='cpu')\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "\tdef __init__(self, data_tensor: torch.Tensor, block_size: int):\n",
    "\t\tself.data = data_tensor\n",
    "\t\tself.block_size = block_size\n",
    "\t\tself.N = len(data_tensor) - block_size\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.N\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tchunk = self.data[idx:idx+self.block_size+1]\n",
    "\t\tx = chunk[:-1]\n",
    "\t\ty = chunk[1:]\n",
    "\t\treturn x, y\n",
    "\n",
    "train_dataset = TextDataset(train_data, config.block_size)\n",
    "val_dataset = TextDataset(val_data, config.block_size)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "\ttrain_dataset, \n",
    "\tbatch_size=config.batch_size,\n",
    "\tshuffle=True,\n",
    "#\tnum_workers=2,\n",
    "#\tpin_memory=True,\n",
    "#\tpin_memory_device=device\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "\tval_dataset, \n",
    "\tbatch_size=config.batch_size,\n",
    "\tshuffle=True,\n",
    "#\tnum_workers=2,\n",
    "#\tpin_memory=True,\n",
    "#\tpin_memory_device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7fd7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEFINE MODEL, OPTIMIZER, ETC.\n",
    "\n",
    "def make_val_loss_function(test_loader):\n",
    "\tval_iterator = iter(test_loader)\n",
    "\n",
    "\t@torch.no_grad()\n",
    "\tdef estimate_val_loss(model):\n",
    "\t\tmodel.eval()\n",
    "\t\tnonlocal val_iterator\n",
    "\t\ttry:\n",
    "\t\t\tX, Y = next(val_iterator)\n",
    "\t\texcept StopIteration:\n",
    "\t\t\tval_iterator = iter(test_loader)\n",
    "\t\t\tX, Y = next(val_iterator)\n",
    "\t\tX = X.to(device, non_blocking=True)\n",
    "\t\tY = Y.to(device, non_blocking=True)\n",
    "\n",
    "\t\t_, loss = model(X, Y)\n",
    "\t\treturn loss.item()\n",
    "\treturn estimate_val_loss\n",
    "\n",
    "estimate_val_loss = make_val_loss_function(val_loader)\n",
    "\n",
    "encode, decode = tokenizer.get_encoder(td), tokenizer.get_decoder(td)\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "m = transformer.LanguageModel(config).to(device=device)\n",
    "m.compile()\n",
    "\n",
    "optimizer = m.get_optimizer(weight_decay=0.001, lr=3e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "\toptimizer, mode='min', factor=0.1, patience=10\n",
    ")\n",
    "\n",
    "@torch.compile(fullgraph=False)\n",
    "def opt_step():\n",
    "\toptimizer.step()\n",
    "\n",
    "total_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9389b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e8a167aa68434a800e05e83026523a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/2000 [00:00<?, ?step/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1119 14:27:00.691000 58524 torch/_logging/_internal.py:1154] [1/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00000/02000] T: 8.40496 V: 7.98577\n",
      "[00025/02000] T: 6.36639\n",
      "[00050/02000] T: 5.62015 V: 5.33748\n",
      "[00075/02000] T: 5.14357\n",
      "[00100/02000] T: 4.74690 V: 4.38976\n",
      "[00125/02000] T: 4.59597\n",
      "[00150/02000] T: 4.30977 V: 3.89473\n",
      "[00175/02000] T: 4.21901\n",
      "[00200/02000] T: 4.09794 V: 3.67623\n",
      "[00225/02000] T: 4.05486\n",
      "[00250/02000] T: 4.02498 V: 3.50230\n",
      "[00275/02000] T: 3.88755\n",
      "training interrupted\n"
     ]
    }
   ],
   "source": [
    "VAL_INTERVAL = 50\n",
    "LOG_INTERVAL = 25\n",
    "MAX_STEPS = 2000\n",
    "\n",
    "rel_max = total_steps + MAX_STEPS\n",
    "try:\n",
    "\tpbar = tqdm(\n",
    "\t\ttotal=rel_max,\n",
    "\t\tinitial=total_steps,\n",
    "\t\tdesc=\"training\",\n",
    "\t\tunit=\"step\",\n",
    "\t\tposition=0,\n",
    "\t\tleave=True\n",
    "\t)\n",
    "\twhile True:\n",
    "\t\tfor x_batch, y_batch in train_loader:\n",
    "\t\t\tx_batch = x_batch.to(device, non_blocking=True)\n",
    "\t\t\ty_batch = y_batch.to(device, non_blocking=True)\n",
    "\t\t\tif total_steps >= rel_max: break\n",
    "\n",
    "\t\t\tm.train()\n",
    "\t\t\twith autocast(device_type=device, dtype=torch.float16):\n",
    "\t\t\t\tlogits, loss = m(x_batch, y_batch)\n",
    "\t\t\t\n",
    "\t\t\toptimizer.zero_grad(set_to_none=True)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\topt_step()\n",
    "\n",
    "\t\t\ttrain_loss = loss.item()\n",
    "\t\t\tcurrent_step = total_steps\n",
    "\t\t\tpbar.set_postfix(train_loss=f\"{train_loss:01.05f}\")\n",
    "\n",
    "\t\t\tif current_step % VAL_INTERVAL == 0:\n",
    "\t\t\t\tval_loss = estimate_val_loss(m)\n",
    "\t\t\t\tscheduler.step(val_loss)\n",
    "\t\t\t\tpbar.write(f\"[{total_steps:05d}/{rel_max:05d}] T: {train_loss:01.05f} V: {val_loss:01.05f}\")\n",
    "\t\t\t\tpbar.refresh()\n",
    "\t\t\telif current_step % LOG_INTERVAL == 0:\n",
    "\t\t\t\tpbar.write(f\"[{total_steps:05d}/{rel_max:05d}] T: {train_loss:01.05f}\")\n",
    "\t\t\t\tpbar.refresh()\n",
    "\n",
    "\t\t\ttotal_steps += 1\n",
    "\t\t\tpbar.update(1)\n",
    "\t\tif total_steps >= rel_max: break\n",
    "\t\tpbar.write(f\"full epoch completed!\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "\tpbar.write(\"training interrupted\")\n",
    "finally:\n",
    "\tpbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113c3ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The , the Fox was so hard Sanvaw grew , and far and took in her talk by the foot of the trail Aside , for Hill As Wishlightel Wippen 's fac New York her , with on that bad usual body of Sir Albert Gotle , confessed miles to , from skerres to the mystery , and leave there for her questions who means expected :\n",
      "\n",
      "\" Pretty Bere do something got by revolver ? carry him it every harm outside the loss of silent places of their falls , and , men , celenal , few fair Sturison , half one side . The summend of the brown bear could not remain without the feeling of that next day , at diserocious craast got to presently . His human word her teeth had learned that he had in some time too reply , walled animals , the right of the window and itself , sat down they doing games of leave temper kissed blissed them to the window . The horses but Angirelas grew on the rael , while Blake was a small sleep Morfle sent over me thus man , but nearly to learn the Peggy of the popularly Andies of Basilup , so by a stone Blue , who told them for the Manna \n"
     ]
    }
   ],
   "source": [
    "SEED = \"The \"\n",
    "idx = torch.tensor([encode(SEED)], dtype=torch.long, device=device)\n",
    "print(SEED, end=\"\", flush=True)\n",
    "for token in m.generate(idx, max_new_tokens=400):\n",
    "\tv = token.item()\n",
    "\tprint(decode([v])[0], end=\"\", flush=True)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
