{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bb403d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from torch.amp import autocast\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "from models import gptv2 as transformer\n",
    "from utilities import text_cleaning, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13846cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.179m\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 512\n",
    "device = \"mps\"\n",
    "config = transformer.GPTv2Config(\n",
    "\tvocab_size=vocab_size,\n",
    "\tdevice=device,\n",
    ")\n",
    "m = transformer.LanguageModel(config)\n",
    "print(m.get_num_parameters(as_str=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "373e8fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../../data/gsm8k/full.txt\"\n",
    "input_file = open(filepath, 'r', encoding='utf-8')\n",
    "raw_text = input_file.read()\n",
    "input_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50eec530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<START>', '<END>', '\\t', '\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '^', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}', '\\xa0', '¢', '£', '³', '¼', '½', '¾', '×', 'ç', 'è', 'é', 'ñ', '÷', 'А', '\\u200b', '–', '—', '‘', '’', '“', '”', '€', '−', '√', ' t', 'he', ' a', 'in', ' the', ' s', 'es', ' o', 're', '00', ' m', 'er', ' h', ' c', ' p', ' b', ' f', ' w', 'an', ' of', ' to', 'is', ' 1', 'nd', 'al', 'ar', ' d', 'ou', ' 2', 'at', 'as', 'or', 'on', 'ed', ' e', 'll', 'ch', 'en', 'ow', ' in', 'ing', ' and', 'ts', ' 3', ' n', 'ay', ' he', ' g', ' th', 'The', ' is', ' l', 'it', ' 4', 'ic', ' co', '50', 'et', '20', 'AR', 'ND', 'END', 'ST', 'ART', 'START', ' for', 'ir', 'tal', ' total', 'st', ' H', '10', 'il', 've', 'ec', ' on', 'ro', ' has', '60', 'ach', 'any', 'id', 'um', 'ut', ' we', 'our', 'ill', 'ak', ' many', ' 5', 'ad', ' st', 'im', 'le', '12', ' she', ' are', 'ber', 'ore', ' re', 'us', '000', '40', 'ac', ' do', 'am', ' each', 'ers', 'ot', ' so', 'gh', 'ks', 'umber', ' as', ' r', ' day', ' number', 'ave', 'ld', '80', ' hour', ' How', ' y', 'If', 'ars', 'all', ' 6', ' 10', '30', 'ft', ' ne', 'oo', 'ri', ' that', ' per', 'th', '15', ' will', 'ag', ' her', ' his', ' sp', 'ver', 'nt', ' mu', ' be', 'ce', ' 20', ' S', '100', ' min', 'her', ' ye', 'ur', ' at', 'ice', 'ent', ' can', ' 8', ' x', ' tw', 'nds', ' how', ' minut', ' tim', 'les', 'ght', 'om', ' have', ' 7', ' mon', 'ion', ' M', ' hours', 'ain', ' wee', 'ies', ' cost', ' If', ' 12', ' le', ' wh', ' minutes', 'qu', 'ake', ' than', ' ch', 'up', '25', ' bu', ' A', 'el', ' much', ' need', 'op', ' J', 'un', ' se', ' more', ' was', ' fin', '24', ' The', ' an', 'irst', ' it', ' sh', '16', 'ith', 'ly', 'ount', ' 30', 'rom', ' pl', ' k', ' week', ' fe', ' mil', ' by', 'akes', 'ree', ' there', ' 9', ' does', 'So', ' with', '18', 'est', 'ents', ' they', 'ick', 'iv', ' from', 'use', 'ey', ' years', 'ons', ' times', '200', ' 15', '14', 'ds', ' T', 'hes', 'There', 'ud', 'ond', 'art', ' pe', ' did', 'se', ' buy', ' B', 'fter', 'day', ' sec', 'gs', ' pi', ' find', ' C', ' had', ' He', ' spe', 'ack', 'pp', ' 40', ' first', ' bec', ' days', ' miles', 'ets', ' two', 'alf', ' me', 'ause', ' j', ' 60', ' bo', ' wor', ' com', 'and', ' left', ' pou', ' because', ' one', 'ie', 'very', 'He', ' all', ' am', 'ok', ' were', 'In', ' tr', 'ul', ' car', ' pa', ' 50', ' ad', ' stud', 'ip', 'if', 'ge', '36', ' three', 'ate', ' cl', ' month', ' half', '120', 'ans', ' or', ' needs', 'ass', ' inc', 'fore', ' 100', 'ess', ' amount', 'Th', 'ab', '75', ' twice', ' W', 'av', 'ople', ' boo', ' then', ' people', '300', ' money', ' pounds', ' old', ' v', '90', 'ought', 'On', 'age', ' gr', ' E', 'ast', 'ight', 'ls', 'ater', ' every', ' pay', ' feet', ' students', ' go', ' second', 'Then', ' ro', 'arn', ' make', ' pr', ' She', ' bl', 'ats', ' rem', 'ist', 'ich', '400', ' app', ' bought', 'ants', ' fl', ' po', ' time', 'ide', 'ive', '35', 'get', 'ong', '45', 'red', 'ap', 'uc', ' te', 'ould', ' pro', 'ish', '70', ' them', ' bot', '150', ' box', ' sell', ' F', 'oc', ' 16', ' get', ' 25', ' earn', 'os']\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "from utilities import text_cleaning\n",
    "from utilities import tokenizer as tokenizer\n",
    "text = raw_text\n",
    "td = tokenizer.create_tokenizer(text, num_tokens=vocab_size, predefined=[\"<START>\", \"<END>\"])\n",
    "print(td.token_set)\n",
    "print(len(td.token_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "475c7eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters, idx_to_token, token_to_idx = td\n",
    "encode = tokenizer.get_encoder(td)\n",
    "decode = tokenizer.get_decoder(td)\n",
    "data = torch.tensor(encode(text), dtype=torch.long).to(device=device)\n",
    "\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "block_size, batch_size = config.block_size, config.batch_size\n",
    "def get_batch(split):\n",
    "  data = train_data if split == 'train' else val_data\n",
    "  idxs = torch.randint(len(data) - block_size, (batch_size,), device=device)\n",
    "  x = torch.stack([data[i:i+block_size] for i in idxs])\n",
    "  y = torch.stack([data[i+1:i+block_size+1] for i in idxs])\n",
    "  return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_val_loss(model):\n",
    "  model.eval()\n",
    "  X, Y = get_batch(\"val\")\n",
    "  _, loss = model(X, Y)\n",
    "  model.train()\n",
    "  return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53aa93b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "m = transformer.LanguageModel(config).to(device=device)\n",
    "m.compile()\n",
    "\n",
    "optimizer = m.get_optimizer(weight_decay=0.01, lr=3e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "\toptimizer, mode='min', factor=0.1, patience=10\n",
    ")\n",
    "\n",
    "@torch.compile(fullgraph=False)\n",
    "def opt_step():\n",
    "\toptimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab190009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0000/1000] train: 1.63325 val: 1.65944\n",
      "[0025/1000] train: 1.64618\n",
      "[0050/1000] train: 1.56198 val: 1.59178\n",
      "[0075/1000] train: 1.53419\n",
      "[0100/1000] train: 1.61095 val: 1.71255\n",
      "[0125/1000] train: 1.58318\n",
      "[0150/1000] train: 1.63872 val: 1.62444\n",
      "[0175/1000] train: 1.64037\n",
      "[0200/1000] train: 1.53076 val: 1.71924\n",
      "[0225/1000] train: 1.63400\n",
      "[0250/1000] train: 1.63927 val: 1.55578\n",
      "[0275/1000] train: 1.57439\n",
      "[0300/1000] train: 1.61565 val: 1.58658\n",
      "[0325/1000] train: 1.54941\n",
      "[0350/1000] train: 1.59138 val: 1.68251\n",
      "[0375/1000] train: 1.62881\n",
      "[0400/1000] train: 1.65113 val: 1.65079\n",
      "[0425/1000] train: 1.55605\n",
      "[0450/1000] train: 1.51294 val: 1.66040\n",
      "[0475/1000] train: 1.56573\n",
      "[0500/1000] train: 1.62771 val: 1.54786\n",
      "[0525/1000] train: 1.62546\n",
      "[0550/1000] train: 1.61106 val: 1.62593\n",
      "[0575/1000] train: 1.56638\n",
      "[0600/1000] train: 1.65033 val: 1.57127\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m loss.backward()\n\u001b[32m      9\u001b[39m opt_step()\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m train_loss, val_loss = \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m step % \u001b[32m50\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     12\u001b[39m \tval_loss = estimate_val_loss(m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "num_steps = 1000\n",
    "for step in range(num_steps):\n",
    "\txb, yb = get_batch('train')\n",
    "\tm.train()\n",
    "\twith autocast(device_type=\"mps\", dtype=torch.float16):\n",
    "\t\tlogits, loss = m(xb, yb)\n",
    "\toptimizer.zero_grad(set_to_none=True)\n",
    "\tloss.backward()\n",
    "\topt_step()\n",
    "\ttrain_loss, val_loss = loss.item(), None\n",
    "\tif step % 50 == 0:\n",
    "\t\tval_loss = estimate_val_loss(m)\n",
    "\t\tscheduler.step(val_loss)\n",
    "\t\tprint(f\"[{step:04d}/{num_steps}] train: {train_loss:01.05f} val: {val_loss:01.05f}\")\n",
    "\telif step % 25 == 0:\n",
    "\t\tprint(f\"[{step:04d}/{num_steps}] train: {train_loss:01.05f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d49e656a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START>I have ten apples. Tobias takes six. How many do I have left?\n",
      "It takes 3 pets and 7 dogs to skill.\n",
      "10 x 3 = <<10*3=30>>30 pets.\n",
      "15 It takes up only 60 - 10 = <<150-10=60>>60 dogs to skill.\n",
      "25 - 30 = <<25-30=10>>10 dogs remaining.\n",
      "#### 10\n"
     ]
    }
   ],
   "source": [
    "seed = \"<START>I have ten apples. Tobias takes six. How many do I have left?\"\n",
    "end_tok = encode(\"<END>\")[0]\n",
    "idx = torch.tensor([encode(seed)], dtype=torch.long, device=device)\n",
    "print(seed, end=\"\", flush=True)\n",
    "for token in m.generate(idx, max_new_tokens=400):\n",
    "\tv = token.item()\n",
    "\tif v == end_tok:\n",
    "\t\tbreak\n",
    "\tprint(decode([v])[0], end=\"\", flush=True)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
