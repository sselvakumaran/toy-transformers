{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Test02: TrainingRun Mid-Epoch Save/Resume\n",
    "\n",
    "This notebook tests the TrainingRun save/load functionality:\n",
    "1. Train a small GPTv1 model for a few batches\n",
    "2. Save checkpoint mid-epoch\n",
    "3. Resume training from exact position\n",
    "4. Create FinalModel for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root: /Users/sriman/dev/apps/toy-transformers\n",
      "Experiment: /Users/sriman/dev/apps/toy-transformers/experiments/test02\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def get_project_info() -> Path:\n",
    "  current = Path.cwd().resolve()\n",
    "  root = current\n",
    "  for parent in [current, *current.parents]:\n",
    "    if (parent / \"toy_transformers\").exists():\n",
    "      root = parent\n",
    "      break\n",
    "  return root, current\n",
    "\n",
    "if 'ROOT_DIR' not in globals():\n",
    "\tROOT_DIR, EXPERIMENT_DIR = get_project_info()\n",
    "\tif str(ROOT_DIR) not in sys.path:\n",
    "\t\tsys.path.append(str(ROOT_DIR))\n",
    "\tif Path.cwd() != ROOT_DIR:\n",
    "\t\tos.chdir(ROOT_DIR)\n",
    "\n",
    "print(f\"Root: {ROOT_DIR}\")\n",
    "print(f\"Experiment: {EXPERIMENT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful\n"
     ]
    }
   ],
   "source": [
    "from toy_transformers.models import gptv1\n",
    "from toy_transformers.training.training_run import TrainingRun\n",
    "from toy_transformers.training.optimizer import OptimizerConfig, AdamWConfig, NoSchedulerConfig\n",
    "from toy_transformers.utilities import io\n",
    "from toy_transformers.data import tokenization\n",
    "from toy_transformers.utilities.reproducibility import set_all_seeds\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting word frequencies...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting words: 100%|██████████| 1.12M/1.12M [00:00<00:00, 15.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 15294 unique words\n",
      "base vocabulary size: 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "merging tokens: 100%|██████████| 500/500 [00:00<00:00, 4213.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 433,344 tokens\n"
     ]
    }
   ],
   "source": [
    "# Load dataset from run01\n",
    "vocab_path = ROOT_DIR / \"experiments/test02/artifacts/vocab256\"\n",
    "data_path = ROOT_DIR / \"experiments/test02/artifacts/data\"\n",
    "\n",
    "if not vocab_path.exists() or not data_path.exists():\n",
    "  tokenization.tokenize_from_scratch(\n",
    "  \t\"data/input.txt\",\n",
    "  \tvocab_path,\n",
    "  \tdata_path,\n",
    "  \t500,\n",
    "  \tverbose=True\n",
    "\t)\n",
    "\n",
    "train_data = io.load(str(data_path))\n",
    "train_dataset = tokenization.TokenizedData.from_state_dict(train_data)\n",
    "print(f\"Dataset loaded: {len(train_dataset.data):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "setup_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: batch_size=4, block_size=32\n",
      "Architecture: n_layers=2, n_embed=64, n_heads=2\n",
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Set reproducibility\n",
    "set_all_seeds(42, deterministic=True)\n",
    "\n",
    "# Small model for testing\n",
    "config = gptv1.GPTv1Config(\n",
    "  batch_size=4,\n",
    "  block_size=32,\n",
    "  n_heads=2,\n",
    "  n_embed=64,\n",
    "  n_layers=2,\n",
    "  dropout=0.1,\n",
    "  device=\"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "vocab_size = 256\n",
    "\n",
    "print(f\"Config: batch_size={config.batch_size}, block_size={config.block_size}\")\n",
    "print(f\"Architecture: n_layers={config.n_layers}, n_embed={config.n_embed}, n_heads={config.n_heads}\")\n",
    "print(f\"Device: {config.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "create_training_run",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingRun created\n",
      "Dataset hash: -2075907010083267406\n",
      "Model parameters: 134,784\n"
     ]
    }
   ],
   "source": [
    "# Create training run\n",
    "training_run = TrainingRun(\n",
    "\tmodel_class=gptv1.LanguageModel,\n",
    "\tconfig_class=gptv1.GPTv1Config,\n",
    "\tmodel_config=config,\n",
    "\toptimizer_config=OptimizerConfig(\n",
    "\t\toptimizer_type=\"adamw\",\n",
    "\t\toptimizer_params=AdamWConfig(lr=1e-3, weight_decay=0.01),\n",
    "\t\tscheduler=NoSchedulerConfig()\n",
    "\t),\n",
    "\tbase_seed=42,\n",
    "\tdataset=train_dataset,\n",
    "\tblock_size=config.block_size,\n",
    "\tbatch_size=config.batch_size,\n",
    "\tvocab_size=vocab_size\n",
    ")\n",
    "\n",
    "model = training_run.create_model().to(config.device)\n",
    "optimizer = training_run.create_optimizer(model)\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"TrainingRun created\")\n",
    "print(f\"Dataset hash: {training_run.dataset_hash}\")\n",
    "print(f\"Model parameters: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "train_initial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training initial batches...\n",
      "Batch 0, Step 1, Loss: 3.8792\n",
      "Batch 1, Step 2, Loss: 4.3184\n",
      "Batch 2, Step 3, Loss: 4.2239\n",
      "\n",
      "⚡ Saving checkpoint at batch 2...\n",
      "✓ Checkpoint saved\n",
      "State: epoch=0, step=3, batches_completed=3\n"
     ]
    }
   ],
   "source": [
    "# Train for a few batches\n",
    "print(\"Training initial batches...\")\n",
    "save_after_batches = 2\n",
    "checkpoint_path = EXPERIMENT_DIR / \"artifacts/checkpoints\"\n",
    "\n",
    "training_run.epoch = 0\n",
    "dataloader = training_run.create_dataloader(train_dataset, epoch=0)\n",
    "\n",
    "for batch_idx, (x, y) in enumerate(dataloader):\n",
    "    x, y = x.to(config.device), y.to(config.device)\n",
    "    \n",
    "    logits, loss = model(x, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    training_run.step += 1\n",
    "    training_run.batches_completed += 1\n",
    "    training_run.log_step(train_loss=loss.item(), lr=optimizer.get_lr())\n",
    "    \n",
    "    print(f\"Batch {batch_idx}, Step {training_run.step}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    if batch_idx == save_after_batches:\n",
    "        print(f\"\\n⚡ Saving checkpoint at batch {batch_idx}...\")\n",
    "        io.save(\n",
    "            training_run.to_state_dict(model, optimizer),\n",
    "            str(checkpoint_path)\n",
    "        )\n",
    "        print(f\"✓ Checkpoint saved\")\n",
    "        print(f\"State: epoch={training_run.epoch}, step={training_run.step}, batches_completed={training_run.batches_completed}\")\n",
    "        break\n",
    "    \n",
    "    if batch_idx >= 9:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "resume",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Testing Resume from Checkpoint\n",
      "============================================================\n",
      "\n",
      "✓ Checkpoint loaded\n",
      "✓ Dataset verified (hash: -2075907010083267406)\n",
      "Resuming from: epoch=0, step=3, batches_completed=3\n",
      "✓ Model and optimizer restored\n"
     ]
    }
   ],
   "source": [
    "# Resume from checkpoint\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing Resume from Checkpoint\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "loaded = io.load(str(checkpoint_path))\n",
    "resumed_run, model_state, opt_state = TrainingRun.from_state_dict(loaded)\n",
    "print(\"✓ Checkpoint loaded\")\n",
    "\n",
    "# Verify dataset\n",
    "resumed_run.verify_dataset(train_dataset)\n",
    "print(f\"✓ Dataset verified (hash: {resumed_run.dataset_hash})\")\n",
    "print(f\"Resuming from: epoch={resumed_run.epoch}, step={resumed_run.step}, batches_completed={resumed_run.batches_completed}\")\n",
    "\n",
    "# Restore model and optimizer\n",
    "model = resumed_run.create_model().to(config.device)\n",
    "model.load_state_dict(model_state)\n",
    "optimizer = resumed_run.create_optimizer(model)\n",
    "optimizer.load_state_dict(opt_state)\n",
    "print(\"✓ Model and optimizer restored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "continue_training",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resuming training from batch 3...\n",
      "Resumed - Batch 3, Step 4, Loss: 4.1431\n",
      "Resumed - Batch 4, Step 5, Loss: 4.6351\n",
      "Resumed - Batch 5, Step 6, Loss: 4.0246\n",
      "Resumed - Batch 6, Step 7, Loss: 3.9792\n",
      "Resumed - Batch 7, Step 8, Loss: 4.1612\n",
      "Resumed - Batch 8, Step 9, Loss: 4.2653\n",
      "Resumed - Batch 9, Step 10, Loss: 4.3622\n",
      "Resumed - Batch 10, Step 11, Loss: 3.9521\n",
      "Resumed - Batch 11, Step 12, Loss: 4.1943\n",
      "Resumed - Batch 12, Step 13, Loss: 4.0568\n",
      "Resumed - Batch 13, Step 14, Loss: 4.1620\n",
      "Resumed - Batch 14, Step 15, Loss: 4.1277\n",
      "Resumed - Batch 15, Step 16, Loss: 3.9290\n",
      "Resumed - Batch 16, Step 17, Loss: 4.0221\n",
      "Resumed - Batch 17, Step 18, Loss: 3.9685\n",
      "Resumed - Batch 18, Step 19, Loss: 3.9574\n",
      "Resumed - Batch 19, Step 20, Loss: 3.6438\n",
      "Resumed - Batch 20, Step 21, Loss: 4.0119\n",
      "Resumed - Batch 21, Step 22, Loss: 4.1739\n",
      "Resumed - Batch 22, Step 23, Loss: 4.1003\n",
      "Resumed - Batch 23, Step 24, Loss: 4.0907\n",
      "Resumed - Batch 24, Step 25, Loss: 3.7849\n",
      "Resumed - Batch 25, Step 26, Loss: 4.3483\n",
      "Resumed - Batch 26, Step 27, Loss: 3.8355\n",
      "Resumed - Batch 27, Step 28, Loss: 4.0538\n",
      "Resumed - Batch 28, Step 29, Loss: 3.9423\n",
      "Resumed - Batch 29, Step 30, Loss: 3.8044\n",
      "Resumed - Batch 30, Step 31, Loss: 4.0636\n",
      "Resumed - Batch 31, Step 32, Loss: 4.4705\n",
      "Resumed - Batch 32, Step 33, Loss: 4.2633\n",
      "Resumed - Batch 33, Step 34, Loss: 4.3198\n",
      "Resumed - Batch 34, Step 35, Loss: 4.2965\n",
      "Resumed - Batch 35, Step 36, Loss: 4.0698\n",
      "Resumed - Batch 36, Step 37, Loss: 3.9972\n",
      "Resumed - Batch 37, Step 38, Loss: 3.5854\n",
      "Resumed - Batch 38, Step 39, Loss: 4.1347\n",
      "Resumed - Batch 39, Step 40, Loss: 4.0718\n",
      "Resumed - Batch 40, Step 41, Loss: 3.9812\n",
      "Resumed - Batch 41, Step 42, Loss: 3.7950\n",
      "Resumed - Batch 42, Step 43, Loss: 4.5493\n",
      "Resumed - Batch 43, Step 44, Loss: 3.9284\n",
      "Resumed - Batch 44, Step 45, Loss: 4.1542\n",
      "Resumed - Batch 45, Step 46, Loss: 3.9983\n",
      "Resumed - Batch 46, Step 47, Loss: 3.6884\n",
      "Resumed - Batch 47, Step 48, Loss: 3.9412\n",
      "Resumed - Batch 48, Step 49, Loss: 3.5853\n",
      "Resumed - Batch 49, Step 50, Loss: 4.1868\n",
      "Resumed - Batch 50, Step 51, Loss: 3.9399\n",
      "Resumed - Batch 51, Step 52, Loss: 3.6225\n",
      "Resumed - Batch 52, Step 53, Loss: 3.9978\n",
      "Resumed - Batch 53, Step 54, Loss: 3.9114\n",
      "Resumed - Batch 54, Step 55, Loss: 4.2918\n",
      "Resumed - Batch 55, Step 56, Loss: 4.4107\n",
      "Resumed - Batch 56, Step 57, Loss: 3.9056\n",
      "Resumed - Batch 57, Step 58, Loss: 3.8672\n",
      "Resumed - Batch 58, Step 59, Loss: 4.3640\n",
      "Resumed - Batch 59, Step 60, Loss: 4.0603\n",
      "Resumed - Batch 60, Step 61, Loss: 4.0060\n",
      "Resumed - Batch 61, Step 62, Loss: 3.8021\n",
      "Resumed - Batch 62, Step 63, Loss: 3.7978\n",
      "Resumed - Batch 63, Step 64, Loss: 4.1439\n",
      "Resumed - Batch 64, Step 65, Loss: 3.9674\n",
      "Resumed - Batch 65, Step 66, Loss: 4.1901\n",
      "Resumed - Batch 66, Step 67, Loss: 4.0960\n",
      "Resumed - Batch 67, Step 68, Loss: 3.8285\n",
      "Resumed - Batch 68, Step 69, Loss: 4.3412\n",
      "Resumed - Batch 69, Step 70, Loss: 3.9646\n",
      "Resumed - Batch 70, Step 71, Loss: 4.0752\n",
      "Resumed - Batch 71, Step 72, Loss: 4.1350\n",
      "Resumed - Batch 72, Step 73, Loss: 3.6495\n",
      "Resumed - Batch 73, Step 74, Loss: 4.1272\n",
      "Resumed - Batch 74, Step 75, Loss: 3.7056\n",
      "Resumed - Batch 75, Step 76, Loss: 3.9942\n",
      "Resumed - Batch 76, Step 77, Loss: 3.8481\n",
      "Resumed - Batch 77, Step 78, Loss: 4.4003\n",
      "Resumed - Batch 78, Step 79, Loss: 4.0358\n",
      "Resumed - Batch 79, Step 80, Loss: 3.7516\n",
      "Resumed - Batch 80, Step 81, Loss: 4.2304\n",
      "Resumed - Batch 81, Step 82, Loss: 3.9924\n",
      "Resumed - Batch 82, Step 83, Loss: 3.7890\n",
      "Resumed - Batch 83, Step 84, Loss: 3.6366\n",
      "Resumed - Batch 84, Step 85, Loss: 4.0156\n",
      "Resumed - Batch 85, Step 86, Loss: 3.6600\n",
      "Resumed - Batch 86, Step 87, Loss: 4.1578\n",
      "Resumed - Batch 87, Step 88, Loss: 4.0556\n",
      "Resumed - Batch 88, Step 89, Loss: 3.7885\n",
      "Resumed - Batch 89, Step 90, Loss: 4.0123\n",
      "Resumed - Batch 90, Step 91, Loss: 3.5049\n",
      "Resumed - Batch 91, Step 92, Loss: 4.0734\n",
      "Resumed - Batch 92, Step 93, Loss: 3.7036\n",
      "Resumed - Batch 93, Step 94, Loss: 4.0430\n",
      "Resumed - Batch 94, Step 95, Loss: 3.8992\n",
      "Resumed - Batch 95, Step 96, Loss: 4.0229\n",
      "Resumed - Batch 96, Step 97, Loss: 3.7326\n",
      "Resumed - Batch 97, Step 98, Loss: 3.9473\n",
      "Resumed - Batch 98, Step 99, Loss: 4.0680\n",
      "Resumed - Batch 99, Step 100, Loss: 3.8122\n",
      "Resumed - Batch 100, Step 101, Loss: 3.9198\n",
      "Resumed - Batch 101, Step 102, Loss: 4.0009\n",
      "Resumed - Batch 102, Step 103, Loss: 3.6251\n",
      "Resumed - Batch 103, Step 104, Loss: 3.7952\n",
      "Resumed - Batch 104, Step 105, Loss: 3.7088\n",
      "Resumed - Batch 105, Step 106, Loss: 3.8081\n",
      "Resumed - Batch 106, Step 107, Loss: 3.8853\n",
      "Resumed - Batch 107, Step 108, Loss: 3.7306\n",
      "Resumed - Batch 108, Step 109, Loss: 4.1806\n",
      "Resumed - Batch 109, Step 110, Loss: 3.7174\n",
      "Resumed - Batch 110, Step 111, Loss: 3.9830\n",
      "Resumed - Batch 111, Step 112, Loss: 3.8817\n",
      "Resumed - Batch 112, Step 113, Loss: 3.9565\n",
      "Resumed - Batch 113, Step 114, Loss: 3.2479\n",
      "Resumed - Batch 114, Step 115, Loss: 3.9381\n",
      "Resumed - Batch 115, Step 116, Loss: 4.0565\n",
      "Resumed - Batch 116, Step 117, Loss: 3.9842\n",
      "Resumed - Batch 117, Step 118, Loss: 3.5705\n",
      "Resumed - Batch 118, Step 119, Loss: 3.8040\n",
      "Resumed - Batch 119, Step 120, Loss: 3.8415\n",
      "Resumed - Batch 120, Step 121, Loss: 3.6127\n",
      "Resumed - Batch 121, Step 122, Loss: 3.8373\n",
      "Resumed - Batch 122, Step 123, Loss: 3.6823\n",
      "Resumed - Batch 123, Step 124, Loss: 3.5893\n",
      "Resumed - Batch 124, Step 125, Loss: 4.0035\n",
      "Resumed - Batch 125, Step 126, Loss: 3.6307\n",
      "Resumed - Batch 126, Step 127, Loss: 3.7514\n",
      "Resumed - Batch 127, Step 128, Loss: 3.8314\n",
      "Resumed - Batch 128, Step 129, Loss: 3.6041\n",
      "Resumed - Batch 129, Step 130, Loss: 3.5558\n",
      "Resumed - Batch 130, Step 131, Loss: 4.0340\n",
      "Resumed - Batch 131, Step 132, Loss: 3.8919\n",
      "Resumed - Batch 132, Step 133, Loss: 3.6679\n",
      "Resumed - Batch 133, Step 134, Loss: 3.7229\n",
      "Resumed - Batch 134, Step 135, Loss: 3.7459\n",
      "Resumed - Batch 135, Step 136, Loss: 3.9569\n",
      "Resumed - Batch 136, Step 137, Loss: 3.5267\n",
      "Resumed - Batch 137, Step 138, Loss: 3.9205\n",
      "Resumed - Batch 138, Step 139, Loss: 4.0420\n",
      "Resumed - Batch 139, Step 140, Loss: 3.5834\n",
      "Resumed - Batch 140, Step 141, Loss: 3.6248\n",
      "Resumed - Batch 141, Step 142, Loss: 3.3681\n",
      "Resumed - Batch 142, Step 143, Loss: 3.7171\n",
      "Resumed - Batch 143, Step 144, Loss: 3.4229\n",
      "Resumed - Batch 144, Step 145, Loss: 3.5896\n",
      "Resumed - Batch 145, Step 146, Loss: 4.1019\n",
      "Resumed - Batch 146, Step 147, Loss: 3.5867\n",
      "Resumed - Batch 147, Step 148, Loss: 3.8096\n",
      "Resumed - Batch 148, Step 149, Loss: 3.5888\n",
      "Resumed - Batch 149, Step 150, Loss: 3.5251\n",
      "Resumed - Batch 150, Step 151, Loss: 3.4878\n",
      "Resumed - Batch 151, Step 152, Loss: 3.8952\n",
      "Resumed - Batch 152, Step 153, Loss: 3.6460\n",
      "Resumed - Batch 153, Step 154, Loss: 3.5513\n",
      "Resumed - Batch 154, Step 155, Loss: 3.6834\n",
      "Resumed - Batch 155, Step 156, Loss: 3.6033\n",
      "Resumed - Batch 156, Step 157, Loss: 3.5467\n",
      "Resumed - Batch 157, Step 158, Loss: 3.6670\n",
      "Resumed - Batch 158, Step 159, Loss: 3.7330\n",
      "Resumed - Batch 159, Step 160, Loss: 3.6832\n",
      "Resumed - Batch 160, Step 161, Loss: 3.4419\n",
      "Resumed - Batch 161, Step 162, Loss: 3.6273\n",
      "Resumed - Batch 162, Step 163, Loss: 3.9170\n",
      "Resumed - Batch 163, Step 164, Loss: 3.7402\n",
      "Resumed - Batch 164, Step 165, Loss: 3.8104\n",
      "Resumed - Batch 165, Step 166, Loss: 3.4641\n",
      "Resumed - Batch 166, Step 167, Loss: 3.3545\n",
      "Resumed - Batch 167, Step 168, Loss: 3.4492\n",
      "Resumed - Batch 168, Step 169, Loss: 3.5921\n",
      "Resumed - Batch 169, Step 170, Loss: 3.6395\n",
      "Resumed - Batch 170, Step 171, Loss: 3.7868\n",
      "Resumed - Batch 171, Step 172, Loss: 3.4218\n",
      "Resumed - Batch 172, Step 173, Loss: 3.8806\n",
      "Resumed - Batch 173, Step 174, Loss: 3.7453\n",
      "Resumed - Batch 174, Step 175, Loss: 3.4543\n",
      "Resumed - Batch 175, Step 176, Loss: 3.6486\n",
      "Resumed - Batch 176, Step 177, Loss: 3.3806\n",
      "Resumed - Batch 177, Step 178, Loss: 3.6272\n",
      "Resumed - Batch 178, Step 179, Loss: 3.6304\n",
      "Resumed - Batch 179, Step 180, Loss: 3.7994\n",
      "Resumed - Batch 180, Step 181, Loss: 3.6409\n",
      "Resumed - Batch 181, Step 182, Loss: 3.5802\n",
      "Resumed - Batch 182, Step 183, Loss: 3.1180\n",
      "Resumed - Batch 183, Step 184, Loss: 3.4274\n",
      "Resumed - Batch 184, Step 185, Loss: 3.4722\n",
      "Resumed - Batch 185, Step 186, Loss: 3.5538\n",
      "Resumed - Batch 186, Step 187, Loss: 3.4803\n",
      "Resumed - Batch 187, Step 188, Loss: 3.3167\n",
      "Resumed - Batch 188, Step 189, Loss: 3.7487\n",
      "Resumed - Batch 189, Step 190, Loss: 3.5962\n",
      "Resumed - Batch 190, Step 191, Loss: 2.8694\n",
      "Resumed - Batch 191, Step 192, Loss: 3.6918\n",
      "Resumed - Batch 192, Step 193, Loss: 3.4725\n",
      "Resumed - Batch 193, Step 194, Loss: 3.6151\n",
      "Resumed - Batch 194, Step 195, Loss: 3.6238\n",
      "Resumed - Batch 195, Step 196, Loss: 3.6255\n",
      "Resumed - Batch 196, Step 197, Loss: 3.3172\n",
      "Resumed - Batch 197, Step 198, Loss: 3.7641\n",
      "Resumed - Batch 198, Step 199, Loss: 3.4950\n",
      "Resumed - Batch 199, Step 200, Loss: 3.3275\n",
      "Resumed - Batch 200, Step 201, Loss: 3.3802\n",
      "Resumed - Batch 201, Step 202, Loss: 3.1859\n",
      "Resumed - Batch 202, Step 203, Loss: 3.4252\n",
      "Resumed - Batch 203, Step 204, Loss: 3.6233\n",
      "Resumed - Batch 204, Step 205, Loss: 3.5104\n",
      "Resumed - Batch 205, Step 206, Loss: 3.3875\n",
      "Resumed - Batch 206, Step 207, Loss: 3.8386\n",
      "Resumed - Batch 207, Step 208, Loss: 3.4289\n",
      "Resumed - Batch 208, Step 209, Loss: 3.5023\n",
      "Resumed - Batch 209, Step 210, Loss: 3.3034\n",
      "Resumed - Batch 210, Step 211, Loss: 3.3002\n",
      "Resumed - Batch 211, Step 212, Loss: 3.2292\n",
      "Resumed - Batch 212, Step 213, Loss: 3.6257\n",
      "Resumed - Batch 213, Step 214, Loss: 3.6195\n",
      "Resumed - Batch 214, Step 215, Loss: 3.4436\n",
      "Resumed - Batch 215, Step 216, Loss: 3.4061\n",
      "Resumed - Batch 216, Step 217, Loss: 3.6519\n",
      "Resumed - Batch 217, Step 218, Loss: 3.9012\n",
      "Resumed - Batch 218, Step 219, Loss: 3.1538\n",
      "Resumed - Batch 219, Step 220, Loss: 3.6434\n",
      "Resumed - Batch 220, Step 221, Loss: 3.0095\n",
      "Resumed - Batch 221, Step 222, Loss: 3.4368\n",
      "Resumed - Batch 222, Step 223, Loss: 3.4473\n",
      "Resumed - Batch 223, Step 224, Loss: 3.6950\n",
      "Resumed - Batch 224, Step 225, Loss: 3.3727\n",
      "Resumed - Batch 225, Step 226, Loss: 3.4943\n",
      "Resumed - Batch 226, Step 227, Loss: 3.2465\n",
      "Resumed - Batch 227, Step 228, Loss: 3.5820\n",
      "Resumed - Batch 228, Step 229, Loss: 3.9446\n",
      "Resumed - Batch 229, Step 230, Loss: 3.2954\n",
      "Resumed - Batch 230, Step 231, Loss: 3.4605\n",
      "Resumed - Batch 231, Step 232, Loss: 3.4667\n",
      "Resumed - Batch 232, Step 233, Loss: 3.7550\n",
      "Resumed - Batch 233, Step 234, Loss: 3.4445\n",
      "Resumed - Batch 234, Step 235, Loss: 3.3624\n",
      "Resumed - Batch 235, Step 236, Loss: 3.1928\n",
      "Resumed - Batch 236, Step 237, Loss: 3.4160\n",
      "Resumed - Batch 237, Step 238, Loss: 3.1830\n",
      "Resumed - Batch 238, Step 239, Loss: 3.6707\n",
      "Resumed - Batch 239, Step 240, Loss: 3.2917\n",
      "Resumed - Batch 240, Step 241, Loss: 3.0835\n",
      "Resumed - Batch 241, Step 242, Loss: 3.2317\n",
      "Resumed - Batch 242, Step 243, Loss: 3.3147\n",
      "Resumed - Batch 243, Step 244, Loss: 3.4137\n",
      "Resumed - Batch 244, Step 245, Loss: 3.6443\n",
      "Resumed - Batch 245, Step 246, Loss: 3.3541\n",
      "Resumed - Batch 246, Step 247, Loss: 3.6864\n",
      "Resumed - Batch 247, Step 248, Loss: 3.2025\n",
      "Resumed - Batch 248, Step 249, Loss: 3.3123\n",
      "Resumed - Batch 249, Step 250, Loss: 3.3218\n",
      "Resumed - Batch 250, Step 251, Loss: 3.4192\n",
      "Resumed - Batch 251, Step 252, Loss: 3.2116\n",
      "Resumed - Batch 252, Step 253, Loss: 3.4030\n",
      "Resumed - Batch 253, Step 254, Loss: 3.3196\n",
      "Resumed - Batch 254, Step 255, Loss: 2.9535\n",
      "Resumed - Batch 255, Step 256, Loss: 3.5072\n",
      "Resumed - Batch 256, Step 257, Loss: 2.7526\n",
      "Resumed - Batch 257, Step 258, Loss: 3.1286\n",
      "Resumed - Batch 258, Step 259, Loss: 4.1611\n",
      "Resumed - Batch 259, Step 260, Loss: 3.7870\n",
      "Resumed - Batch 260, Step 261, Loss: 3.5894\n",
      "Resumed - Batch 261, Step 262, Loss: 3.8840\n",
      "Resumed - Batch 262, Step 263, Loss: 3.5787\n",
      "Resumed - Batch 263, Step 264, Loss: 3.1211\n",
      "Resumed - Batch 264, Step 265, Loss: 3.6177\n",
      "Resumed - Batch 265, Step 266, Loss: 3.6023\n",
      "Resumed - Batch 266, Step 267, Loss: 3.2633\n",
      "Resumed - Batch 267, Step 268, Loss: 3.4671\n",
      "Resumed - Batch 268, Step 269, Loss: 3.5493\n",
      "Resumed - Batch 269, Step 270, Loss: 3.1857\n",
      "Resumed - Batch 270, Step 271, Loss: 3.4776\n",
      "Resumed - Batch 271, Step 272, Loss: 3.2084\n",
      "Resumed - Batch 272, Step 273, Loss: 3.2018\n",
      "Resumed - Batch 273, Step 274, Loss: 2.8609\n",
      "Resumed - Batch 274, Step 275, Loss: 3.1251\n",
      "Resumed - Batch 275, Step 276, Loss: 3.3761\n",
      "Resumed - Batch 276, Step 277, Loss: 3.2735\n",
      "Resumed - Batch 277, Step 278, Loss: 3.3615\n",
      "Resumed - Batch 278, Step 279, Loss: 3.3841\n",
      "Resumed - Batch 279, Step 280, Loss: 3.5318\n",
      "Resumed - Batch 280, Step 281, Loss: 3.5005\n",
      "Resumed - Batch 281, Step 282, Loss: 3.2009\n",
      "Resumed - Batch 282, Step 283, Loss: 3.1706\n",
      "Resumed - Batch 283, Step 284, Loss: 3.1442\n",
      "Resumed - Batch 284, Step 285, Loss: 3.5113\n",
      "Resumed - Batch 285, Step 286, Loss: 3.4725\n",
      "Resumed - Batch 286, Step 287, Loss: 3.3727\n",
      "Resumed - Batch 287, Step 288, Loss: 3.5086\n",
      "Resumed - Batch 288, Step 289, Loss: 3.4356\n",
      "Resumed - Batch 289, Step 290, Loss: 3.1540\n",
      "Resumed - Batch 290, Step 291, Loss: 3.3631\n",
      "Resumed - Batch 291, Step 292, Loss: 3.2073\n",
      "Resumed - Batch 292, Step 293, Loss: 3.6007\n",
      "Resumed - Batch 293, Step 294, Loss: 3.4566\n",
      "Resumed - Batch 294, Step 295, Loss: 3.3605\n",
      "Resumed - Batch 295, Step 296, Loss: 3.3531\n",
      "Resumed - Batch 296, Step 297, Loss: 3.4253\n",
      "Resumed - Batch 297, Step 298, Loss: 3.5581\n",
      "Resumed - Batch 298, Step 299, Loss: 3.3471\n",
      "Resumed - Batch 299, Step 300, Loss: 3.2656\n",
      "Resumed - Batch 300, Step 301, Loss: 3.0531\n",
      "Resumed - Batch 301, Step 302, Loss: 3.2706\n",
      "Resumed - Batch 302, Step 303, Loss: 3.2374\n",
      "Resumed - Batch 303, Step 304, Loss: 3.2986\n",
      "Resumed - Batch 304, Step 305, Loss: 3.3589\n",
      "Resumed - Batch 305, Step 306, Loss: 3.0390\n",
      "Resumed - Batch 306, Step 307, Loss: 3.2167\n",
      "Resumed - Batch 307, Step 308, Loss: 3.3211\n",
      "Resumed - Batch 308, Step 309, Loss: 2.9254\n",
      "Resumed - Batch 309, Step 310, Loss: 3.4100\n",
      "Resumed - Batch 310, Step 311, Loss: 3.0520\n",
      "Resumed - Batch 311, Step 312, Loss: 3.4475\n",
      "Resumed - Batch 312, Step 313, Loss: 3.4182\n",
      "Resumed - Batch 313, Step 314, Loss: 3.2232\n",
      "Resumed - Batch 314, Step 315, Loss: 3.3137\n",
      "Resumed - Batch 315, Step 316, Loss: 3.2149\n",
      "Resumed - Batch 316, Step 317, Loss: 3.3663\n",
      "Resumed - Batch 317, Step 318, Loss: 3.3895\n",
      "Resumed - Batch 318, Step 319, Loss: 3.3360\n",
      "Resumed - Batch 319, Step 320, Loss: 3.3614\n",
      "Resumed - Batch 320, Step 321, Loss: 3.2808\n",
      "Resumed - Batch 321, Step 322, Loss: 3.2470\n",
      "Resumed - Batch 322, Step 323, Loss: 3.3961\n",
      "Resumed - Batch 323, Step 324, Loss: 3.1628\n",
      "Resumed - Batch 324, Step 325, Loss: 3.3112\n",
      "Resumed - Batch 325, Step 326, Loss: 3.1202\n",
      "Resumed - Batch 326, Step 327, Loss: 3.2402\n",
      "Resumed - Batch 327, Step 328, Loss: 3.2762\n",
      "Resumed - Batch 328, Step 329, Loss: 3.4233\n",
      "Resumed - Batch 329, Step 330, Loss: 3.0790\n",
      "Resumed - Batch 330, Step 331, Loss: 3.2040\n",
      "Resumed - Batch 331, Step 332, Loss: 3.5096\n",
      "Resumed - Batch 332, Step 333, Loss: 3.4920\n",
      "Resumed - Batch 333, Step 334, Loss: 3.4271\n",
      "Resumed - Batch 334, Step 335, Loss: 3.1770\n",
      "Resumed - Batch 335, Step 336, Loss: 2.9799\n",
      "Resumed - Batch 336, Step 337, Loss: 3.4254\n",
      "Resumed - Batch 337, Step 338, Loss: 3.5697\n",
      "Resumed - Batch 338, Step 339, Loss: 3.5818\n",
      "Resumed - Batch 339, Step 340, Loss: 3.2690\n",
      "Resumed - Batch 340, Step 341, Loss: 3.2353\n",
      "Resumed - Batch 341, Step 342, Loss: 3.0038\n",
      "Resumed - Batch 342, Step 343, Loss: 3.4952\n",
      "Resumed - Batch 343, Step 344, Loss: 3.0175\n",
      "Resumed - Batch 344, Step 345, Loss: 3.1059\n",
      "Resumed - Batch 345, Step 346, Loss: 3.2216\n",
      "Resumed - Batch 346, Step 347, Loss: 3.2025\n",
      "Resumed - Batch 347, Step 348, Loss: 3.2025\n",
      "Resumed - Batch 348, Step 349, Loss: 3.3549\n",
      "Resumed - Batch 349, Step 350, Loss: 3.4409\n",
      "Resumed - Batch 350, Step 351, Loss: 2.8851\n",
      "Resumed - Batch 351, Step 352, Loss: 3.2904\n",
      "Resumed - Batch 352, Step 353, Loss: 3.2694\n",
      "Resumed - Batch 353, Step 354, Loss: 3.2790\n",
      "Resumed - Batch 354, Step 355, Loss: 3.0844\n",
      "Resumed - Batch 355, Step 356, Loss: 3.1708\n",
      "Resumed - Batch 356, Step 357, Loss: 3.2409\n",
      "Resumed - Batch 357, Step 358, Loss: 3.3179\n",
      "Resumed - Batch 358, Step 359, Loss: 3.0029\n",
      "Resumed - Batch 359, Step 360, Loss: 3.1709\n",
      "Resumed - Batch 360, Step 361, Loss: 2.9825\n",
      "Resumed - Batch 361, Step 362, Loss: 3.1587\n",
      "Resumed - Batch 362, Step 363, Loss: 2.9954\n",
      "Resumed - Batch 363, Step 364, Loss: 3.1119\n",
      "Resumed - Batch 364, Step 365, Loss: 3.3326\n",
      "Resumed - Batch 365, Step 366, Loss: 3.2028\n",
      "Resumed - Batch 366, Step 367, Loss: 3.1584\n",
      "Resumed - Batch 367, Step 368, Loss: 3.0196\n",
      "Resumed - Batch 368, Step 369, Loss: 3.3098\n",
      "Resumed - Batch 369, Step 370, Loss: 3.3916\n",
      "Resumed - Batch 370, Step 371, Loss: 3.3355\n",
      "Resumed - Batch 371, Step 372, Loss: 3.3001\n",
      "Resumed - Batch 372, Step 373, Loss: 3.3987\n",
      "Resumed - Batch 373, Step 374, Loss: 3.6238\n",
      "Resumed - Batch 374, Step 375, Loss: 3.1340\n",
      "Resumed - Batch 375, Step 376, Loss: 2.9991\n",
      "Resumed - Batch 376, Step 377, Loss: 2.9135\n",
      "Resumed - Batch 377, Step 378, Loss: 3.2709\n",
      "Resumed - Batch 378, Step 379, Loss: 3.3268\n",
      "Resumed - Batch 379, Step 380, Loss: 3.1154\n",
      "Resumed - Batch 380, Step 381, Loss: 3.4284\n",
      "Resumed - Batch 381, Step 382, Loss: 3.3432\n",
      "Resumed - Batch 382, Step 383, Loss: 3.1873\n",
      "Resumed - Batch 383, Step 384, Loss: 3.0358\n",
      "Resumed - Batch 384, Step 385, Loss: 3.0019\n",
      "Resumed - Batch 385, Step 386, Loss: 3.2809\n",
      "Resumed - Batch 386, Step 387, Loss: 3.2729\n",
      "Resumed - Batch 387, Step 388, Loss: 3.4603\n",
      "Resumed - Batch 388, Step 389, Loss: 3.1490\n",
      "Resumed - Batch 389, Step 390, Loss: 3.5447\n",
      "Resumed - Batch 390, Step 391, Loss: 3.0421\n",
      "Resumed - Batch 391, Step 392, Loss: 3.0597\n",
      "Resumed - Batch 392, Step 393, Loss: 3.5564\n",
      "Resumed - Batch 393, Step 394, Loss: 3.3048\n",
      "Resumed - Batch 394, Step 395, Loss: 3.1935\n",
      "Resumed - Batch 395, Step 396, Loss: 3.1050\n",
      "Resumed - Batch 396, Step 397, Loss: 3.2065\n",
      "Resumed - Batch 397, Step 398, Loss: 2.9072\n",
      "Resumed - Batch 398, Step 399, Loss: 3.0811\n",
      "Resumed - Batch 399, Step 400, Loss: 3.1007\n",
      "Resumed - Batch 400, Step 401, Loss: 3.2132\n",
      "Resumed - Batch 401, Step 402, Loss: 2.8069\n",
      "Resumed - Batch 402, Step 403, Loss: 3.1408\n",
      "Resumed - Batch 403, Step 404, Loss: 3.1113\n",
      "Resumed - Batch 404, Step 405, Loss: 3.0901\n",
      "Resumed - Batch 405, Step 406, Loss: 3.0886\n",
      "Resumed - Batch 406, Step 407, Loss: 3.2312\n",
      "Resumed - Batch 407, Step 408, Loss: 3.1724\n",
      "Resumed - Batch 408, Step 409, Loss: 2.8753\n",
      "Resumed - Batch 409, Step 410, Loss: 3.2128\n",
      "Resumed - Batch 410, Step 411, Loss: 3.3087\n",
      "Resumed - Batch 411, Step 412, Loss: 3.2911\n",
      "Resumed - Batch 412, Step 413, Loss: 3.2163\n",
      "Resumed - Batch 413, Step 414, Loss: 3.3455\n",
      "Resumed - Batch 414, Step 415, Loss: 2.8786\n",
      "Resumed - Batch 415, Step 416, Loss: 3.0508\n",
      "Resumed - Batch 416, Step 417, Loss: 3.2857\n",
      "Resumed - Batch 417, Step 418, Loss: 3.7627\n",
      "Resumed - Batch 418, Step 419, Loss: 3.0695\n",
      "Resumed - Batch 419, Step 420, Loss: 3.2274\n",
      "Resumed - Batch 420, Step 421, Loss: 3.1400\n",
      "Resumed - Batch 421, Step 422, Loss: 3.0145\n",
      "Resumed - Batch 422, Step 423, Loss: 3.1001\n",
      "Resumed - Batch 423, Step 424, Loss: 3.3842\n",
      "Resumed - Batch 424, Step 425, Loss: 3.4836\n",
      "Resumed - Batch 425, Step 426, Loss: 3.3611\n",
      "Resumed - Batch 426, Step 427, Loss: 2.9620\n",
      "Resumed - Batch 427, Step 428, Loss: 3.0757\n",
      "Resumed - Batch 428, Step 429, Loss: 2.7999\n",
      "Resumed - Batch 429, Step 430, Loss: 2.9125\n",
      "Resumed - Batch 430, Step 431, Loss: 3.1301\n",
      "Resumed - Batch 431, Step 432, Loss: 3.1357\n",
      "Resumed - Batch 432, Step 433, Loss: 2.7256\n",
      "Resumed - Batch 433, Step 434, Loss: 2.9733\n",
      "Resumed - Batch 434, Step 435, Loss: 2.7723\n",
      "Resumed - Batch 435, Step 436, Loss: 3.1612\n",
      "Resumed - Batch 436, Step 437, Loss: 3.0225\n",
      "Resumed - Batch 437, Step 438, Loss: 3.4321\n",
      "Resumed - Batch 438, Step 439, Loss: 2.8875\n",
      "Resumed - Batch 439, Step 440, Loss: 3.2468\n",
      "Resumed - Batch 440, Step 441, Loss: 3.0855\n",
      "Resumed - Batch 441, Step 442, Loss: 3.2684\n",
      "Resumed - Batch 442, Step 443, Loss: 2.9337\n",
      "Resumed - Batch 443, Step 444, Loss: 3.2521\n",
      "Resumed - Batch 444, Step 445, Loss: 3.1352\n",
      "Resumed - Batch 445, Step 446, Loss: 3.1000\n",
      "Resumed - Batch 446, Step 447, Loss: 3.0132\n",
      "Resumed - Batch 447, Step 448, Loss: 2.9695\n",
      "Resumed - Batch 448, Step 449, Loss: 3.2414\n",
      "Resumed - Batch 449, Step 450, Loss: 2.9953\n",
      "Resumed - Batch 450, Step 451, Loss: 3.0502\n",
      "Resumed - Batch 451, Step 452, Loss: 3.1354\n",
      "Resumed - Batch 452, Step 453, Loss: 3.1739\n",
      "Resumed - Batch 453, Step 454, Loss: 3.2750\n",
      "Resumed - Batch 454, Step 455, Loss: 3.1804\n",
      "Resumed - Batch 455, Step 456, Loss: 2.9802\n",
      "Resumed - Batch 456, Step 457, Loss: 3.1361\n",
      "Resumed - Batch 457, Step 458, Loss: 3.1364\n",
      "Resumed - Batch 458, Step 459, Loss: 3.4329\n",
      "Resumed - Batch 459, Step 460, Loss: 3.2145\n",
      "Resumed - Batch 460, Step 461, Loss: 2.9487\n",
      "Resumed - Batch 461, Step 462, Loss: 3.1009\n",
      "Resumed - Batch 462, Step 463, Loss: 3.0376\n",
      "Resumed - Batch 463, Step 464, Loss: 3.0426\n",
      "Resumed - Batch 464, Step 465, Loss: 3.0501\n",
      "Resumed - Batch 465, Step 466, Loss: 3.4477\n",
      "Resumed - Batch 466, Step 467, Loss: 2.9672\n",
      "Resumed - Batch 467, Step 468, Loss: 3.1579\n",
      "Resumed - Batch 468, Step 469, Loss: 2.8184\n",
      "Resumed - Batch 469, Step 470, Loss: 2.9841\n",
      "Resumed - Batch 470, Step 471, Loss: 3.2423\n",
      "Resumed - Batch 471, Step 472, Loss: 3.3091\n",
      "Resumed - Batch 472, Step 473, Loss: 2.6593\n",
      "Resumed - Batch 473, Step 474, Loss: 2.8982\n",
      "Resumed - Batch 474, Step 475, Loss: 2.9888\n",
      "Resumed - Batch 475, Step 476, Loss: 3.2428\n",
      "Resumed - Batch 476, Step 477, Loss: 3.2214\n",
      "Resumed - Batch 477, Step 478, Loss: 2.8440\n",
      "Resumed - Batch 478, Step 479, Loss: 2.9985\n",
      "Resumed - Batch 479, Step 480, Loss: 3.0581\n",
      "Resumed - Batch 480, Step 481, Loss: 2.9380\n",
      "Resumed - Batch 481, Step 482, Loss: 3.3194\n",
      "Resumed - Batch 482, Step 483, Loss: 3.5387\n",
      "Resumed - Batch 483, Step 484, Loss: 2.9092\n",
      "Resumed - Batch 484, Step 485, Loss: 2.9587\n",
      "Resumed - Batch 485, Step 486, Loss: 2.7672\n",
      "Resumed - Batch 486, Step 487, Loss: 3.1311\n",
      "Resumed - Batch 487, Step 488, Loss: 3.4084\n",
      "Resumed - Batch 488, Step 489, Loss: 3.3618\n",
      "Resumed - Batch 489, Step 490, Loss: 3.2667\n",
      "Resumed - Batch 490, Step 491, Loss: 3.0610\n",
      "Resumed - Batch 491, Step 492, Loss: 2.6433\n",
      "Resumed - Batch 492, Step 493, Loss: 3.2924\n",
      "Resumed - Batch 493, Step 494, Loss: 3.2847\n",
      "Resumed - Batch 494, Step 495, Loss: 3.3056\n",
      "Resumed - Batch 495, Step 496, Loss: 3.1662\n",
      "Resumed - Batch 496, Step 497, Loss: 2.9061\n",
      "Resumed - Batch 497, Step 498, Loss: 2.9027\n",
      "Resumed - Batch 498, Step 499, Loss: 3.1796\n",
      "Resumed - Batch 499, Step 500, Loss: 3.1380\n",
      "Resumed - Batch 500, Step 501, Loss: 2.8396\n",
      "Resumed - Batch 501, Step 502, Loss: 2.9743\n",
      "Resumed - Batch 502, Step 503, Loss: 2.8538\n",
      "Resumed - Batch 503, Step 504, Loss: 3.3244\n",
      "Resumed - Batch 504, Step 505, Loss: 3.2754\n",
      "Resumed - Batch 505, Step 506, Loss: 2.8753\n",
      "Resumed - Batch 506, Step 507, Loss: 3.1024\n",
      "Resumed - Batch 507, Step 508, Loss: 2.8500\n",
      "Resumed - Batch 508, Step 509, Loss: 3.1669\n",
      "Resumed - Batch 509, Step 510, Loss: 2.9904\n",
      "Resumed - Batch 510, Step 511, Loss: 3.0607\n",
      "Resumed - Batch 511, Step 512, Loss: 2.4812\n",
      "Resumed - Batch 512, Step 513, Loss: 2.8570\n",
      "Resumed - Batch 513, Step 514, Loss: 3.1748\n",
      "Resumed - Batch 514, Step 515, Loss: 3.1058\n",
      "Resumed - Batch 515, Step 516, Loss: 2.9784\n",
      "Resumed - Batch 516, Step 517, Loss: 3.0227\n",
      "Resumed - Batch 517, Step 518, Loss: 3.0372\n",
      "Resumed - Batch 518, Step 519, Loss: 3.3764\n",
      "Resumed - Batch 519, Step 520, Loss: 3.2309\n",
      "Resumed - Batch 520, Step 521, Loss: 2.8153\n",
      "Resumed - Batch 521, Step 522, Loss: 2.9194\n",
      "Resumed - Batch 522, Step 523, Loss: 2.9350\n",
      "Resumed - Batch 523, Step 524, Loss: 2.5461\n",
      "Resumed - Batch 524, Step 525, Loss: 3.1975\n",
      "Resumed - Batch 525, Step 526, Loss: 3.0936\n",
      "Resumed - Batch 526, Step 527, Loss: 3.2880\n",
      "Resumed - Batch 527, Step 528, Loss: 2.7536\n",
      "Resumed - Batch 528, Step 529, Loss: 3.2350\n",
      "Resumed - Batch 529, Step 530, Loss: 3.2077\n",
      "Resumed - Batch 530, Step 531, Loss: 3.0383\n",
      "Resumed - Batch 531, Step 532, Loss: 3.0832\n",
      "Resumed - Batch 532, Step 533, Loss: 3.1606\n",
      "Resumed - Batch 533, Step 534, Loss: 3.3572\n",
      "Resumed - Batch 534, Step 535, Loss: 3.2244\n",
      "Resumed - Batch 535, Step 536, Loss: 3.3099\n",
      "Resumed - Batch 536, Step 537, Loss: 3.2476\n",
      "Resumed - Batch 537, Step 538, Loss: 3.3410\n",
      "Resumed - Batch 538, Step 539, Loss: 2.7695\n",
      "Resumed - Batch 539, Step 540, Loss: 3.3281\n",
      "Resumed - Batch 540, Step 541, Loss: 2.9365\n",
      "Resumed - Batch 541, Step 542, Loss: 2.7924\n",
      "Resumed - Batch 542, Step 543, Loss: 3.0468\n",
      "Resumed - Batch 543, Step 544, Loss: 3.0755\n",
      "Resumed - Batch 544, Step 545, Loss: 3.2527\n",
      "Resumed - Batch 545, Step 546, Loss: 3.3482\n",
      "Resumed - Batch 546, Step 547, Loss: 3.2094\n",
      "Resumed - Batch 547, Step 548, Loss: 3.1868\n",
      "Resumed - Batch 548, Step 549, Loss: 3.1080\n",
      "Resumed - Batch 549, Step 550, Loss: 2.9781\n",
      "Resumed - Batch 550, Step 551, Loss: 3.3541\n",
      "Resumed - Batch 551, Step 552, Loss: 2.9330\n",
      "Resumed - Batch 552, Step 553, Loss: 3.5620\n",
      "Resumed - Batch 553, Step 554, Loss: 3.1362\n",
      "Resumed - Batch 554, Step 555, Loss: 2.9047\n",
      "Resumed - Batch 555, Step 556, Loss: 3.4953\n",
      "Resumed - Batch 556, Step 557, Loss: 3.0614\n",
      "Resumed - Batch 557, Step 558, Loss: 3.4801\n",
      "Resumed - Batch 558, Step 559, Loss: 3.2235\n",
      "Resumed - Batch 559, Step 560, Loss: 3.5028\n",
      "Resumed - Batch 560, Step 561, Loss: 3.2604\n",
      "Resumed - Batch 561, Step 562, Loss: 3.3410\n",
      "Resumed - Batch 562, Step 563, Loss: 3.0419\n",
      "Resumed - Batch 563, Step 564, Loss: 3.5487\n",
      "Resumed - Batch 564, Step 565, Loss: 3.1513\n",
      "Resumed - Batch 565, Step 566, Loss: 3.1267\n",
      "Resumed - Batch 566, Step 567, Loss: 3.1985\n",
      "Resumed - Batch 567, Step 568, Loss: 3.7760\n",
      "Resumed - Batch 568, Step 569, Loss: 3.0299\n",
      "Resumed - Batch 569, Step 570, Loss: 3.4564\n",
      "Resumed - Batch 570, Step 571, Loss: 3.5015\n",
      "Resumed - Batch 571, Step 572, Loss: 3.1199\n",
      "Resumed - Batch 572, Step 573, Loss: 2.9455\n",
      "Resumed - Batch 573, Step 574, Loss: 3.2141\n",
      "Resumed - Batch 574, Step 575, Loss: 3.1257\n",
      "Resumed - Batch 575, Step 576, Loss: 2.9861\n",
      "Resumed - Batch 576, Step 577, Loss: 2.8660\n",
      "Resumed - Batch 577, Step 578, Loss: 3.0479\n",
      "Resumed - Batch 578, Step 579, Loss: 3.1138\n",
      "Resumed - Batch 579, Step 580, Loss: 2.9854\n",
      "Resumed - Batch 580, Step 581, Loss: 3.0254\n",
      "Resumed - Batch 581, Step 582, Loss: 3.4711\n",
      "Resumed - Batch 582, Step 583, Loss: 2.9247\n",
      "Resumed - Batch 583, Step 584, Loss: 2.9138\n",
      "Resumed - Batch 584, Step 585, Loss: 2.8768\n",
      "Resumed - Batch 585, Step 586, Loss: 3.0989\n",
      "Resumed - Batch 586, Step 587, Loss: 3.0325\n",
      "Resumed - Batch 587, Step 588, Loss: 3.1721\n",
      "Resumed - Batch 588, Step 589, Loss: 2.9288\n",
      "Resumed - Batch 589, Step 590, Loss: 3.0445\n",
      "Resumed - Batch 590, Step 591, Loss: 3.0171\n",
      "Resumed - Batch 591, Step 592, Loss: 2.9437\n",
      "Resumed - Batch 592, Step 593, Loss: 2.9513\n",
      "Resumed - Batch 593, Step 594, Loss: 3.0961\n",
      "Resumed - Batch 594, Step 595, Loss: 3.0008\n",
      "Resumed - Batch 595, Step 596, Loss: 3.1034\n",
      "Resumed - Batch 596, Step 597, Loss: 3.0888\n",
      "Resumed - Batch 597, Step 598, Loss: 3.2486\n",
      "Resumed - Batch 598, Step 599, Loss: 2.5053\n",
      "Resumed - Batch 599, Step 600, Loss: 3.3904\n",
      "Resumed - Batch 600, Step 601, Loss: 2.8323\n",
      "Resumed - Batch 601, Step 602, Loss: 3.0889\n",
      "Resumed - Batch 602, Step 603, Loss: 3.2930\n",
      "Resumed - Batch 603, Step 604, Loss: 3.2006\n",
      "Resumed - Batch 604, Step 605, Loss: 3.0240\n",
      "Resumed - Batch 605, Step 606, Loss: 2.6657\n",
      "Resumed - Batch 606, Step 607, Loss: 3.0131\n",
      "Resumed - Batch 607, Step 608, Loss: 2.9832\n",
      "Resumed - Batch 608, Step 609, Loss: 3.3000\n",
      "Resumed - Batch 609, Step 610, Loss: 2.6764\n",
      "Resumed - Batch 610, Step 611, Loss: 3.0964\n",
      "Resumed - Batch 611, Step 612, Loss: 2.9203\n",
      "Resumed - Batch 612, Step 613, Loss: 3.0370\n",
      "Resumed - Batch 613, Step 614, Loss: 2.7780\n",
      "Resumed - Batch 614, Step 615, Loss: 2.9540\n",
      "Resumed - Batch 615, Step 616, Loss: 3.0615\n",
      "Resumed - Batch 616, Step 617, Loss: 3.1145\n",
      "Resumed - Batch 617, Step 618, Loss: 3.2953\n",
      "Resumed - Batch 618, Step 619, Loss: 3.0637\n",
      "Resumed - Batch 619, Step 620, Loss: 3.1239\n",
      "Resumed - Batch 620, Step 621, Loss: 2.7942\n",
      "Resumed - Batch 621, Step 622, Loss: 2.9598\n",
      "Resumed - Batch 622, Step 623, Loss: 3.3058\n",
      "Resumed - Batch 623, Step 624, Loss: 3.1806\n",
      "Resumed - Batch 624, Step 625, Loss: 3.1556\n",
      "Resumed - Batch 625, Step 626, Loss: 3.1190\n",
      "Resumed - Batch 626, Step 627, Loss: 2.7135\n",
      "Resumed - Batch 627, Step 628, Loss: 3.1744\n",
      "Resumed - Batch 628, Step 629, Loss: 3.1165\n",
      "Resumed - Batch 629, Step 630, Loss: 2.9757\n",
      "Resumed - Batch 630, Step 631, Loss: 2.6491\n",
      "Resumed - Batch 631, Step 632, Loss: 2.9041\n",
      "Resumed - Batch 632, Step 633, Loss: 3.0401\n",
      "Resumed - Batch 633, Step 634, Loss: 2.9406\n",
      "Resumed - Batch 634, Step 635, Loss: 2.6587\n",
      "Resumed - Batch 635, Step 636, Loss: 3.0927\n",
      "Resumed - Batch 636, Step 637, Loss: 2.9709\n",
      "Resumed - Batch 637, Step 638, Loss: 3.6636\n",
      "Resumed - Batch 638, Step 639, Loss: 3.2512\n",
      "Resumed - Batch 639, Step 640, Loss: 3.1864\n",
      "Resumed - Batch 640, Step 641, Loss: 2.8082\n",
      "Resumed - Batch 641, Step 642, Loss: 3.2305\n",
      "Resumed - Batch 642, Step 643, Loss: 3.3752\n",
      "Resumed - Batch 643, Step 644, Loss: 3.1206\n",
      "Resumed - Batch 644, Step 645, Loss: 2.8897\n",
      "Resumed - Batch 645, Step 646, Loss: 2.9943\n",
      "Resumed - Batch 646, Step 647, Loss: 3.2031\n",
      "Resumed - Batch 647, Step 648, Loss: 2.6017\n",
      "Resumed - Batch 648, Step 649, Loss: 3.0611\n",
      "Resumed - Batch 649, Step 650, Loss: 2.9959\n",
      "Resumed - Batch 650, Step 651, Loss: 3.5472\n",
      "Resumed - Batch 651, Step 652, Loss: 2.6594\n",
      "Resumed - Batch 652, Step 653, Loss: 2.9704\n",
      "Resumed - Batch 653, Step 654, Loss: 3.0883\n",
      "Resumed - Batch 654, Step 655, Loss: 3.3065\n",
      "Resumed - Batch 655, Step 656, Loss: 2.7937\n",
      "Resumed - Batch 656, Step 657, Loss: 2.8179\n",
      "Resumed - Batch 657, Step 658, Loss: 2.9928\n",
      "Resumed - Batch 658, Step 659, Loss: 2.9373\n",
      "Resumed - Batch 659, Step 660, Loss: 3.1274\n",
      "Resumed - Batch 660, Step 661, Loss: 2.8631\n",
      "Resumed - Batch 661, Step 662, Loss: 3.3052\n",
      "Resumed - Batch 662, Step 663, Loss: 3.1728\n",
      "Resumed - Batch 663, Step 664, Loss: 3.2557\n",
      "Resumed - Batch 664, Step 665, Loss: 3.1797\n",
      "Resumed - Batch 665, Step 666, Loss: 3.2983\n",
      "Resumed - Batch 666, Step 667, Loss: 2.7820\n",
      "Resumed - Batch 667, Step 668, Loss: 3.1685\n",
      "Resumed - Batch 668, Step 669, Loss: 2.7434\n",
      "Resumed - Batch 669, Step 670, Loss: 3.2342\n",
      "Resumed - Batch 670, Step 671, Loss: 2.6352\n",
      "Resumed - Batch 671, Step 672, Loss: 2.9293\n",
      "Resumed - Batch 672, Step 673, Loss: 2.7689\n",
      "Resumed - Batch 673, Step 674, Loss: 3.0119\n",
      "Resumed - Batch 674, Step 675, Loss: 3.1362\n",
      "Resumed - Batch 675, Step 676, Loss: 3.0578\n",
      "Resumed - Batch 676, Step 677, Loss: 2.9125\n",
      "Resumed - Batch 677, Step 678, Loss: 2.7266\n",
      "Resumed - Batch 678, Step 679, Loss: 3.1213\n",
      "Resumed - Batch 679, Step 680, Loss: 3.0077\n",
      "Resumed - Batch 680, Step 681, Loss: 2.9896\n",
      "Resumed - Batch 681, Step 682, Loss: 2.9335\n",
      "Resumed - Batch 682, Step 683, Loss: 3.0662\n",
      "Resumed - Batch 683, Step 684, Loss: 2.8571\n",
      "Resumed - Batch 684, Step 685, Loss: 2.6785\n",
      "Resumed - Batch 685, Step 686, Loss: 3.1801\n",
      "Resumed - Batch 686, Step 687, Loss: 3.1686\n",
      "Resumed - Batch 687, Step 688, Loss: 3.0515\n",
      "Resumed - Batch 688, Step 689, Loss: 3.0491\n",
      "Resumed - Batch 689, Step 690, Loss: 2.8234\n",
      "Resumed - Batch 690, Step 691, Loss: 3.6320\n",
      "Resumed - Batch 691, Step 692, Loss: 2.9752\n",
      "Resumed - Batch 692, Step 693, Loss: 2.8459\n",
      "Resumed - Batch 693, Step 694, Loss: 2.9927\n",
      "Resumed - Batch 694, Step 695, Loss: 2.9833\n",
      "Resumed - Batch 695, Step 696, Loss: 2.8978\n",
      "Resumed - Batch 696, Step 697, Loss: 3.0183\n",
      "Resumed - Batch 697, Step 698, Loss: 3.0309\n",
      "Resumed - Batch 698, Step 699, Loss: 3.0737\n",
      "Resumed - Batch 699, Step 700, Loss: 2.9365\n",
      "Resumed - Batch 700, Step 701, Loss: 3.0578\n",
      "Resumed - Batch 701, Step 702, Loss: 2.9629\n",
      "Resumed - Batch 702, Step 703, Loss: 2.7271\n",
      "Resumed - Batch 703, Step 704, Loss: 3.1406\n",
      "Resumed - Batch 704, Step 705, Loss: 2.9052\n",
      "Resumed - Batch 705, Step 706, Loss: 2.8975\n",
      "Resumed - Batch 706, Step 707, Loss: 2.8541\n",
      "Resumed - Batch 707, Step 708, Loss: 2.8351\n",
      "Resumed - Batch 708, Step 709, Loss: 3.1588\n",
      "Resumed - Batch 709, Step 710, Loss: 2.7469\n",
      "Resumed - Batch 710, Step 711, Loss: 2.9370\n",
      "Resumed - Batch 711, Step 712, Loss: 3.0214\n",
      "Resumed - Batch 712, Step 713, Loss: 3.0796\n",
      "Resumed - Batch 713, Step 714, Loss: 2.8546\n",
      "Resumed - Batch 714, Step 715, Loss: 2.9972\n",
      "Resumed - Batch 715, Step 716, Loss: 3.0130\n",
      "Resumed - Batch 716, Step 717, Loss: 2.8015\n",
      "Resumed - Batch 717, Step 718, Loss: 2.9691\n",
      "Resumed - Batch 718, Step 719, Loss: 3.0126\n",
      "Resumed - Batch 719, Step 720, Loss: 3.0819\n",
      "Resumed - Batch 720, Step 721, Loss: 2.7901\n",
      "Resumed - Batch 721, Step 722, Loss: 3.0563\n",
      "Resumed - Batch 722, Step 723, Loss: 2.8825\n",
      "Resumed - Batch 723, Step 724, Loss: 2.5811\n",
      "Resumed - Batch 724, Step 725, Loss: 2.8492\n",
      "Resumed - Batch 725, Step 726, Loss: 2.8357\n",
      "Resumed - Batch 726, Step 727, Loss: 2.8208\n",
      "Resumed - Batch 727, Step 728, Loss: 3.0651\n",
      "Resumed - Batch 728, Step 729, Loss: 3.1018\n",
      "Resumed - Batch 729, Step 730, Loss: 3.0138\n",
      "Resumed - Batch 730, Step 731, Loss: 2.7219\n",
      "Resumed - Batch 731, Step 732, Loss: 2.9189\n",
      "Resumed - Batch 732, Step 733, Loss: 2.7917\n",
      "Resumed - Batch 733, Step 734, Loss: 2.9384\n",
      "Resumed - Batch 734, Step 735, Loss: 3.1076\n",
      "Resumed - Batch 735, Step 736, Loss: 2.9140\n",
      "Resumed - Batch 736, Step 737, Loss: 2.5311\n",
      "Resumed - Batch 737, Step 738, Loss: 3.1056\n",
      "Resumed - Batch 738, Step 739, Loss: 2.9382\n",
      "Resumed - Batch 739, Step 740, Loss: 2.6569\n",
      "Resumed - Batch 740, Step 741, Loss: 2.6897\n",
      "Resumed - Batch 741, Step 742, Loss: 2.5971\n",
      "Resumed - Batch 742, Step 743, Loss: 3.0812\n",
      "Resumed - Batch 743, Step 744, Loss: 2.8108\n",
      "Resumed - Batch 744, Step 745, Loss: 3.3669\n",
      "Resumed - Batch 745, Step 746, Loss: 2.9607\n",
      "Resumed - Batch 746, Step 747, Loss: 2.9216\n",
      "Resumed - Batch 747, Step 748, Loss: 2.7426\n",
      "Resumed - Batch 748, Step 749, Loss: 2.9832\n",
      "Resumed - Batch 749, Step 750, Loss: 2.8365\n",
      "Resumed - Batch 750, Step 751, Loss: 3.1343\n",
      "Resumed - Batch 751, Step 752, Loss: 2.8696\n",
      "Resumed - Batch 752, Step 753, Loss: 2.8336\n",
      "Resumed - Batch 753, Step 754, Loss: 3.1129\n",
      "Resumed - Batch 754, Step 755, Loss: 3.0419\n",
      "Resumed - Batch 755, Step 756, Loss: 3.0882\n",
      "Resumed - Batch 756, Step 757, Loss: 2.9835\n",
      "Resumed - Batch 757, Step 758, Loss: 2.7538\n",
      "Resumed - Batch 758, Step 759, Loss: 3.1726\n",
      "Resumed - Batch 759, Step 760, Loss: 2.9192\n",
      "Resumed - Batch 760, Step 761, Loss: 3.2147\n",
      "Resumed - Batch 761, Step 762, Loss: 2.7846\n",
      "Resumed - Batch 762, Step 763, Loss: 2.6577\n",
      "Resumed - Batch 763, Step 764, Loss: 2.5434\n",
      "Resumed - Batch 764, Step 765, Loss: 2.8904\n",
      "Resumed - Batch 765, Step 766, Loss: 3.1299\n",
      "Resumed - Batch 766, Step 767, Loss: 2.5570\n",
      "Resumed - Batch 767, Step 768, Loss: 2.9898\n",
      "Resumed - Batch 768, Step 769, Loss: 2.9790\n",
      "Resumed - Batch 769, Step 770, Loss: 2.5309\n",
      "Resumed - Batch 770, Step 771, Loss: 2.7315\n",
      "Resumed - Batch 771, Step 772, Loss: 2.7860\n",
      "Resumed - Batch 772, Step 773, Loss: 2.9835\n",
      "Resumed - Batch 773, Step 774, Loss: 2.5360\n",
      "Resumed - Batch 774, Step 775, Loss: 2.8415\n",
      "Resumed - Batch 775, Step 776, Loss: 3.1075\n",
      "Resumed - Batch 776, Step 777, Loss: 2.7084\n",
      "Resumed - Batch 777, Step 778, Loss: 2.7125\n",
      "Resumed - Batch 778, Step 779, Loss: 3.0397\n",
      "Resumed - Batch 779, Step 780, Loss: 2.9361\n",
      "Resumed - Batch 780, Step 781, Loss: 2.8756\n",
      "Resumed - Batch 781, Step 782, Loss: 2.8330\n",
      "Resumed - Batch 782, Step 783, Loss: 2.9630\n",
      "Resumed - Batch 783, Step 784, Loss: 2.9620\n",
      "Resumed - Batch 784, Step 785, Loss: 2.8384\n",
      "Resumed - Batch 785, Step 786, Loss: 2.8781\n",
      "Resumed - Batch 786, Step 787, Loss: 3.2718\n",
      "Resumed - Batch 787, Step 788, Loss: 2.8910\n",
      "Resumed - Batch 788, Step 789, Loss: 2.6530\n",
      "Resumed - Batch 789, Step 790, Loss: 2.8986\n",
      "Resumed - Batch 790, Step 791, Loss: 2.9132\n",
      "Resumed - Batch 791, Step 792, Loss: 2.9602\n",
      "Resumed - Batch 792, Step 793, Loss: 2.7337\n",
      "Resumed - Batch 793, Step 794, Loss: 3.1004\n",
      "Resumed - Batch 794, Step 795, Loss: 2.8915\n",
      "Resumed - Batch 795, Step 796, Loss: 2.7635\n",
      "Resumed - Batch 796, Step 797, Loss: 3.0546\n",
      "Resumed - Batch 797, Step 798, Loss: 2.5664\n",
      "Resumed - Batch 798, Step 799, Loss: 2.9422\n",
      "Resumed - Batch 799, Step 800, Loss: 2.9037\n",
      "Resumed - Batch 800, Step 801, Loss: 2.6855\n",
      "Resumed - Batch 801, Step 802, Loss: 3.2285\n",
      "Resumed - Batch 802, Step 803, Loss: 3.0120\n",
      "Resumed - Batch 803, Step 804, Loss: 2.7480\n",
      "Resumed - Batch 804, Step 805, Loss: 2.7553\n",
      "Resumed - Batch 805, Step 806, Loss: 2.9796\n",
      "Resumed - Batch 806, Step 807, Loss: 3.0009\n",
      "Resumed - Batch 807, Step 808, Loss: 2.9505\n",
      "Resumed - Batch 808, Step 809, Loss: 2.9192\n",
      "Resumed - Batch 809, Step 810, Loss: 2.8957\n",
      "Resumed - Batch 810, Step 811, Loss: 3.2121\n",
      "Resumed - Batch 811, Step 812, Loss: 3.0986\n",
      "Resumed - Batch 812, Step 813, Loss: 3.3656\n",
      "Resumed - Batch 813, Step 814, Loss: 3.1320\n",
      "Resumed - Batch 814, Step 815, Loss: 3.0448\n",
      "Resumed - Batch 815, Step 816, Loss: 2.9517\n",
      "Resumed - Batch 816, Step 817, Loss: 2.8813\n",
      "Resumed - Batch 817, Step 818, Loss: 2.9209\n",
      "Resumed - Batch 818, Step 819, Loss: 2.9875\n",
      "Resumed - Batch 819, Step 820, Loss: 3.2619\n",
      "Resumed - Batch 820, Step 821, Loss: 3.0289\n",
      "Resumed - Batch 821, Step 822, Loss: 2.7858\n",
      "Resumed - Batch 822, Step 823, Loss: 2.5703\n",
      "Resumed - Batch 823, Step 824, Loss: 3.1448\n",
      "Resumed - Batch 824, Step 825, Loss: 2.9131\n",
      "Resumed - Batch 825, Step 826, Loss: 3.1843\n",
      "Resumed - Batch 826, Step 827, Loss: 2.5956\n",
      "Resumed - Batch 827, Step 828, Loss: 3.1841\n",
      "Resumed - Batch 828, Step 829, Loss: 2.9319\n",
      "Resumed - Batch 829, Step 830, Loss: 3.3414\n",
      "Resumed - Batch 830, Step 831, Loss: 2.8228\n",
      "Resumed - Batch 831, Step 832, Loss: 2.8483\n",
      "Resumed - Batch 832, Step 833, Loss: 3.0976\n",
      "Resumed - Batch 833, Step 834, Loss: 3.0067\n",
      "Resumed - Batch 834, Step 835, Loss: 3.2291\n",
      "Resumed - Batch 835, Step 836, Loss: 3.0226\n",
      "Resumed - Batch 836, Step 837, Loss: 3.1430\n",
      "Resumed - Batch 837, Step 838, Loss: 3.1372\n",
      "Resumed - Batch 838, Step 839, Loss: 2.5187\n",
      "Resumed - Batch 839, Step 840, Loss: 2.6833\n",
      "Resumed - Batch 840, Step 841, Loss: 2.9965\n",
      "Resumed - Batch 841, Step 842, Loss: 2.4187\n",
      "Resumed - Batch 842, Step 843, Loss: 2.7779\n",
      "Resumed - Batch 843, Step 844, Loss: 2.9383\n",
      "Resumed - Batch 844, Step 845, Loss: 3.0558\n",
      "Resumed - Batch 845, Step 846, Loss: 2.6783\n",
      "Resumed - Batch 846, Step 847, Loss: 2.7366\n",
      "Resumed - Batch 847, Step 848, Loss: 2.9502\n",
      "Resumed - Batch 848, Step 849, Loss: 3.1882\n",
      "Resumed - Batch 849, Step 850, Loss: 2.9171\n",
      "Resumed - Batch 850, Step 851, Loss: 3.1762\n",
      "Resumed - Batch 851, Step 852, Loss: 3.0873\n",
      "Resumed - Batch 852, Step 853, Loss: 3.2491\n",
      "Resumed - Batch 853, Step 854, Loss: 2.8076\n",
      "Resumed - Batch 854, Step 855, Loss: 3.0028\n",
      "Resumed - Batch 855, Step 856, Loss: 2.9539\n",
      "Resumed - Batch 856, Step 857, Loss: 3.0723\n",
      "Resumed - Batch 857, Step 858, Loss: 2.4162\n",
      "Resumed - Batch 858, Step 859, Loss: 3.1127\n",
      "Resumed - Batch 859, Step 860, Loss: 3.1726\n",
      "Resumed - Batch 860, Step 861, Loss: 3.4442\n",
      "Resumed - Batch 861, Step 862, Loss: 2.4805\n",
      "Resumed - Batch 862, Step 863, Loss: 3.0464\n",
      "Resumed - Batch 863, Step 864, Loss: 3.1052\n",
      "Resumed - Batch 864, Step 865, Loss: 3.0800\n",
      "Resumed - Batch 865, Step 866, Loss: 3.0395\n",
      "Resumed - Batch 866, Step 867, Loss: 2.8602\n",
      "Resumed - Batch 867, Step 868, Loss: 2.5551\n",
      "Resumed - Batch 868, Step 869, Loss: 2.8168\n",
      "Resumed - Batch 869, Step 870, Loss: 2.7894\n",
      "Resumed - Batch 870, Step 871, Loss: 3.0689\n",
      "Resumed - Batch 871, Step 872, Loss: 2.9178\n",
      "Resumed - Batch 872, Step 873, Loss: 2.7528\n",
      "Resumed - Batch 873, Step 874, Loss: 2.7954\n",
      "Resumed - Batch 874, Step 875, Loss: 2.8778\n",
      "Resumed - Batch 875, Step 876, Loss: 2.6240\n",
      "Resumed - Batch 876, Step 877, Loss: 2.6795\n",
      "Resumed - Batch 877, Step 878, Loss: 3.1308\n",
      "Resumed - Batch 878, Step 879, Loss: 3.0950\n",
      "Resumed - Batch 879, Step 880, Loss: 2.9840\n",
      "Resumed - Batch 880, Step 881, Loss: 2.6553\n",
      "Resumed - Batch 881, Step 882, Loss: 2.8948\n",
      "Resumed - Batch 882, Step 883, Loss: 3.1173\n",
      "Resumed - Batch 883, Step 884, Loss: 3.1000\n",
      "Resumed - Batch 884, Step 885, Loss: 2.6814\n",
      "Resumed - Batch 885, Step 886, Loss: 2.7847\n",
      "Resumed - Batch 886, Step 887, Loss: 2.7470\n",
      "Resumed - Batch 887, Step 888, Loss: 3.2788\n",
      "Resumed - Batch 888, Step 889, Loss: 2.8867\n",
      "Resumed - Batch 889, Step 890, Loss: 2.5748\n",
      "Resumed - Batch 890, Step 891, Loss: 2.9286\n",
      "Resumed - Batch 891, Step 892, Loss: 2.9029\n",
      "Resumed - Batch 892, Step 893, Loss: 3.1526\n",
      "Resumed - Batch 893, Step 894, Loss: 3.0150\n",
      "Resumed - Batch 894, Step 895, Loss: 2.9836\n",
      "Resumed - Batch 895, Step 896, Loss: 2.6316\n",
      "Resumed - Batch 896, Step 897, Loss: 3.1836\n",
      "Resumed - Batch 897, Step 898, Loss: 2.6683\n",
      "Resumed - Batch 898, Step 899, Loss: 2.9860\n",
      "Resumed - Batch 899, Step 900, Loss: 2.7753\n",
      "Resumed - Batch 900, Step 901, Loss: 3.0458\n",
      "Resumed - Batch 901, Step 902, Loss: 2.7750\n",
      "Resumed - Batch 902, Step 903, Loss: 2.9195\n",
      "Resumed - Batch 903, Step 904, Loss: 2.9084\n",
      "Resumed - Batch 904, Step 905, Loss: 2.2126\n",
      "Resumed - Batch 905, Step 906, Loss: 3.0633\n",
      "Resumed - Batch 906, Step 907, Loss: 2.9978\n",
      "Resumed - Batch 907, Step 908, Loss: 2.7872\n",
      "Resumed - Batch 908, Step 909, Loss: 2.8493\n",
      "Resumed - Batch 909, Step 910, Loss: 2.7157\n",
      "Resumed - Batch 910, Step 911, Loss: 2.4315\n",
      "Resumed - Batch 911, Step 912, Loss: 2.5340\n",
      "Resumed - Batch 912, Step 913, Loss: 3.0450\n",
      "Resumed - Batch 913, Step 914, Loss: 2.8028\n",
      "Resumed - Batch 914, Step 915, Loss: 3.0500\n",
      "Resumed - Batch 915, Step 916, Loss: 2.5935\n",
      "Resumed - Batch 916, Step 917, Loss: 3.1258\n",
      "Resumed - Batch 917, Step 918, Loss: 3.0876\n",
      "Resumed - Batch 918, Step 919, Loss: 2.7473\n",
      "Resumed - Batch 919, Step 920, Loss: 3.0994\n",
      "Resumed - Batch 920, Step 921, Loss: 3.0746\n",
      "Resumed - Batch 921, Step 922, Loss: 3.1794\n",
      "Resumed - Batch 922, Step 923, Loss: 3.2258\n",
      "Resumed - Batch 923, Step 924, Loss: 2.9370\n",
      "Resumed - Batch 924, Step 925, Loss: 2.7549\n",
      "Resumed - Batch 925, Step 926, Loss: 2.8844\n",
      "Resumed - Batch 926, Step 927, Loss: 2.6236\n",
      "Resumed - Batch 927, Step 928, Loss: 2.5315\n",
      "Resumed - Batch 928, Step 929, Loss: 2.7161\n",
      "Resumed - Batch 929, Step 930, Loss: 2.8618\n",
      "Resumed - Batch 930, Step 931, Loss: 2.8437\n",
      "Resumed - Batch 931, Step 932, Loss: 2.9076\n",
      "Resumed - Batch 932, Step 933, Loss: 3.1379\n",
      "Resumed - Batch 933, Step 934, Loss: 2.6559\n",
      "Resumed - Batch 934, Step 935, Loss: 3.0307\n",
      "Resumed - Batch 935, Step 936, Loss: 2.7032\n",
      "Resumed - Batch 936, Step 937, Loss: 3.0846\n",
      "Resumed - Batch 937, Step 938, Loss: 2.8152\n",
      "Resumed - Batch 938, Step 939, Loss: 2.6443\n",
      "Resumed - Batch 939, Step 940, Loss: 2.9808\n",
      "Resumed - Batch 940, Step 941, Loss: 2.6090\n",
      "Resumed - Batch 941, Step 942, Loss: 3.1303\n",
      "Resumed - Batch 942, Step 943, Loss: 2.7865\n",
      "Resumed - Batch 943, Step 944, Loss: 3.1098\n",
      "Resumed - Batch 944, Step 945, Loss: 2.6600\n",
      "Resumed - Batch 945, Step 946, Loss: 3.1069\n",
      "Resumed - Batch 946, Step 947, Loss: 3.1444\n",
      "Resumed - Batch 947, Step 948, Loss: 3.0432\n",
      "Resumed - Batch 948, Step 949, Loss: 2.5434\n",
      "Resumed - Batch 949, Step 950, Loss: 2.7522\n",
      "Resumed - Batch 950, Step 951, Loss: 2.4570\n",
      "Resumed - Batch 951, Step 952, Loss: 2.6683\n",
      "Resumed - Batch 952, Step 953, Loss: 2.4937\n",
      "Resumed - Batch 953, Step 954, Loss: 2.7955\n",
      "Resumed - Batch 954, Step 955, Loss: 2.6972\n",
      "Resumed - Batch 955, Step 956, Loss: 2.6349\n",
      "Resumed - Batch 956, Step 957, Loss: 2.9578\n",
      "Resumed - Batch 957, Step 958, Loss: 3.0462\n",
      "Resumed - Batch 958, Step 959, Loss: 2.5426\n",
      "Resumed - Batch 959, Step 960, Loss: 2.7335\n",
      "Resumed - Batch 960, Step 961, Loss: 2.9815\n",
      "Resumed - Batch 961, Step 962, Loss: 2.6841\n",
      "Resumed - Batch 962, Step 963, Loss: 2.6747\n",
      "Resumed - Batch 963, Step 964, Loss: 2.8052\n",
      "Resumed - Batch 964, Step 965, Loss: 2.7087\n",
      "Resumed - Batch 965, Step 966, Loss: 2.9708\n",
      "Resumed - Batch 966, Step 967, Loss: 3.4016\n",
      "Resumed - Batch 967, Step 968, Loss: 2.7839\n",
      "Resumed - Batch 968, Step 969, Loss: 2.8156\n",
      "Resumed - Batch 969, Step 970, Loss: 3.0493\n",
      "Resumed - Batch 970, Step 971, Loss: 2.7085\n",
      "Resumed - Batch 971, Step 972, Loss: 3.2760\n",
      "Resumed - Batch 972, Step 973, Loss: 2.7441\n",
      "Resumed - Batch 973, Step 974, Loss: 2.9335\n",
      "Resumed - Batch 974, Step 975, Loss: 2.5964\n",
      "Resumed - Batch 975, Step 976, Loss: 2.7956\n",
      "Resumed - Batch 976, Step 977, Loss: 2.7334\n",
      "Resumed - Batch 977, Step 978, Loss: 2.8725\n",
      "Resumed - Batch 978, Step 979, Loss: 2.6777\n",
      "Resumed - Batch 979, Step 980, Loss: 2.8800\n",
      "Resumed - Batch 980, Step 981, Loss: 2.9077\n",
      "Resumed - Batch 981, Step 982, Loss: 2.5275\n",
      "Resumed - Batch 982, Step 983, Loss: 2.6958\n",
      "Resumed - Batch 983, Step 984, Loss: 2.7348\n",
      "Resumed - Batch 984, Step 985, Loss: 2.8924\n",
      "Resumed - Batch 985, Step 986, Loss: 2.6558\n",
      "Resumed - Batch 986, Step 987, Loss: 2.7357\n",
      "Resumed - Batch 987, Step 988, Loss: 2.6503\n",
      "Resumed - Batch 988, Step 989, Loss: 3.0523\n",
      "Resumed - Batch 989, Step 990, Loss: 3.0385\n",
      "Resumed - Batch 990, Step 991, Loss: 2.9368\n",
      "Resumed - Batch 991, Step 992, Loss: 2.8111\n",
      "Resumed - Batch 992, Step 993, Loss: 2.5366\n",
      "Resumed - Batch 993, Step 994, Loss: 3.2854\n",
      "Resumed - Batch 994, Step 995, Loss: 2.5685\n",
      "Resumed - Batch 995, Step 996, Loss: 3.0105\n",
      "Resumed - Batch 996, Step 997, Loss: 3.1396\n",
      "Resumed - Batch 997, Step 998, Loss: 2.7200\n",
      "Resumed - Batch 998, Step 999, Loss: 2.7079\n",
      "Resumed - Batch 999, Step 1000, Loss: 2.7446\n",
      "Resumed - Batch 1000, Step 1001, Loss: 3.4835\n",
      "Resumed - Batch 1001, Step 1002, Loss: 2.8624\n",
      "Resumed - Batch 1002, Step 1003, Loss: 3.5077\n",
      "Resumed - Batch 1003, Step 1004, Loss: 2.4981\n",
      "Resumed - Batch 1004, Step 1005, Loss: 3.0157\n",
      "Resumed - Batch 1005, Step 1006, Loss: 2.5966\n",
      "Resumed - Batch 1006, Step 1007, Loss: 2.8706\n",
      "Resumed - Batch 1007, Step 1008, Loss: 2.8786\n",
      "Resumed - Batch 1008, Step 1009, Loss: 2.4455\n",
      "Resumed - Batch 1009, Step 1010, Loss: 2.8155\n",
      "Resumed - Batch 1010, Step 1011, Loss: 3.1106\n",
      "Resumed - Batch 1011, Step 1012, Loss: 2.6265\n",
      "Resumed - Batch 1012, Step 1013, Loss: 2.8702\n",
      "Resumed - Batch 1013, Step 1014, Loss: 3.0066\n",
      "Resumed - Batch 1014, Step 1015, Loss: 2.7393\n",
      "Resumed - Batch 1015, Step 1016, Loss: 2.7729\n",
      "Resumed - Batch 1016, Step 1017, Loss: 3.1685\n",
      "Resumed - Batch 1017, Step 1018, Loss: 3.1722\n",
      "Resumed - Batch 1018, Step 1019, Loss: 2.7582\n",
      "Resumed - Batch 1019, Step 1020, Loss: 2.7272\n",
      "Resumed - Batch 1020, Step 1021, Loss: 2.8938\n",
      "Resumed - Batch 1021, Step 1022, Loss: 2.7158\n",
      "Resumed - Batch 1022, Step 1023, Loss: 2.7537\n",
      "Resumed - Batch 1023, Step 1024, Loss: 2.6452\n",
      "Resumed - Batch 1024, Step 1025, Loss: 3.4215\n",
      "Resumed - Batch 1025, Step 1026, Loss: 2.9055\n",
      "Resumed - Batch 1026, Step 1027, Loss: 2.8235\n",
      "Resumed - Batch 1027, Step 1028, Loss: 2.7423\n",
      "Resumed - Batch 1028, Step 1029, Loss: 3.0475\n",
      "Resumed - Batch 1029, Step 1030, Loss: 2.8524\n",
      "Resumed - Batch 1030, Step 1031, Loss: 2.6499\n",
      "Resumed - Batch 1031, Step 1032, Loss: 2.9773\n",
      "Resumed - Batch 1032, Step 1033, Loss: 2.7111\n",
      "Resumed - Batch 1033, Step 1034, Loss: 2.4787\n",
      "Resumed - Batch 1034, Step 1035, Loss: 3.0209\n",
      "Resumed - Batch 1035, Step 1036, Loss: 3.1145\n",
      "Resumed - Batch 1036, Step 1037, Loss: 2.8513\n",
      "Resumed - Batch 1037, Step 1038, Loss: 3.3073\n",
      "Resumed - Batch 1038, Step 1039, Loss: 2.5793\n",
      "Resumed - Batch 1039, Step 1040, Loss: 2.4279\n",
      "Resumed - Batch 1040, Step 1041, Loss: 3.0388\n",
      "Resumed - Batch 1041, Step 1042, Loss: 2.7664\n",
      "Resumed - Batch 1042, Step 1043, Loss: 2.9401\n",
      "Resumed - Batch 1043, Step 1044, Loss: 2.9594\n",
      "Resumed - Batch 1044, Step 1045, Loss: 2.5083\n",
      "Resumed - Batch 1045, Step 1046, Loss: 2.6043\n",
      "Resumed - Batch 1046, Step 1047, Loss: 2.6495\n",
      "Resumed - Batch 1047, Step 1048, Loss: 2.9251\n",
      "Resumed - Batch 1048, Step 1049, Loss: 2.8574\n",
      "Resumed - Batch 1049, Step 1050, Loss: 3.0063\n",
      "Resumed - Batch 1050, Step 1051, Loss: 2.9140\n",
      "Resumed - Batch 1051, Step 1052, Loss: 2.5196\n",
      "Resumed - Batch 1052, Step 1053, Loss: 2.7715\n",
      "Resumed - Batch 1053, Step 1054, Loss: 2.7530\n",
      "Resumed - Batch 1054, Step 1055, Loss: 2.6629\n",
      "Resumed - Batch 1055, Step 1056, Loss: 2.9644\n",
      "Resumed - Batch 1056, Step 1057, Loss: 3.0286\n",
      "Resumed - Batch 1057, Step 1058, Loss: 2.7175\n",
      "Resumed - Batch 1058, Step 1059, Loss: 2.8384\n",
      "Resumed - Batch 1059, Step 1060, Loss: 3.1432\n",
      "Resumed - Batch 1060, Step 1061, Loss: 2.9377\n",
      "Resumed - Batch 1061, Step 1062, Loss: 2.7400\n",
      "Resumed - Batch 1062, Step 1063, Loss: 3.1247\n",
      "Resumed - Batch 1063, Step 1064, Loss: 2.8961\n",
      "Resumed - Batch 1064, Step 1065, Loss: 3.2889\n",
      "Resumed - Batch 1065, Step 1066, Loss: 3.0374\n",
      "Resumed - Batch 1066, Step 1067, Loss: 2.7310\n",
      "Resumed - Batch 1067, Step 1068, Loss: 2.4495\n",
      "Resumed - Batch 1068, Step 1069, Loss: 3.1365\n",
      "Resumed - Batch 1069, Step 1070, Loss: 2.8040\n",
      "Resumed - Batch 1070, Step 1071, Loss: 2.7872\n",
      "Resumed - Batch 1071, Step 1072, Loss: 3.0916\n",
      "Resumed - Batch 1072, Step 1073, Loss: 2.8255\n",
      "Resumed - Batch 1073, Step 1074, Loss: 2.6853\n",
      "Resumed - Batch 1074, Step 1075, Loss: 3.1573\n",
      "Resumed - Batch 1075, Step 1076, Loss: 2.7609\n",
      "Resumed - Batch 1076, Step 1077, Loss: 2.9942\n",
      "Resumed - Batch 1077, Step 1078, Loss: 2.9240\n",
      "Resumed - Batch 1078, Step 1079, Loss: 2.3819\n",
      "Resumed - Batch 1079, Step 1080, Loss: 3.1691\n",
      "Resumed - Batch 1080, Step 1081, Loss: 3.5082\n",
      "Resumed - Batch 1081, Step 1082, Loss: 3.0423\n",
      "Resumed - Batch 1082, Step 1083, Loss: 2.9018\n",
      "Resumed - Batch 1083, Step 1084, Loss: 2.6365\n",
      "Resumed - Batch 1084, Step 1085, Loss: 2.8835\n",
      "Resumed - Batch 1085, Step 1086, Loss: 2.1501\n",
      "Resumed - Batch 1086, Step 1087, Loss: 2.9122\n",
      "Resumed - Batch 1087, Step 1088, Loss: 2.7679\n",
      "Resumed - Batch 1088, Step 1089, Loss: 3.1139\n",
      "Resumed - Batch 1089, Step 1090, Loss: 2.6148\n",
      "Resumed - Batch 1090, Step 1091, Loss: 2.5097\n",
      "Resumed - Batch 1091, Step 1092, Loss: 2.5255\n",
      "Resumed - Batch 1092, Step 1093, Loss: 2.8706\n",
      "Resumed - Batch 1093, Step 1094, Loss: 2.8887\n",
      "Resumed - Batch 1094, Step 1095, Loss: 2.8590\n",
      "Resumed - Batch 1095, Step 1096, Loss: 2.7932\n",
      "Resumed - Batch 1096, Step 1097, Loss: 2.9970\n",
      "Resumed - Batch 1097, Step 1098, Loss: 2.7127\n",
      "Resumed - Batch 1098, Step 1099, Loss: 2.7610\n",
      "Resumed - Batch 1099, Step 1100, Loss: 3.0692\n",
      "Resumed - Batch 1100, Step 1101, Loss: 2.7827\n",
      "Resumed - Batch 1101, Step 1102, Loss: 2.7190\n",
      "Resumed - Batch 1102, Step 1103, Loss: 2.9074\n",
      "Resumed - Batch 1103, Step 1104, Loss: 2.9131\n",
      "Resumed - Batch 1104, Step 1105, Loss: 2.4394\n",
      "Resumed - Batch 1105, Step 1106, Loss: 3.0533\n",
      "Resumed - Batch 1106, Step 1107, Loss: 3.2898\n",
      "Resumed - Batch 1107, Step 1108, Loss: 3.1743\n",
      "Resumed - Batch 1108, Step 1109, Loss: 2.6286\n",
      "Resumed - Batch 1109, Step 1110, Loss: 2.7459\n",
      "Resumed - Batch 1110, Step 1111, Loss: 2.8018\n",
      "Resumed - Batch 1111, Step 1112, Loss: 2.4263\n",
      "Resumed - Batch 1112, Step 1113, Loss: 2.9844\n",
      "Resumed - Batch 1113, Step 1114, Loss: 2.7291\n",
      "Resumed - Batch 1114, Step 1115, Loss: 2.8039\n",
      "Resumed - Batch 1115, Step 1116, Loss: 3.3759\n",
      "Resumed - Batch 1116, Step 1117, Loss: 2.8787\n",
      "Resumed - Batch 1117, Step 1118, Loss: 3.2396\n",
      "Resumed - Batch 1118, Step 1119, Loss: 3.0186\n",
      "Resumed - Batch 1119, Step 1120, Loss: 2.6111\n",
      "Resumed - Batch 1120, Step 1121, Loss: 2.8182\n",
      "Resumed - Batch 1121, Step 1122, Loss: 2.6201\n",
      "Resumed - Batch 1122, Step 1123, Loss: 2.5012\n",
      "Resumed - Batch 1123, Step 1124, Loss: 2.8538\n",
      "Resumed - Batch 1124, Step 1125, Loss: 2.4549\n",
      "Resumed - Batch 1125, Step 1126, Loss: 2.8294\n",
      "Resumed - Batch 1126, Step 1127, Loss: 3.0981\n",
      "Resumed - Batch 1127, Step 1128, Loss: 2.7594\n",
      "Resumed - Batch 1128, Step 1129, Loss: 2.5843\n",
      "Resumed - Batch 1129, Step 1130, Loss: 2.7634\n",
      "Resumed - Batch 1130, Step 1131, Loss: 3.2043\n",
      "Resumed - Batch 1131, Step 1132, Loss: 2.7245\n",
      "Resumed - Batch 1132, Step 1133, Loss: 2.9007\n",
      "Resumed - Batch 1133, Step 1134, Loss: 2.4627\n",
      "Resumed - Batch 1134, Step 1135, Loss: 2.8136\n",
      "Resumed - Batch 1135, Step 1136, Loss: 2.7335\n",
      "Resumed - Batch 1136, Step 1137, Loss: 2.8732\n",
      "Resumed - Batch 1137, Step 1138, Loss: 3.1514\n",
      "Resumed - Batch 1138, Step 1139, Loss: 2.5315\n",
      "Resumed - Batch 1139, Step 1140, Loss: 2.9304\n",
      "Resumed - Batch 1140, Step 1141, Loss: 2.8910\n",
      "Resumed - Batch 1141, Step 1142, Loss: 3.0345\n",
      "Resumed - Batch 1142, Step 1143, Loss: 3.0050\n",
      "Resumed - Batch 1143, Step 1144, Loss: 2.6855\n",
      "Resumed - Batch 1144, Step 1145, Loss: 2.9593\n",
      "Resumed - Batch 1145, Step 1146, Loss: 2.7159\n",
      "Resumed - Batch 1146, Step 1147, Loss: 3.1168\n",
      "Resumed - Batch 1147, Step 1148, Loss: 2.6766\n",
      "Resumed - Batch 1148, Step 1149, Loss: 2.9235\n",
      "Resumed - Batch 1149, Step 1150, Loss: 2.8536\n",
      "Resumed - Batch 1150, Step 1151, Loss: 2.5487\n",
      "Resumed - Batch 1151, Step 1152, Loss: 2.5817\n",
      "Resumed - Batch 1152, Step 1153, Loss: 3.1522\n",
      "Resumed - Batch 1153, Step 1154, Loss: 2.5553\n",
      "Resumed - Batch 1154, Step 1155, Loss: 3.1173\n",
      "Resumed - Batch 1155, Step 1156, Loss: 3.0445\n",
      "Resumed - Batch 1156, Step 1157, Loss: 2.5724\n",
      "Resumed - Batch 1157, Step 1158, Loss: 2.8252\n",
      "Resumed - Batch 1158, Step 1159, Loss: 2.6962\n",
      "Resumed - Batch 1159, Step 1160, Loss: 2.8480\n",
      "Resumed - Batch 1160, Step 1161, Loss: 2.3013\n",
      "Resumed - Batch 1161, Step 1162, Loss: 2.9985\n",
      "Resumed - Batch 1162, Step 1163, Loss: 3.2947\n",
      "Resumed - Batch 1163, Step 1164, Loss: 2.6575\n",
      "Resumed - Batch 1164, Step 1165, Loss: 2.7605\n",
      "Resumed - Batch 1165, Step 1166, Loss: 2.6248\n",
      "Resumed - Batch 1166, Step 1167, Loss: 2.9409\n",
      "Resumed - Batch 1167, Step 1168, Loss: 2.8484\n",
      "Resumed - Batch 1168, Step 1169, Loss: 2.4857\n",
      "Resumed - Batch 1169, Step 1170, Loss: 2.6708\n",
      "Resumed - Batch 1170, Step 1171, Loss: 2.7080\n",
      "Resumed - Batch 1171, Step 1172, Loss: 2.8111\n",
      "Resumed - Batch 1172, Step 1173, Loss: 2.6719\n",
      "Resumed - Batch 1173, Step 1174, Loss: 2.6086\n",
      "Resumed - Batch 1174, Step 1175, Loss: 2.7353\n",
      "Resumed - Batch 1175, Step 1176, Loss: 3.0520\n",
      "Resumed - Batch 1176, Step 1177, Loss: 3.0146\n",
      "Resumed - Batch 1177, Step 1178, Loss: 2.9070\n",
      "Resumed - Batch 1178, Step 1179, Loss: 3.0442\n",
      "Resumed - Batch 1179, Step 1180, Loss: 3.1544\n",
      "Resumed - Batch 1180, Step 1181, Loss: 2.9354\n",
      "Resumed - Batch 1181, Step 1182, Loss: 2.6013\n",
      "Resumed - Batch 1182, Step 1183, Loss: 2.9230\n",
      "Resumed - Batch 1183, Step 1184, Loss: 2.8935\n",
      "Resumed - Batch 1184, Step 1185, Loss: 3.2523\n",
      "Resumed - Batch 1185, Step 1186, Loss: 2.2011\n",
      "Resumed - Batch 1186, Step 1187, Loss: 2.6780\n",
      "Resumed - Batch 1187, Step 1188, Loss: 2.7558\n",
      "Resumed - Batch 1188, Step 1189, Loss: 2.9728\n",
      "Resumed - Batch 1189, Step 1190, Loss: 2.8662\n",
      "Resumed - Batch 1190, Step 1191, Loss: 2.8303\n",
      "Resumed - Batch 1191, Step 1192, Loss: 2.8674\n",
      "Resumed - Batch 1192, Step 1193, Loss: 2.7329\n",
      "Resumed - Batch 1193, Step 1194, Loss: 2.6086\n",
      "Resumed - Batch 1194, Step 1195, Loss: 2.8480\n",
      "Resumed - Batch 1195, Step 1196, Loss: 2.7984\n",
      "Resumed - Batch 1196, Step 1197, Loss: 2.8969\n",
      "Resumed - Batch 1197, Step 1198, Loss: 2.7282\n",
      "Resumed - Batch 1198, Step 1199, Loss: 2.7335\n",
      "Resumed - Batch 1199, Step 1200, Loss: 2.6667\n",
      "Resumed - Batch 1200, Step 1201, Loss: 2.4405\n",
      "Resumed - Batch 1201, Step 1202, Loss: 2.6213\n",
      "Resumed - Batch 1202, Step 1203, Loss: 2.7175\n",
      "Resumed - Batch 1203, Step 1204, Loss: 3.0357\n",
      "Resumed - Batch 1204, Step 1205, Loss: 2.7289\n",
      "Resumed - Batch 1205, Step 1206, Loss: 2.5442\n",
      "Resumed - Batch 1206, Step 1207, Loss: 3.1746\n",
      "Resumed - Batch 1207, Step 1208, Loss: 2.6530\n",
      "Resumed - Batch 1208, Step 1209, Loss: 2.5025\n",
      "Resumed - Batch 1209, Step 1210, Loss: 3.0276\n",
      "Resumed - Batch 1210, Step 1211, Loss: 3.0627\n",
      "Resumed - Batch 1211, Step 1212, Loss: 3.0284\n",
      "Resumed - Batch 1212, Step 1213, Loss: 2.8300\n",
      "Resumed - Batch 1213, Step 1214, Loss: 2.8501\n",
      "Resumed - Batch 1214, Step 1215, Loss: 2.9510\n",
      "Resumed - Batch 1215, Step 1216, Loss: 2.6361\n",
      "Resumed - Batch 1216, Step 1217, Loss: 2.8941\n",
      "Resumed - Batch 1217, Step 1218, Loss: 2.7057\n",
      "Resumed - Batch 1218, Step 1219, Loss: 2.5947\n",
      "Resumed - Batch 1219, Step 1220, Loss: 2.9548\n",
      "Resumed - Batch 1220, Step 1221, Loss: 2.9319\n",
      "Resumed - Batch 1221, Step 1222, Loss: 2.7642\n",
      "Resumed - Batch 1222, Step 1223, Loss: 3.0412\n",
      "Resumed - Batch 1223, Step 1224, Loss: 3.0026\n",
      "Resumed - Batch 1224, Step 1225, Loss: 2.6499\n",
      "Resumed - Batch 1225, Step 1226, Loss: 2.8731\n",
      "Resumed - Batch 1226, Step 1227, Loss: 2.6818\n",
      "Resumed - Batch 1227, Step 1228, Loss: 2.8598\n",
      "Resumed - Batch 1228, Step 1229, Loss: 2.9463\n",
      "Resumed - Batch 1229, Step 1230, Loss: 3.1258\n",
      "Resumed - Batch 1230, Step 1231, Loss: 2.6636\n",
      "Resumed - Batch 1231, Step 1232, Loss: 2.8539\n",
      "Resumed - Batch 1232, Step 1233, Loss: 2.7494\n",
      "Resumed - Batch 1233, Step 1234, Loss: 2.7216\n",
      "Resumed - Batch 1234, Step 1235, Loss: 2.7116\n",
      "Resumed - Batch 1235, Step 1236, Loss: 2.6174\n",
      "Resumed - Batch 1236, Step 1237, Loss: 2.5416\n",
      "Resumed - Batch 1237, Step 1238, Loss: 2.5463\n",
      "Resumed - Batch 1238, Step 1239, Loss: 2.7540\n",
      "Resumed - Batch 1239, Step 1240, Loss: 2.5140\n",
      "Resumed - Batch 1240, Step 1241, Loss: 2.6451\n",
      "Resumed - Batch 1241, Step 1242, Loss: 2.4713\n",
      "Resumed - Batch 1242, Step 1243, Loss: 2.6994\n",
      "Resumed - Batch 1243, Step 1244, Loss: 2.6031\n",
      "Resumed - Batch 1244, Step 1245, Loss: 2.4909\n",
      "Resumed - Batch 1245, Step 1246, Loss: 2.7376\n",
      "Resumed - Batch 1246, Step 1247, Loss: 2.7072\n",
      "Resumed - Batch 1247, Step 1248, Loss: 2.5893\n",
      "Resumed - Batch 1248, Step 1249, Loss: 2.5132\n",
      "Resumed - Batch 1249, Step 1250, Loss: 2.9026\n",
      "Resumed - Batch 1250, Step 1251, Loss: 2.7543\n",
      "Resumed - Batch 1251, Step 1252, Loss: 2.7266\n",
      "Resumed - Batch 1252, Step 1253, Loss: 2.9961\n",
      "Resumed - Batch 1253, Step 1254, Loss: 2.7020\n",
      "Resumed - Batch 1254, Step 1255, Loss: 2.7772\n",
      "Resumed - Batch 1255, Step 1256, Loss: 2.6731\n",
      "Resumed - Batch 1256, Step 1257, Loss: 2.9034\n",
      "Resumed - Batch 1257, Step 1258, Loss: 2.6445\n",
      "Resumed - Batch 1258, Step 1259, Loss: 2.9029\n",
      "Resumed - Batch 1259, Step 1260, Loss: 2.4799\n",
      "Resumed - Batch 1260, Step 1261, Loss: 2.9846\n",
      "Resumed - Batch 1261, Step 1262, Loss: 3.0926\n",
      "Resumed - Batch 1262, Step 1263, Loss: 2.8814\n",
      "Resumed - Batch 1263, Step 1264, Loss: 2.6790\n",
      "Resumed - Batch 1264, Step 1265, Loss: 3.1126\n",
      "Resumed - Batch 1265, Step 1266, Loss: 2.7254\n",
      "Resumed - Batch 1266, Step 1267, Loss: 2.8331\n",
      "Resumed - Batch 1267, Step 1268, Loss: 2.8274\n",
      "Resumed - Batch 1268, Step 1269, Loss: 2.5263\n",
      "Resumed - Batch 1269, Step 1270, Loss: 2.7319\n",
      "Resumed - Batch 1270, Step 1271, Loss: 3.3604\n",
      "Resumed - Batch 1271, Step 1272, Loss: 2.9029\n",
      "Resumed - Batch 1272, Step 1273, Loss: 2.6931\n",
      "Resumed - Batch 1273, Step 1274, Loss: 2.8275\n",
      "Resumed - Batch 1274, Step 1275, Loss: 2.8546\n",
      "Resumed - Batch 1275, Step 1276, Loss: 2.8192\n",
      "Resumed - Batch 1276, Step 1277, Loss: 3.1464\n",
      "Resumed - Batch 1277, Step 1278, Loss: 3.0224\n",
      "Resumed - Batch 1278, Step 1279, Loss: 2.7213\n",
      "Resumed - Batch 1279, Step 1280, Loss: 2.6977\n",
      "Resumed - Batch 1280, Step 1281, Loss: 2.6108\n",
      "Resumed - Batch 1281, Step 1282, Loss: 2.8752\n",
      "Resumed - Batch 1282, Step 1283, Loss: 2.7517\n",
      "Resumed - Batch 1283, Step 1284, Loss: 2.6840\n",
      "Resumed - Batch 1284, Step 1285, Loss: 2.6371\n",
      "Resumed - Batch 1285, Step 1286, Loss: 2.6031\n",
      "Resumed - Batch 1286, Step 1287, Loss: 2.7572\n",
      "Resumed - Batch 1287, Step 1288, Loss: 2.8924\n",
      "Resumed - Batch 1288, Step 1289, Loss: 3.0032\n",
      "Resumed - Batch 1289, Step 1290, Loss: 2.8483\n",
      "Resumed - Batch 1290, Step 1291, Loss: 2.9699\n",
      "Resumed - Batch 1291, Step 1292, Loss: 2.6972\n",
      "Resumed - Batch 1292, Step 1293, Loss: 2.6860\n",
      "Resumed - Batch 1293, Step 1294, Loss: 2.7526\n",
      "Resumed - Batch 1294, Step 1295, Loss: 2.8264\n",
      "Resumed - Batch 1295, Step 1296, Loss: 2.9010\n",
      "Resumed - Batch 1296, Step 1297, Loss: 2.7221\n",
      "Resumed - Batch 1297, Step 1298, Loss: 3.0032\n",
      "Resumed - Batch 1298, Step 1299, Loss: 2.6046\n",
      "Resumed - Batch 1299, Step 1300, Loss: 2.6308\n",
      "Resumed - Batch 1300, Step 1301, Loss: 2.8504\n",
      "Resumed - Batch 1301, Step 1302, Loss: 2.9857\n",
      "Resumed - Batch 1302, Step 1303, Loss: 2.5685\n",
      "Resumed - Batch 1303, Step 1304, Loss: 2.9870\n",
      "Resumed - Batch 1304, Step 1305, Loss: 2.9727\n",
      "Resumed - Batch 1305, Step 1306, Loss: 2.6913\n",
      "Resumed - Batch 1306, Step 1307, Loss: 3.3865\n",
      "Resumed - Batch 1307, Step 1308, Loss: 3.0146\n",
      "Resumed - Batch 1308, Step 1309, Loss: 2.7604\n",
      "Resumed - Batch 1309, Step 1310, Loss: 2.3006\n",
      "Resumed - Batch 1310, Step 1311, Loss: 2.8650\n",
      "Resumed - Batch 1311, Step 1312, Loss: 2.7027\n",
      "Resumed - Batch 1312, Step 1313, Loss: 2.4848\n",
      "Resumed - Batch 1313, Step 1314, Loss: 2.4613\n",
      "Resumed - Batch 1314, Step 1315, Loss: 2.7340\n",
      "Resumed - Batch 1315, Step 1316, Loss: 2.6730\n",
      "Resumed - Batch 1316, Step 1317, Loss: 3.1039\n",
      "Resumed - Batch 1317, Step 1318, Loss: 2.8212\n",
      "Resumed - Batch 1318, Step 1319, Loss: 2.9865\n",
      "Resumed - Batch 1319, Step 1320, Loss: 2.6754\n",
      "Resumed - Batch 1320, Step 1321, Loss: 2.6478\n",
      "Resumed - Batch 1321, Step 1322, Loss: 2.5202\n",
      "Resumed - Batch 1322, Step 1323, Loss: 2.8156\n",
      "Resumed - Batch 1323, Step 1324, Loss: 3.0585\n",
      "Resumed - Batch 1324, Step 1325, Loss: 2.6843\n",
      "Resumed - Batch 1325, Step 1326, Loss: 2.5609\n",
      "Resumed - Batch 1326, Step 1327, Loss: 2.5766\n",
      "Resumed - Batch 1327, Step 1328, Loss: 2.6312\n",
      "Resumed - Batch 1328, Step 1329, Loss: 2.8408\n",
      "Resumed - Batch 1329, Step 1330, Loss: 2.6913\n",
      "Resumed - Batch 1330, Step 1331, Loss: 2.3089\n",
      "Resumed - Batch 1331, Step 1332, Loss: 2.3509\n",
      "Resumed - Batch 1332, Step 1333, Loss: 2.8184\n",
      "Resumed - Batch 1333, Step 1334, Loss: 2.5405\n",
      "Resumed - Batch 1334, Step 1335, Loss: 2.8795\n",
      "Resumed - Batch 1335, Step 1336, Loss: 2.6429\n",
      "Resumed - Batch 1336, Step 1337, Loss: 2.9237\n",
      "Resumed - Batch 1337, Step 1338, Loss: 2.8317\n",
      "Resumed - Batch 1338, Step 1339, Loss: 2.5340\n",
      "Resumed - Batch 1339, Step 1340, Loss: 2.8844\n",
      "Resumed - Batch 1340, Step 1341, Loss: 3.0816\n",
      "Resumed - Batch 1341, Step 1342, Loss: 2.6929\n",
      "Resumed - Batch 1342, Step 1343, Loss: 2.5121\n",
      "Resumed - Batch 1343, Step 1344, Loss: 2.8855\n",
      "Resumed - Batch 1344, Step 1345, Loss: 2.8837\n",
      "Resumed - Batch 1345, Step 1346, Loss: 2.4923\n",
      "Resumed - Batch 1346, Step 1347, Loss: 3.1088\n",
      "Resumed - Batch 1347, Step 1348, Loss: 2.9498\n",
      "Resumed - Batch 1348, Step 1349, Loss: 2.8266\n",
      "Resumed - Batch 1349, Step 1350, Loss: 2.9309\n",
      "Resumed - Batch 1350, Step 1351, Loss: 2.3290\n",
      "Resumed - Batch 1351, Step 1352, Loss: 2.9069\n",
      "Resumed - Batch 1352, Step 1353, Loss: 2.5562\n",
      "Resumed - Batch 1353, Step 1354, Loss: 2.9489\n",
      "Resumed - Batch 1354, Step 1355, Loss: 2.8389\n",
      "Resumed - Batch 1355, Step 1356, Loss: 2.9447\n",
      "Resumed - Batch 1356, Step 1357, Loss: 2.7909\n",
      "Resumed - Batch 1357, Step 1358, Loss: 3.0616\n",
      "Resumed - Batch 1358, Step 1359, Loss: 2.8177\n",
      "Resumed - Batch 1359, Step 1360, Loss: 2.8113\n",
      "Resumed - Batch 1360, Step 1361, Loss: 2.4765\n",
      "Resumed - Batch 1361, Step 1362, Loss: 2.7156\n",
      "Resumed - Batch 1362, Step 1363, Loss: 2.9723\n",
      "Resumed - Batch 1363, Step 1364, Loss: 2.5806\n",
      "Resumed - Batch 1364, Step 1365, Loss: 2.7206\n",
      "Resumed - Batch 1365, Step 1366, Loss: 2.9161\n",
      "Resumed - Batch 1366, Step 1367, Loss: 2.6860\n",
      "Resumed - Batch 1367, Step 1368, Loss: 2.8393\n",
      "Resumed - Batch 1368, Step 1369, Loss: 2.8292\n",
      "Resumed - Batch 1369, Step 1370, Loss: 2.6658\n",
      "Resumed - Batch 1370, Step 1371, Loss: 2.5381\n",
      "Resumed - Batch 1371, Step 1372, Loss: 2.2885\n",
      "Resumed - Batch 1372, Step 1373, Loss: 2.5784\n",
      "Resumed - Batch 1373, Step 1374, Loss: 2.8418\n",
      "Resumed - Batch 1374, Step 1375, Loss: 3.0168\n",
      "Resumed - Batch 1375, Step 1376, Loss: 2.8649\n",
      "Resumed - Batch 1376, Step 1377, Loss: 2.7642\n",
      "Resumed - Batch 1377, Step 1378, Loss: 2.8819\n",
      "Resumed - Batch 1378, Step 1379, Loss: 2.6508\n",
      "Resumed - Batch 1379, Step 1380, Loss: 3.2018\n",
      "Resumed - Batch 1380, Step 1381, Loss: 2.8097\n",
      "Resumed - Batch 1381, Step 1382, Loss: 2.7722\n",
      "Resumed - Batch 1382, Step 1383, Loss: 2.6999\n",
      "Resumed - Batch 1383, Step 1384, Loss: 2.8829\n",
      "Resumed - Batch 1384, Step 1385, Loss: 2.5715\n",
      "Resumed - Batch 1385, Step 1386, Loss: 2.8340\n",
      "Resumed - Batch 1386, Step 1387, Loss: 2.7009\n",
      "Resumed - Batch 1387, Step 1388, Loss: 2.7824\n",
      "Resumed - Batch 1388, Step 1389, Loss: 3.1820\n",
      "Resumed - Batch 1389, Step 1390, Loss: 2.6483\n",
      "Resumed - Batch 1390, Step 1391, Loss: 3.0418\n",
      "Resumed - Batch 1391, Step 1392, Loss: 2.7051\n",
      "Resumed - Batch 1392, Step 1393, Loss: 2.7134\n",
      "Resumed - Batch 1393, Step 1394, Loss: 2.9736\n",
      "Resumed - Batch 1394, Step 1395, Loss: 2.5937\n",
      "Resumed - Batch 1395, Step 1396, Loss: 2.1851\n",
      "Resumed - Batch 1396, Step 1397, Loss: 3.0318\n",
      "Resumed - Batch 1397, Step 1398, Loss: 3.1196\n",
      "Resumed - Batch 1398, Step 1399, Loss: 3.0101\n",
      "Resumed - Batch 1399, Step 1400, Loss: 2.7551\n",
      "Resumed - Batch 1400, Step 1401, Loss: 2.4515\n",
      "Resumed - Batch 1401, Step 1402, Loss: 3.0788\n",
      "Resumed - Batch 1402, Step 1403, Loss: 2.8403\n",
      "Resumed - Batch 1403, Step 1404, Loss: 2.6479\n",
      "Resumed - Batch 1404, Step 1405, Loss: 2.6509\n",
      "Resumed - Batch 1405, Step 1406, Loss: 2.9471\n",
      "Resumed - Batch 1406, Step 1407, Loss: 2.5320\n",
      "Resumed - Batch 1407, Step 1408, Loss: 2.7411\n",
      "Resumed - Batch 1408, Step 1409, Loss: 2.9387\n",
      "Resumed - Batch 1409, Step 1410, Loss: 3.1161\n",
      "Resumed - Batch 1410, Step 1411, Loss: 2.6456\n",
      "Resumed - Batch 1411, Step 1412, Loss: 2.7804\n",
      "Resumed - Batch 1412, Step 1413, Loss: 2.8171\n",
      "Resumed - Batch 1413, Step 1414, Loss: 2.7340\n",
      "Resumed - Batch 1414, Step 1415, Loss: 2.8277\n",
      "Resumed - Batch 1415, Step 1416, Loss: 2.9904\n",
      "Resumed - Batch 1416, Step 1417, Loss: 2.9336\n",
      "Resumed - Batch 1417, Step 1418, Loss: 2.8869\n",
      "Resumed - Batch 1418, Step 1419, Loss: 2.7823\n",
      "Resumed - Batch 1419, Step 1420, Loss: 2.9747\n",
      "Resumed - Batch 1420, Step 1421, Loss: 2.4858\n",
      "Resumed - Batch 1421, Step 1422, Loss: 2.6707\n",
      "Resumed - Batch 1422, Step 1423, Loss: 2.4096\n",
      "Resumed - Batch 1423, Step 1424, Loss: 2.8845\n",
      "Resumed - Batch 1424, Step 1425, Loss: 2.6925\n",
      "Resumed - Batch 1425, Step 1426, Loss: 2.5982\n",
      "Resumed - Batch 1426, Step 1427, Loss: 2.7627\n",
      "Resumed - Batch 1427, Step 1428, Loss: 2.8890\n",
      "Resumed - Batch 1428, Step 1429, Loss: 2.5788\n",
      "Resumed - Batch 1429, Step 1430, Loss: 2.4683\n",
      "Resumed - Batch 1430, Step 1431, Loss: 2.6306\n",
      "Resumed - Batch 1431, Step 1432, Loss: 2.6952\n",
      "Resumed - Batch 1432, Step 1433, Loss: 2.7821\n",
      "Resumed - Batch 1433, Step 1434, Loss: 2.3569\n",
      "Resumed - Batch 1434, Step 1435, Loss: 2.8581\n",
      "Resumed - Batch 1435, Step 1436, Loss: 3.1543\n",
      "Resumed - Batch 1436, Step 1437, Loss: 2.6904\n",
      "Resumed - Batch 1437, Step 1438, Loss: 2.7650\n",
      "Resumed - Batch 1438, Step 1439, Loss: 2.6325\n",
      "Resumed - Batch 1439, Step 1440, Loss: 2.5471\n",
      "Resumed - Batch 1440, Step 1441, Loss: 2.7316\n",
      "Resumed - Batch 1441, Step 1442, Loss: 2.7794\n",
      "Resumed - Batch 1442, Step 1443, Loss: 3.0122\n",
      "Resumed - Batch 1443, Step 1444, Loss: 2.9543\n",
      "Resumed - Batch 1444, Step 1445, Loss: 2.9874\n",
      "Resumed - Batch 1445, Step 1446, Loss: 2.5598\n",
      "Resumed - Batch 1446, Step 1447, Loss: 2.6965\n",
      "Resumed - Batch 1447, Step 1448, Loss: 2.4155\n",
      "Resumed - Batch 1448, Step 1449, Loss: 2.5784\n",
      "Resumed - Batch 1449, Step 1450, Loss: 2.6198\n",
      "Resumed - Batch 1450, Step 1451, Loss: 2.3790\n",
      "Resumed - Batch 1451, Step 1452, Loss: 2.1683\n",
      "Resumed - Batch 1452, Step 1453, Loss: 2.4677\n",
      "Resumed - Batch 1453, Step 1454, Loss: 2.6115\n",
      "Resumed - Batch 1454, Step 1455, Loss: 2.5949\n",
      "Resumed - Batch 1455, Step 1456, Loss: 2.3957\n",
      "Resumed - Batch 1456, Step 1457, Loss: 2.7935\n",
      "Resumed - Batch 1457, Step 1458, Loss: 2.4040\n",
      "Resumed - Batch 1458, Step 1459, Loss: 2.6425\n",
      "Resumed - Batch 1459, Step 1460, Loss: 2.7519\n",
      "Resumed - Batch 1460, Step 1461, Loss: 2.9922\n",
      "Resumed - Batch 1461, Step 1462, Loss: 2.5155\n",
      "Resumed - Batch 1462, Step 1463, Loss: 2.6935\n",
      "Resumed - Batch 1463, Step 1464, Loss: 2.4310\n",
      "Resumed - Batch 1464, Step 1465, Loss: 2.4158\n",
      "Resumed - Batch 1465, Step 1466, Loss: 2.5332\n",
      "Resumed - Batch 1466, Step 1467, Loss: 2.7435\n",
      "Resumed - Batch 1467, Step 1468, Loss: 2.7803\n",
      "Resumed - Batch 1468, Step 1469, Loss: 2.7893\n",
      "Resumed - Batch 1469, Step 1470, Loss: 2.8270\n",
      "Resumed - Batch 1470, Step 1471, Loss: 2.8716\n",
      "Resumed - Batch 1471, Step 1472, Loss: 2.4199\n",
      "Resumed - Batch 1472, Step 1473, Loss: 2.7809\n",
      "Resumed - Batch 1473, Step 1474, Loss: 2.5924\n",
      "Resumed - Batch 1474, Step 1475, Loss: 3.1166\n",
      "Resumed - Batch 1475, Step 1476, Loss: 2.6844\n",
      "Resumed - Batch 1476, Step 1477, Loss: 2.6065\n",
      "Resumed - Batch 1477, Step 1478, Loss: 2.9400\n",
      "Resumed - Batch 1478, Step 1479, Loss: 2.6611\n",
      "Resumed - Batch 1479, Step 1480, Loss: 2.8555\n",
      "Resumed - Batch 1480, Step 1481, Loss: 2.6880\n",
      "Resumed - Batch 1481, Step 1482, Loss: 2.5551\n",
      "Resumed - Batch 1482, Step 1483, Loss: 2.9702\n",
      "Resumed - Batch 1483, Step 1484, Loss: 2.8285\n",
      "Resumed - Batch 1484, Step 1485, Loss: 2.7246\n",
      "Resumed - Batch 1485, Step 1486, Loss: 2.6051\n",
      "Resumed - Batch 1486, Step 1487, Loss: 2.6795\n",
      "Resumed - Batch 1487, Step 1488, Loss: 3.0598\n",
      "Resumed - Batch 1488, Step 1489, Loss: 2.4981\n",
      "Resumed - Batch 1489, Step 1490, Loss: 3.0016\n",
      "Resumed - Batch 1490, Step 1491, Loss: 2.5313\n",
      "Resumed - Batch 1491, Step 1492, Loss: 2.5342\n",
      "Resumed - Batch 1492, Step 1493, Loss: 2.7844\n",
      "Resumed - Batch 1493, Step 1494, Loss: 2.5120\n",
      "Resumed - Batch 1494, Step 1495, Loss: 2.4127\n",
      "Resumed - Batch 1495, Step 1496, Loss: 2.2600\n",
      "Resumed - Batch 1496, Step 1497, Loss: 2.7914\n",
      "Resumed - Batch 1497, Step 1498, Loss: 2.3107\n",
      "Resumed - Batch 1498, Step 1499, Loss: 2.9943\n",
      "Resumed - Batch 1499, Step 1500, Loss: 2.9457\n",
      "Resumed - Batch 1500, Step 1501, Loss: 2.7257\n",
      "Resumed - Batch 1501, Step 1502, Loss: 2.2592\n",
      "Resumed - Batch 1502, Step 1503, Loss: 2.4066\n",
      "Resumed - Batch 1503, Step 1504, Loss: 2.6430\n",
      "Resumed - Batch 1504, Step 1505, Loss: 2.4233\n",
      "Resumed - Batch 1505, Step 1506, Loss: 3.0366\n",
      "Resumed - Batch 1506, Step 1507, Loss: 2.7727\n",
      "Resumed - Batch 1507, Step 1508, Loss: 2.5350\n",
      "Resumed - Batch 1508, Step 1509, Loss: 2.6173\n",
      "Resumed - Batch 1509, Step 1510, Loss: 2.8625\n",
      "Resumed - Batch 1510, Step 1511, Loss: 2.6045\n",
      "Resumed - Batch 1511, Step 1512, Loss: 2.5771\n",
      "Resumed - Batch 1512, Step 1513, Loss: 2.6583\n",
      "Resumed - Batch 1513, Step 1514, Loss: 2.7835\n",
      "Resumed - Batch 1514, Step 1515, Loss: 2.6779\n",
      "Resumed - Batch 1515, Step 1516, Loss: 3.0560\n",
      "Resumed - Batch 1516, Step 1517, Loss: 2.4807\n",
      "Resumed - Batch 1517, Step 1518, Loss: 2.5851\n",
      "Resumed - Batch 1518, Step 1519, Loss: 3.1365\n",
      "Resumed - Batch 1519, Step 1520, Loss: 2.3996\n",
      "Resumed - Batch 1520, Step 1521, Loss: 2.7360\n",
      "Resumed - Batch 1521, Step 1522, Loss: 2.7854\n",
      "Resumed - Batch 1522, Step 1523, Loss: 2.4196\n",
      "Resumed - Batch 1523, Step 1524, Loss: 2.8269\n",
      "Resumed - Batch 1524, Step 1525, Loss: 2.9444\n",
      "Resumed - Batch 1525, Step 1526, Loss: 2.9466\n",
      "Resumed - Batch 1526, Step 1527, Loss: 2.4215\n",
      "Resumed - Batch 1527, Step 1528, Loss: 2.8200\n",
      "Resumed - Batch 1528, Step 1529, Loss: 2.4765\n",
      "Resumed - Batch 1529, Step 1530, Loss: 2.4152\n",
      "Resumed - Batch 1530, Step 1531, Loss: 2.7130\n",
      "Resumed - Batch 1531, Step 1532, Loss: 2.7448\n",
      "Resumed - Batch 1532, Step 1533, Loss: 2.6959\n",
      "Resumed - Batch 1533, Step 1534, Loss: 2.8835\n",
      "Resumed - Batch 1534, Step 1535, Loss: 2.3777\n",
      "Resumed - Batch 1535, Step 1536, Loss: 2.5726\n",
      "Resumed - Batch 1536, Step 1537, Loss: 2.5861\n",
      "Resumed - Batch 1537, Step 1538, Loss: 2.8745\n",
      "Resumed - Batch 1538, Step 1539, Loss: 2.4407\n",
      "Resumed - Batch 1539, Step 1540, Loss: 3.0148\n",
      "Resumed - Batch 1540, Step 1541, Loss: 2.5724\n",
      "Resumed - Batch 1541, Step 1542, Loss: 2.7962\n",
      "Resumed - Batch 1542, Step 1543, Loss: 3.1077\n",
      "Resumed - Batch 1543, Step 1544, Loss: 2.6274\n",
      "Resumed - Batch 1544, Step 1545, Loss: 2.5016\n",
      "Resumed - Batch 1545, Step 1546, Loss: 2.5883\n",
      "Resumed - Batch 1546, Step 1547, Loss: 2.8408\n",
      "Resumed - Batch 1547, Step 1548, Loss: 2.7548\n",
      "Resumed - Batch 1548, Step 1549, Loss: 2.8708\n",
      "Resumed - Batch 1549, Step 1550, Loss: 2.4658\n",
      "Resumed - Batch 1550, Step 1551, Loss: 2.9027\n",
      "Resumed - Batch 1551, Step 1552, Loss: 2.6421\n",
      "Resumed - Batch 1552, Step 1553, Loss: 2.8337\n",
      "Resumed - Batch 1553, Step 1554, Loss: 2.6250\n",
      "Resumed - Batch 1554, Step 1555, Loss: 2.5901\n",
      "Resumed - Batch 1555, Step 1556, Loss: 2.6103\n",
      "Resumed - Batch 1556, Step 1557, Loss: 2.6252\n",
      "Resumed - Batch 1557, Step 1558, Loss: 2.9091\n",
      "Resumed - Batch 1558, Step 1559, Loss: 2.2315\n",
      "Resumed - Batch 1559, Step 1560, Loss: 2.3414\n",
      "Resumed - Batch 1560, Step 1561, Loss: 2.5847\n",
      "Resumed - Batch 1561, Step 1562, Loss: 2.3286\n",
      "Resumed - Batch 1562, Step 1563, Loss: 2.5850\n",
      "Resumed - Batch 1563, Step 1564, Loss: 2.6286\n",
      "Resumed - Batch 1564, Step 1565, Loss: 2.7995\n",
      "Resumed - Batch 1565, Step 1566, Loss: 2.7107\n",
      "Resumed - Batch 1566, Step 1567, Loss: 2.2400\n",
      "Resumed - Batch 1567, Step 1568, Loss: 2.6153\n",
      "Resumed - Batch 1568, Step 1569, Loss: 2.9106\n",
      "Resumed - Batch 1569, Step 1570, Loss: 2.4172\n",
      "Resumed - Batch 1570, Step 1571, Loss: 2.6197\n",
      "Resumed - Batch 1571, Step 1572, Loss: 2.6078\n",
      "Resumed - Batch 1572, Step 1573, Loss: 2.6796\n",
      "Resumed - Batch 1573, Step 1574, Loss: 2.5324\n",
      "Resumed - Batch 1574, Step 1575, Loss: 2.7923\n",
      "Resumed - Batch 1575, Step 1576, Loss: 3.1322\n",
      "Resumed - Batch 1576, Step 1577, Loss: 2.7339\n",
      "Resumed - Batch 1577, Step 1578, Loss: 3.0611\n",
      "Resumed - Batch 1578, Step 1579, Loss: 2.7554\n",
      "Resumed - Batch 1579, Step 1580, Loss: 2.8208\n",
      "Resumed - Batch 1580, Step 1581, Loss: 2.7296\n",
      "Resumed - Batch 1581, Step 1582, Loss: 2.7129\n",
      "Resumed - Batch 1582, Step 1583, Loss: 2.2270\n",
      "Resumed - Batch 1583, Step 1584, Loss: 2.4661\n",
      "Resumed - Batch 1584, Step 1585, Loss: 2.6931\n",
      "Resumed - Batch 1585, Step 1586, Loss: 2.4105\n",
      "Resumed - Batch 1586, Step 1587, Loss: 2.1717\n",
      "Resumed - Batch 1587, Step 1588, Loss: 2.6255\n",
      "Resumed - Batch 1588, Step 1589, Loss: 2.6822\n",
      "Resumed - Batch 1589, Step 1590, Loss: 2.7654\n",
      "Resumed - Batch 1590, Step 1591, Loss: 2.3617\n",
      "Resumed - Batch 1591, Step 1592, Loss: 2.8715\n",
      "Resumed - Batch 1592, Step 1593, Loss: 2.5930\n",
      "Resumed - Batch 1593, Step 1594, Loss: 2.8179\n",
      "Resumed - Batch 1594, Step 1595, Loss: 2.6464\n",
      "Resumed - Batch 1595, Step 1596, Loss: 2.6391\n",
      "Resumed - Batch 1596, Step 1597, Loss: 2.7704\n",
      "Resumed - Batch 1597, Step 1598, Loss: 2.5910\n",
      "Resumed - Batch 1598, Step 1599, Loss: 2.4715\n",
      "Resumed - Batch 1599, Step 1600, Loss: 2.6487\n",
      "Resumed - Batch 1600, Step 1601, Loss: 2.9325\n",
      "Resumed - Batch 1601, Step 1602, Loss: 2.5328\n",
      "Resumed - Batch 1602, Step 1603, Loss: 2.6939\n",
      "Resumed - Batch 1603, Step 1604, Loss: 2.8164\n",
      "Resumed - Batch 1604, Step 1605, Loss: 3.1239\n",
      "Resumed - Batch 1605, Step 1606, Loss: 2.8553\n",
      "Resumed - Batch 1606, Step 1607, Loss: 2.7760\n",
      "Resumed - Batch 1607, Step 1608, Loss: 2.9109\n",
      "Resumed - Batch 1608, Step 1609, Loss: 2.6899\n",
      "Resumed - Batch 1609, Step 1610, Loss: 2.4672\n",
      "Resumed - Batch 1610, Step 1611, Loss: 2.7048\n",
      "Resumed - Batch 1611, Step 1612, Loss: 2.6800\n",
      "Resumed - Batch 1612, Step 1613, Loss: 2.8649\n",
      "Resumed - Batch 1613, Step 1614, Loss: 2.9983\n",
      "Resumed - Batch 1614, Step 1615, Loss: 2.4689\n",
      "Resumed - Batch 1615, Step 1616, Loss: 2.6569\n",
      "Resumed - Batch 1616, Step 1617, Loss: 2.6664\n",
      "Resumed - Batch 1617, Step 1618, Loss: 2.6428\n",
      "Resumed - Batch 1618, Step 1619, Loss: 2.3892\n",
      "Resumed - Batch 1619, Step 1620, Loss: 2.8742\n",
      "Resumed - Batch 1620, Step 1621, Loss: 2.3027\n",
      "Resumed - Batch 1621, Step 1622, Loss: 2.8982\n",
      "Resumed - Batch 1622, Step 1623, Loss: 2.7188\n",
      "Resumed - Batch 1623, Step 1624, Loss: 2.8907\n",
      "Resumed - Batch 1624, Step 1625, Loss: 2.8630\n",
      "Resumed - Batch 1625, Step 1626, Loss: 2.8236\n",
      "Resumed - Batch 1626, Step 1627, Loss: 2.9404\n",
      "Resumed - Batch 1627, Step 1628, Loss: 2.6754\n",
      "Resumed - Batch 1628, Step 1629, Loss: 2.6171\n",
      "Resumed - Batch 1629, Step 1630, Loss: 2.7164\n",
      "Resumed - Batch 1630, Step 1631, Loss: 2.3819\n",
      "Resumed - Batch 1631, Step 1632, Loss: 2.6645\n",
      "Resumed - Batch 1632, Step 1633, Loss: 2.8430\n",
      "Resumed - Batch 1633, Step 1634, Loss: 2.6439\n",
      "Resumed - Batch 1634, Step 1635, Loss: 2.9194\n",
      "Resumed - Batch 1635, Step 1636, Loss: 2.8479\n",
      "Resumed - Batch 1636, Step 1637, Loss: 2.7386\n",
      "Resumed - Batch 1637, Step 1638, Loss: 2.3785\n",
      "Resumed - Batch 1638, Step 1639, Loss: 2.6620\n",
      "Resumed - Batch 1639, Step 1640, Loss: 2.6877\n",
      "Resumed - Batch 1640, Step 1641, Loss: 2.3104\n",
      "Resumed - Batch 1641, Step 1642, Loss: 2.4760\n",
      "Resumed - Batch 1642, Step 1643, Loss: 2.7076\n",
      "Resumed - Batch 1643, Step 1644, Loss: 3.0940\n",
      "Resumed - Batch 1644, Step 1645, Loss: 2.4218\n",
      "Resumed - Batch 1645, Step 1646, Loss: 2.4605\n",
      "Resumed - Batch 1646, Step 1647, Loss: 2.9050\n",
      "Resumed - Batch 1647, Step 1648, Loss: 2.5241\n",
      "Resumed - Batch 1648, Step 1649, Loss: 2.9357\n",
      "Resumed - Batch 1649, Step 1650, Loss: 2.7268\n",
      "Resumed - Batch 1650, Step 1651, Loss: 2.5294\n",
      "Resumed - Batch 1651, Step 1652, Loss: 2.6360\n",
      "Resumed - Batch 1652, Step 1653, Loss: 2.6689\n",
      "Resumed - Batch 1653, Step 1654, Loss: 2.3927\n",
      "Resumed - Batch 1654, Step 1655, Loss: 2.3626\n",
      "Resumed - Batch 1655, Step 1656, Loss: 2.7981\n",
      "Resumed - Batch 1656, Step 1657, Loss: 2.5113\n",
      "Resumed - Batch 1657, Step 1658, Loss: 2.3966\n",
      "Resumed - Batch 1658, Step 1659, Loss: 2.7456\n",
      "Resumed - Batch 1659, Step 1660, Loss: 2.5447\n",
      "Resumed - Batch 1660, Step 1661, Loss: 3.0143\n",
      "Resumed - Batch 1661, Step 1662, Loss: 2.2287\n",
      "Resumed - Batch 1662, Step 1663, Loss: 2.5738\n",
      "Resumed - Batch 1663, Step 1664, Loss: 2.4230\n",
      "Resumed - Batch 1664, Step 1665, Loss: 2.9246\n",
      "Resumed - Batch 1665, Step 1666, Loss: 2.7654\n",
      "Resumed - Batch 1666, Step 1667, Loss: 2.7288\n",
      "Resumed - Batch 1667, Step 1668, Loss: 2.8107\n",
      "Resumed - Batch 1668, Step 1669, Loss: 2.4345\n",
      "Resumed - Batch 1669, Step 1670, Loss: 2.4108\n",
      "Resumed - Batch 1670, Step 1671, Loss: 2.8566\n",
      "Resumed - Batch 1671, Step 1672, Loss: 2.8183\n",
      "Resumed - Batch 1672, Step 1673, Loss: 2.6699\n",
      "Resumed - Batch 1673, Step 1674, Loss: 2.4827\n",
      "Resumed - Batch 1674, Step 1675, Loss: 2.5946\n",
      "Resumed - Batch 1675, Step 1676, Loss: 2.7744\n",
      "Resumed - Batch 1676, Step 1677, Loss: 2.5267\n",
      "Resumed - Batch 1677, Step 1678, Loss: 2.3160\n",
      "Resumed - Batch 1678, Step 1679, Loss: 3.2247\n",
      "Resumed - Batch 1679, Step 1680, Loss: 2.8244\n",
      "Resumed - Batch 1680, Step 1681, Loss: 3.2151\n",
      "Resumed - Batch 1681, Step 1682, Loss: 2.7214\n",
      "Resumed - Batch 1682, Step 1683, Loss: 3.1043\n",
      "Resumed - Batch 1683, Step 1684, Loss: 2.6619\n",
      "Resumed - Batch 1684, Step 1685, Loss: 2.7807\n",
      "Resumed - Batch 1685, Step 1686, Loss: 2.6629\n",
      "Resumed - Batch 1686, Step 1687, Loss: 2.4675\n",
      "Resumed - Batch 1687, Step 1688, Loss: 2.3768\n",
      "Resumed - Batch 1688, Step 1689, Loss: 2.9117\n",
      "Resumed - Batch 1689, Step 1690, Loss: 2.6566\n",
      "Resumed - Batch 1690, Step 1691, Loss: 3.0990\n",
      "Resumed - Batch 1691, Step 1692, Loss: 3.3704\n",
      "Resumed - Batch 1692, Step 1693, Loss: 2.6328\n",
      "Resumed - Batch 1693, Step 1694, Loss: 2.5712\n",
      "Resumed - Batch 1694, Step 1695, Loss: 2.5729\n",
      "Resumed - Batch 1695, Step 1696, Loss: 2.6387\n",
      "Resumed - Batch 1696, Step 1697, Loss: 2.6770\n",
      "Resumed - Batch 1697, Step 1698, Loss: 2.7114\n",
      "Resumed - Batch 1698, Step 1699, Loss: 2.6113\n",
      "Resumed - Batch 1699, Step 1700, Loss: 2.7214\n",
      "Resumed - Batch 1700, Step 1701, Loss: 2.9262\n",
      "Resumed - Batch 1701, Step 1702, Loss: 2.9942\n",
      "Resumed - Batch 1702, Step 1703, Loss: 2.5603\n",
      "Resumed - Batch 1703, Step 1704, Loss: 2.5435\n",
      "Resumed - Batch 1704, Step 1705, Loss: 2.8018\n",
      "Resumed - Batch 1705, Step 1706, Loss: 2.6357\n",
      "Resumed - Batch 1706, Step 1707, Loss: 2.5789\n",
      "Resumed - Batch 1707, Step 1708, Loss: 2.6834\n",
      "Resumed - Batch 1708, Step 1709, Loss: 2.4684\n",
      "Resumed - Batch 1709, Step 1710, Loss: 2.8268\n",
      "Resumed - Batch 1710, Step 1711, Loss: 2.3661\n",
      "Resumed - Batch 1711, Step 1712, Loss: 2.8468\n",
      "Resumed - Batch 1712, Step 1713, Loss: 2.4210\n",
      "Resumed - Batch 1713, Step 1714, Loss: 2.6598\n",
      "Resumed - Batch 1714, Step 1715, Loss: 2.5756\n",
      "Resumed - Batch 1715, Step 1716, Loss: 2.1153\n",
      "Resumed - Batch 1716, Step 1717, Loss: 2.9805\n",
      "Resumed - Batch 1717, Step 1718, Loss: 2.7329\n",
      "Resumed - Batch 1718, Step 1719, Loss: 2.4669\n",
      "Resumed - Batch 1719, Step 1720, Loss: 2.4374\n",
      "Resumed - Batch 1720, Step 1721, Loss: 2.6520\n",
      "Resumed - Batch 1721, Step 1722, Loss: 2.6832\n",
      "Resumed - Batch 1722, Step 1723, Loss: 2.6286\n",
      "Resumed - Batch 1723, Step 1724, Loss: 2.7234\n",
      "Resumed - Batch 1724, Step 1725, Loss: 2.6136\n",
      "Resumed - Batch 1725, Step 1726, Loss: 2.3823\n",
      "Resumed - Batch 1726, Step 1727, Loss: 2.6632\n",
      "Resumed - Batch 1727, Step 1728, Loss: 2.5024\n",
      "Resumed - Batch 1728, Step 1729, Loss: 2.3725\n",
      "Resumed - Batch 1729, Step 1730, Loss: 2.3565\n",
      "Resumed - Batch 1730, Step 1731, Loss: 2.4766\n",
      "Resumed - Batch 1731, Step 1732, Loss: 2.3826\n",
      "Resumed - Batch 1732, Step 1733, Loss: 2.6363\n",
      "Resumed - Batch 1733, Step 1734, Loss: 2.4609\n",
      "Resumed - Batch 1734, Step 1735, Loss: 2.5283\n",
      "Resumed - Batch 1735, Step 1736, Loss: 2.7923\n",
      "Resumed - Batch 1736, Step 1737, Loss: 2.6247\n",
      "Resumed - Batch 1737, Step 1738, Loss: 2.7795\n",
      "Resumed - Batch 1738, Step 1739, Loss: 2.4624\n",
      "Resumed - Batch 1739, Step 1740, Loss: 2.0788\n",
      "Resumed - Batch 1740, Step 1741, Loss: 2.4755\n",
      "Resumed - Batch 1741, Step 1742, Loss: 2.8918\n",
      "Resumed - Batch 1742, Step 1743, Loss: 2.7888\n",
      "Resumed - Batch 1743, Step 1744, Loss: 2.5638\n",
      "Resumed - Batch 1744, Step 1745, Loss: 2.7929\n",
      "Resumed - Batch 1745, Step 1746, Loss: 2.5476\n",
      "Resumed - Batch 1746, Step 1747, Loss: 2.6282\n",
      "Resumed - Batch 1747, Step 1748, Loss: 2.6911\n",
      "Resumed - Batch 1748, Step 1749, Loss: 2.7795\n",
      "Resumed - Batch 1749, Step 1750, Loss: 2.3971\n",
      "Resumed - Batch 1750, Step 1751, Loss: 2.2897\n",
      "Resumed - Batch 1751, Step 1752, Loss: 2.5718\n",
      "Resumed - Batch 1752, Step 1753, Loss: 2.6995\n",
      "Resumed - Batch 1753, Step 1754, Loss: 2.7148\n",
      "Resumed - Batch 1754, Step 1755, Loss: 2.5628\n",
      "Resumed - Batch 1755, Step 1756, Loss: 2.4278\n",
      "Resumed - Batch 1756, Step 1757, Loss: 2.7245\n",
      "Resumed - Batch 1757, Step 1758, Loss: 2.5532\n",
      "Resumed - Batch 1758, Step 1759, Loss: 2.6353\n",
      "Resumed - Batch 1759, Step 1760, Loss: 2.1393\n",
      "Resumed - Batch 1760, Step 1761, Loss: 2.7263\n",
      "Resumed - Batch 1761, Step 1762, Loss: 2.6739\n",
      "Resumed - Batch 1762, Step 1763, Loss: 2.7020\n",
      "Resumed - Batch 1763, Step 1764, Loss: 2.5719\n",
      "Resumed - Batch 1764, Step 1765, Loss: 2.9199\n",
      "Resumed - Batch 1765, Step 1766, Loss: 2.1007\n",
      "Resumed - Batch 1766, Step 1767, Loss: 2.5989\n",
      "Resumed - Batch 1767, Step 1768, Loss: 2.5422\n",
      "Resumed - Batch 1768, Step 1769, Loss: 2.7815\n",
      "Resumed - Batch 1769, Step 1770, Loss: 2.6394\n",
      "Resumed - Batch 1770, Step 1771, Loss: 2.8480\n",
      "Resumed - Batch 1771, Step 1772, Loss: 2.7103\n",
      "Resumed - Batch 1772, Step 1773, Loss: 2.6122\n",
      "Resumed - Batch 1773, Step 1774, Loss: 2.4889\n",
      "Resumed - Batch 1774, Step 1775, Loss: 3.0410\n",
      "Resumed - Batch 1775, Step 1776, Loss: 2.6238\n",
      "Resumed - Batch 1776, Step 1777, Loss: 2.7844\n",
      "Resumed - Batch 1777, Step 1778, Loss: 2.7160\n",
      "Resumed - Batch 1778, Step 1779, Loss: 2.3952\n",
      "Resumed - Batch 1779, Step 1780, Loss: 2.3812\n",
      "Resumed - Batch 1780, Step 1781, Loss: 2.7996\n",
      "Resumed - Batch 1781, Step 1782, Loss: 2.5341\n",
      "Resumed - Batch 1782, Step 1783, Loss: 2.8654\n",
      "Resumed - Batch 1783, Step 1784, Loss: 2.8461\n",
      "Resumed - Batch 1784, Step 1785, Loss: 2.3593\n",
      "Resumed - Batch 1785, Step 1786, Loss: 2.5288\n",
      "Resumed - Batch 1786, Step 1787, Loss: 2.0383\n",
      "Resumed - Batch 1787, Step 1788, Loss: 2.4584\n",
      "Resumed - Batch 1788, Step 1789, Loss: 2.5353\n",
      "Resumed - Batch 1789, Step 1790, Loss: 2.3102\n",
      "Resumed - Batch 1790, Step 1791, Loss: 2.5390\n",
      "Resumed - Batch 1791, Step 1792, Loss: 2.6961\n",
      "Resumed - Batch 1792, Step 1793, Loss: 2.5851\n",
      "Resumed - Batch 1793, Step 1794, Loss: 2.8980\n",
      "Resumed - Batch 1794, Step 1795, Loss: 2.5217\n",
      "Resumed - Batch 1795, Step 1796, Loss: 2.4933\n",
      "Resumed - Batch 1796, Step 1797, Loss: 2.4739\n",
      "Resumed - Batch 1797, Step 1798, Loss: 2.9649\n",
      "Resumed - Batch 1798, Step 1799, Loss: 3.0758\n",
      "Resumed - Batch 1799, Step 1800, Loss: 2.8648\n",
      "Resumed - Batch 1800, Step 1801, Loss: 3.0941\n",
      "Resumed - Batch 1801, Step 1802, Loss: 2.5217\n",
      "Resumed - Batch 1802, Step 1803, Loss: 2.5214\n",
      "Resumed - Batch 1803, Step 1804, Loss: 2.3150\n",
      "Resumed - Batch 1804, Step 1805, Loss: 2.8433\n",
      "Resumed - Batch 1805, Step 1806, Loss: 2.3110\n",
      "Resumed - Batch 1806, Step 1807, Loss: 2.5849\n",
      "Resumed - Batch 1807, Step 1808, Loss: 2.7925\n",
      "Resumed - Batch 1808, Step 1809, Loss: 2.5150\n",
      "Resumed - Batch 1809, Step 1810, Loss: 2.5663\n",
      "Resumed - Batch 1810, Step 1811, Loss: 2.7742\n",
      "Resumed - Batch 1811, Step 1812, Loss: 2.8820\n",
      "Resumed - Batch 1812, Step 1813, Loss: 2.6571\n",
      "Resumed - Batch 1813, Step 1814, Loss: 2.4107\n",
      "Resumed - Batch 1814, Step 1815, Loss: 3.0300\n",
      "Resumed - Batch 1815, Step 1816, Loss: 2.4422\n",
      "Resumed - Batch 1816, Step 1817, Loss: 2.3712\n",
      "Resumed - Batch 1817, Step 1818, Loss: 2.6630\n",
      "Resumed - Batch 1818, Step 1819, Loss: 2.4253\n",
      "Resumed - Batch 1819, Step 1820, Loss: 2.6008\n",
      "Resumed - Batch 1820, Step 1821, Loss: 2.8552\n",
      "Resumed - Batch 1821, Step 1822, Loss: 2.6162\n",
      "Resumed - Batch 1822, Step 1823, Loss: 2.7890\n",
      "Resumed - Batch 1823, Step 1824, Loss: 2.2430\n",
      "Resumed - Batch 1824, Step 1825, Loss: 2.6936\n",
      "Resumed - Batch 1825, Step 1826, Loss: 2.9348\n",
      "Resumed - Batch 1826, Step 1827, Loss: 2.3646\n",
      "Resumed - Batch 1827, Step 1828, Loss: 2.6643\n",
      "Resumed - Batch 1828, Step 1829, Loss: 2.3891\n",
      "Resumed - Batch 1829, Step 1830, Loss: 2.3997\n",
      "Resumed - Batch 1830, Step 1831, Loss: 2.7616\n",
      "Resumed - Batch 1831, Step 1832, Loss: 2.4298\n",
      "Resumed - Batch 1832, Step 1833, Loss: 2.0821\n",
      "Resumed - Batch 1833, Step 1834, Loss: 2.8670\n",
      "Resumed - Batch 1834, Step 1835, Loss: 2.7530\n",
      "Resumed - Batch 1835, Step 1836, Loss: 2.8689\n",
      "Resumed - Batch 1836, Step 1837, Loss: 2.5197\n",
      "Resumed - Batch 1837, Step 1838, Loss: 2.7349\n",
      "Resumed - Batch 1838, Step 1839, Loss: 2.7653\n",
      "Resumed - Batch 1839, Step 1840, Loss: 2.4103\n",
      "Resumed - Batch 1840, Step 1841, Loss: 2.9245\n",
      "Resumed - Batch 1841, Step 1842, Loss: 2.4002\n",
      "Resumed - Batch 1842, Step 1843, Loss: 2.4854\n",
      "Resumed - Batch 1843, Step 1844, Loss: 2.4979\n",
      "Resumed - Batch 1844, Step 1845, Loss: 2.4856\n",
      "Resumed - Batch 1845, Step 1846, Loss: 2.6810\n",
      "Resumed - Batch 1846, Step 1847, Loss: 2.7527\n",
      "Resumed - Batch 1847, Step 1848, Loss: 2.7739\n",
      "Resumed - Batch 1848, Step 1849, Loss: 2.4879\n",
      "Resumed - Batch 1849, Step 1850, Loss: 2.5110\n",
      "Resumed - Batch 1850, Step 1851, Loss: 2.4665\n",
      "Resumed - Batch 1851, Step 1852, Loss: 2.7781\n",
      "Resumed - Batch 1852, Step 1853, Loss: 2.6022\n",
      "Resumed - Batch 1853, Step 1854, Loss: 3.0442\n",
      "Resumed - Batch 1854, Step 1855, Loss: 1.9627\n",
      "Resumed - Batch 1855, Step 1856, Loss: 2.3769\n",
      "Resumed - Batch 1856, Step 1857, Loss: 2.4498\n",
      "Resumed - Batch 1857, Step 1858, Loss: 2.6987\n",
      "Resumed - Batch 1858, Step 1859, Loss: 2.1780\n",
      "Resumed - Batch 1859, Step 1860, Loss: 2.6059\n",
      "Resumed - Batch 1860, Step 1861, Loss: 2.3051\n",
      "Resumed - Batch 1861, Step 1862, Loss: 2.6351\n",
      "Resumed - Batch 1862, Step 1863, Loss: 2.1685\n",
      "Resumed - Batch 1863, Step 1864, Loss: 2.6258\n",
      "Resumed - Batch 1864, Step 1865, Loss: 2.4397\n",
      "Resumed - Batch 1865, Step 1866, Loss: 2.6504\n",
      "Resumed - Batch 1866, Step 1867, Loss: 2.7131\n",
      "Resumed - Batch 1867, Step 1868, Loss: 2.4182\n",
      "Resumed - Batch 1868, Step 1869, Loss: 2.3772\n",
      "Resumed - Batch 1869, Step 1870, Loss: 2.7970\n",
      "Resumed - Batch 1870, Step 1871, Loss: 2.2306\n",
      "Resumed - Batch 1871, Step 1872, Loss: 2.4908\n",
      "Resumed - Batch 1872, Step 1873, Loss: 2.5420\n",
      "Resumed - Batch 1873, Step 1874, Loss: 2.8533\n",
      "Resumed - Batch 1874, Step 1875, Loss: 2.4115\n",
      "Resumed - Batch 1875, Step 1876, Loss: 2.5116\n",
      "Resumed - Batch 1876, Step 1877, Loss: 2.6571\n",
      "Resumed - Batch 1877, Step 1878, Loss: 2.8427\n",
      "Resumed - Batch 1878, Step 1879, Loss: 2.6931\n",
      "Resumed - Batch 1879, Step 1880, Loss: 2.7895\n",
      "Resumed - Batch 1880, Step 1881, Loss: 2.0915\n",
      "Resumed - Batch 1881, Step 1882, Loss: 2.6537\n",
      "Resumed - Batch 1882, Step 1883, Loss: 2.3147\n",
      "Resumed - Batch 1883, Step 1884, Loss: 2.7205\n",
      "Resumed - Batch 1884, Step 1885, Loss: 2.6449\n",
      "Resumed - Batch 1885, Step 1886, Loss: 2.6796\n",
      "Resumed - Batch 1886, Step 1887, Loss: 2.4752\n",
      "Resumed - Batch 1887, Step 1888, Loss: 2.7038\n",
      "Resumed - Batch 1888, Step 1889, Loss: 2.5353\n",
      "Resumed - Batch 1889, Step 1890, Loss: 2.3894\n",
      "Resumed - Batch 1890, Step 1891, Loss: 2.7382\n",
      "Resumed - Batch 1891, Step 1892, Loss: 2.6153\n",
      "Resumed - Batch 1892, Step 1893, Loss: 3.0065\n",
      "Resumed - Batch 1893, Step 1894, Loss: 2.6747\n",
      "Resumed - Batch 1894, Step 1895, Loss: 2.8550\n",
      "Resumed - Batch 1895, Step 1896, Loss: 2.6746\n",
      "Resumed - Batch 1896, Step 1897, Loss: 2.5584\n",
      "Resumed - Batch 1897, Step 1898, Loss: 2.2642\n",
      "Resumed - Batch 1898, Step 1899, Loss: 2.7510\n",
      "Resumed - Batch 1899, Step 1900, Loss: 2.7560\n",
      "Resumed - Batch 1900, Step 1901, Loss: 2.5788\n",
      "Resumed - Batch 1901, Step 1902, Loss: 2.4679\n",
      "Resumed - Batch 1902, Step 1903, Loss: 2.5520\n",
      "Resumed - Batch 1903, Step 1904, Loss: 2.6438\n",
      "Resumed - Batch 1904, Step 1905, Loss: 2.3336\n",
      "Resumed - Batch 1905, Step 1906, Loss: 3.0124\n",
      "Resumed - Batch 1906, Step 1907, Loss: 2.6723\n",
      "Resumed - Batch 1907, Step 1908, Loss: 2.2586\n",
      "Resumed - Batch 1908, Step 1909, Loss: 2.9497\n",
      "Resumed - Batch 1909, Step 1910, Loss: 2.7768\n",
      "Resumed - Batch 1910, Step 1911, Loss: 2.3213\n",
      "Resumed - Batch 1911, Step 1912, Loss: 2.6790\n",
      "Resumed - Batch 1912, Step 1913, Loss: 2.2342\n",
      "Resumed - Batch 1913, Step 1914, Loss: 2.6976\n",
      "Resumed - Batch 1914, Step 1915, Loss: 2.5436\n",
      "Resumed - Batch 1915, Step 1916, Loss: 2.1969\n",
      "Resumed - Batch 1916, Step 1917, Loss: 2.3611\n",
      "Resumed - Batch 1917, Step 1918, Loss: 2.6869\n",
      "Resumed - Batch 1918, Step 1919, Loss: 2.2970\n",
      "Resumed - Batch 1919, Step 1920, Loss: 2.7348\n",
      "Resumed - Batch 1920, Step 1921, Loss: 2.5763\n",
      "Resumed - Batch 1921, Step 1922, Loss: 2.6406\n",
      "Resumed - Batch 1922, Step 1923, Loss: 2.6369\n",
      "Resumed - Batch 1923, Step 1924, Loss: 2.6385\n",
      "Resumed - Batch 1924, Step 1925, Loss: 2.5961\n",
      "Resumed - Batch 1925, Step 1926, Loss: 2.7912\n",
      "Resumed - Batch 1926, Step 1927, Loss: 3.0058\n",
      "Resumed - Batch 1927, Step 1928, Loss: 2.7124\n",
      "Resumed - Batch 1928, Step 1929, Loss: 2.4700\n",
      "Resumed - Batch 1929, Step 1930, Loss: 2.0797\n",
      "Resumed - Batch 1930, Step 1931, Loss: 2.6749\n",
      "Resumed - Batch 1931, Step 1932, Loss: 2.5736\n",
      "Resumed - Batch 1932, Step 1933, Loss: 2.9251\n",
      "Resumed - Batch 1933, Step 1934, Loss: 2.9435\n",
      "Resumed - Batch 1934, Step 1935, Loss: 2.8196\n",
      "Resumed - Batch 1935, Step 1936, Loss: 2.5416\n",
      "Resumed - Batch 1936, Step 1937, Loss: 2.1313\n",
      "Resumed - Batch 1937, Step 1938, Loss: 2.3768\n",
      "Resumed - Batch 1938, Step 1939, Loss: 2.7886\n",
      "Resumed - Batch 1939, Step 1940, Loss: 2.3248\n",
      "Resumed - Batch 1940, Step 1941, Loss: 3.1054\n",
      "Resumed - Batch 1941, Step 1942, Loss: 2.9324\n",
      "Resumed - Batch 1942, Step 1943, Loss: 2.1771\n",
      "Resumed - Batch 1943, Step 1944, Loss: 2.3409\n",
      "Resumed - Batch 1944, Step 1945, Loss: 2.3400\n",
      "Resumed - Batch 1945, Step 1946, Loss: 2.0849\n",
      "Resumed - Batch 1946, Step 1947, Loss: 2.8293\n",
      "Resumed - Batch 1947, Step 1948, Loss: 2.2590\n",
      "Resumed - Batch 1948, Step 1949, Loss: 2.6665\n",
      "Resumed - Batch 1949, Step 1950, Loss: 3.0911\n",
      "Resumed - Batch 1950, Step 1951, Loss: 2.3628\n",
      "Resumed - Batch 1951, Step 1952, Loss: 2.9288\n",
      "Resumed - Batch 1952, Step 1953, Loss: 2.5921\n",
      "Resumed - Batch 1953, Step 1954, Loss: 2.5485\n",
      "Resumed - Batch 1954, Step 1955, Loss: 2.7414\n",
      "Resumed - Batch 1955, Step 1956, Loss: 2.7433\n",
      "Resumed - Batch 1956, Step 1957, Loss: 2.7756\n",
      "Resumed - Batch 1957, Step 1958, Loss: 2.7115\n",
      "Resumed - Batch 1958, Step 1959, Loss: 2.7307\n",
      "Resumed - Batch 1959, Step 1960, Loss: 2.6826\n",
      "Resumed - Batch 1960, Step 1961, Loss: 2.9838\n",
      "Resumed - Batch 1961, Step 1962, Loss: 2.5834\n",
      "Resumed - Batch 1962, Step 1963, Loss: 2.5786\n",
      "Resumed - Batch 1963, Step 1964, Loss: 2.6157\n",
      "Resumed - Batch 1964, Step 1965, Loss: 2.6763\n",
      "Resumed - Batch 1965, Step 1966, Loss: 2.7924\n",
      "Resumed - Batch 1966, Step 1967, Loss: 2.3896\n",
      "Resumed - Batch 1967, Step 1968, Loss: 2.5669\n",
      "Resumed - Batch 1968, Step 1969, Loss: 2.5533\n",
      "Resumed - Batch 1969, Step 1970, Loss: 2.6772\n",
      "Resumed - Batch 1970, Step 1971, Loss: 2.5856\n",
      "Resumed - Batch 1971, Step 1972, Loss: 2.6868\n",
      "Resumed - Batch 1972, Step 1973, Loss: 2.6599\n",
      "Resumed - Batch 1973, Step 1974, Loss: 2.1918\n",
      "Resumed - Batch 1974, Step 1975, Loss: 2.4467\n",
      "Resumed - Batch 1975, Step 1976, Loss: 2.5744\n",
      "Resumed - Batch 1976, Step 1977, Loss: 2.6971\n",
      "Resumed - Batch 1977, Step 1978, Loss: 2.5719\n",
      "Resumed - Batch 1978, Step 1979, Loss: 2.7360\n",
      "Resumed - Batch 1979, Step 1980, Loss: 2.8470\n",
      "Resumed - Batch 1980, Step 1981, Loss: 2.5412\n",
      "Resumed - Batch 1981, Step 1982, Loss: 2.4820\n",
      "Resumed - Batch 1982, Step 1983, Loss: 2.6435\n",
      "Resumed - Batch 1983, Step 1984, Loss: 2.6188\n",
      "Resumed - Batch 1984, Step 1985, Loss: 2.3578\n",
      "Resumed - Batch 1985, Step 1986, Loss: 2.5167\n",
      "Resumed - Batch 1986, Step 1987, Loss: 2.5292\n",
      "Resumed - Batch 1987, Step 1988, Loss: 2.6927\n",
      "Resumed - Batch 1988, Step 1989, Loss: 2.7162\n",
      "Resumed - Batch 1989, Step 1990, Loss: 2.8371\n",
      "Resumed - Batch 1990, Step 1991, Loss: 2.8291\n",
      "Resumed - Batch 1991, Step 1992, Loss: 2.6290\n",
      "Resumed - Batch 1992, Step 1993, Loss: 2.4651\n",
      "Resumed - Batch 1993, Step 1994, Loss: 2.2078\n",
      "Resumed - Batch 1994, Step 1995, Loss: 2.7693\n",
      "Resumed - Batch 1995, Step 1996, Loss: 2.6892\n",
      "Resumed - Batch 1996, Step 1997, Loss: 2.5844\n",
      "Resumed - Batch 1997, Step 1998, Loss: 2.7462\n",
      "Resumed - Batch 1998, Step 1999, Loss: 2.7502\n",
      "Resumed - Batch 1999, Step 2000, Loss: 2.4390\n",
      "Resumed - Batch 2000, Step 2001, Loss: 2.3164\n",
      "Resumed - Batch 2001, Step 2002, Loss: 2.9908\n",
      "Resumed - Batch 2002, Step 2003, Loss: 2.8278\n",
      "Resumed - Batch 2003, Step 2004, Loss: 2.4111\n",
      "Resumed - Batch 2004, Step 2005, Loss: 2.5208\n",
      "Resumed - Batch 2005, Step 2006, Loss: 2.5332\n",
      "Resumed - Batch 2006, Step 2007, Loss: 2.6870\n",
      "Resumed - Batch 2007, Step 2008, Loss: 2.6094\n",
      "Resumed - Batch 2008, Step 2009, Loss: 2.6260\n",
      "Resumed - Batch 2009, Step 2010, Loss: 2.3327\n",
      "Resumed - Batch 2010, Step 2011, Loss: 2.5715\n",
      "Resumed - Batch 2011, Step 2012, Loss: 2.4944\n",
      "Resumed - Batch 2012, Step 2013, Loss: 2.6459\n",
      "Resumed - Batch 2013, Step 2014, Loss: 2.2403\n",
      "Resumed - Batch 2014, Step 2015, Loss: 2.2021\n",
      "Resumed - Batch 2015, Step 2016, Loss: 2.6004\n",
      "Resumed - Batch 2016, Step 2017, Loss: 2.5288\n",
      "Resumed - Batch 2017, Step 2018, Loss: 2.6709\n",
      "Resumed - Batch 2018, Step 2019, Loss: 2.8297\n",
      "Resumed - Batch 2019, Step 2020, Loss: 2.9220\n",
      "Resumed - Batch 2020, Step 2021, Loss: 2.5357\n",
      "Resumed - Batch 2021, Step 2022, Loss: 2.3277\n",
      "Resumed - Batch 2022, Step 2023, Loss: 2.9202\n",
      "Resumed - Batch 2023, Step 2024, Loss: 2.1375\n",
      "Resumed - Batch 2024, Step 2025, Loss: 2.6487\n",
      "Resumed - Batch 2025, Step 2026, Loss: 2.2136\n",
      "Resumed - Batch 2026, Step 2027, Loss: 2.2393\n",
      "Resumed - Batch 2027, Step 2028, Loss: 2.5022\n",
      "Resumed - Batch 2028, Step 2029, Loss: 2.6591\n",
      "Resumed - Batch 2029, Step 2030, Loss: 2.8689\n",
      "Resumed - Batch 2030, Step 2031, Loss: 2.7205\n",
      "Resumed - Batch 2031, Step 2032, Loss: 2.7210\n",
      "Resumed - Batch 2032, Step 2033, Loss: 2.4528\n",
      "Resumed - Batch 2033, Step 2034, Loss: 2.5780\n",
      "Resumed - Batch 2034, Step 2035, Loss: 2.4112\n",
      "Resumed - Batch 2035, Step 2036, Loss: 2.5351\n",
      "Resumed - Batch 2036, Step 2037, Loss: 2.8063\n",
      "Resumed - Batch 2037, Step 2038, Loss: 2.1706\n",
      "Resumed - Batch 2038, Step 2039, Loss: 2.6471\n",
      "Resumed - Batch 2039, Step 2040, Loss: 2.8544\n",
      "Resumed - Batch 2040, Step 2041, Loss: 2.9000\n",
      "Resumed - Batch 2041, Step 2042, Loss: 2.4189\n",
      "Resumed - Batch 2042, Step 2043, Loss: 2.3016\n",
      "Resumed - Batch 2043, Step 2044, Loss: 2.5047\n",
      "Resumed - Batch 2044, Step 2045, Loss: 2.1775\n",
      "Resumed - Batch 2045, Step 2046, Loss: 2.6233\n",
      "Resumed - Batch 2046, Step 2047, Loss: 2.5426\n",
      "Resumed - Batch 2047, Step 2048, Loss: 2.9751\n",
      "Resumed - Batch 2048, Step 2049, Loss: 2.7470\n",
      "Resumed - Batch 2049, Step 2050, Loss: 2.5439\n",
      "Resumed - Batch 2050, Step 2051, Loss: 2.1233\n",
      "Resumed - Batch 2051, Step 2052, Loss: 2.1871\n",
      "Resumed - Batch 2052, Step 2053, Loss: 2.5846\n",
      "Resumed - Batch 2053, Step 2054, Loss: 3.0037\n",
      "Resumed - Batch 2054, Step 2055, Loss: 2.5737\n",
      "Resumed - Batch 2055, Step 2056, Loss: 2.5018\n",
      "Resumed - Batch 2056, Step 2057, Loss: 2.4721\n",
      "Resumed - Batch 2057, Step 2058, Loss: 2.5313\n",
      "Resumed - Batch 2058, Step 2059, Loss: 2.6585\n",
      "Resumed - Batch 2059, Step 2060, Loss: 2.4002\n",
      "Resumed - Batch 2060, Step 2061, Loss: 2.5072\n",
      "Resumed - Batch 2061, Step 2062, Loss: 2.6532\n",
      "Resumed - Batch 2062, Step 2063, Loss: 3.0181\n",
      "Resumed - Batch 2063, Step 2064, Loss: 2.7452\n",
      "Resumed - Batch 2064, Step 2065, Loss: 2.2202\n",
      "Resumed - Batch 2065, Step 2066, Loss: 2.4675\n",
      "Resumed - Batch 2066, Step 2067, Loss: 2.5384\n",
      "Resumed - Batch 2067, Step 2068, Loss: 2.3978\n",
      "Resumed - Batch 2068, Step 2069, Loss: 2.5980\n",
      "Resumed - Batch 2069, Step 2070, Loss: 2.4002\n",
      "Resumed - Batch 2070, Step 2071, Loss: 2.8058\n",
      "Resumed - Batch 2071, Step 2072, Loss: 2.7864\n",
      "Resumed - Batch 2072, Step 2073, Loss: 2.7309\n",
      "Resumed - Batch 2073, Step 2074, Loss: 2.4295\n",
      "Resumed - Batch 2074, Step 2075, Loss: 2.4719\n",
      "Resumed - Batch 2075, Step 2076, Loss: 3.0764\n",
      "Resumed - Batch 2076, Step 2077, Loss: 2.6563\n",
      "Resumed - Batch 2077, Step 2078, Loss: 2.7379\n",
      "Resumed - Batch 2078, Step 2079, Loss: 2.6753\n",
      "Resumed - Batch 2079, Step 2080, Loss: 2.6496\n",
      "Resumed - Batch 2080, Step 2081, Loss: 2.2215\n",
      "Resumed - Batch 2081, Step 2082, Loss: 2.7024\n",
      "Resumed - Batch 2082, Step 2083, Loss: 2.7871\n",
      "Resumed - Batch 2083, Step 2084, Loss: 2.5474\n",
      "Resumed - Batch 2084, Step 2085, Loss: 2.6055\n",
      "Resumed - Batch 2085, Step 2086, Loss: 2.4898\n",
      "Resumed - Batch 2086, Step 2087, Loss: 2.5073\n",
      "Resumed - Batch 2087, Step 2088, Loss: 3.0027\n",
      "Resumed - Batch 2088, Step 2089, Loss: 2.3034\n",
      "Resumed - Batch 2089, Step 2090, Loss: 2.4487\n",
      "Resumed - Batch 2090, Step 2091, Loss: 2.2509\n",
      "Resumed - Batch 2091, Step 2092, Loss: 2.2708\n",
      "Resumed - Batch 2092, Step 2093, Loss: 2.6603\n",
      "Resumed - Batch 2093, Step 2094, Loss: 2.0033\n",
      "Resumed - Batch 2094, Step 2095, Loss: 2.7226\n",
      "Resumed - Batch 2095, Step 2096, Loss: 2.4318\n",
      "Resumed - Batch 2096, Step 2097, Loss: 2.7548\n",
      "Resumed - Batch 2097, Step 2098, Loss: 2.4674\n",
      "Resumed - Batch 2098, Step 2099, Loss: 2.6727\n",
      "Resumed - Batch 2099, Step 2100, Loss: 2.3710\n",
      "Resumed - Batch 2100, Step 2101, Loss: 2.2353\n",
      "Resumed - Batch 2101, Step 2102, Loss: 2.1423\n",
      "Resumed - Batch 2102, Step 2103, Loss: 2.3686\n",
      "Resumed - Batch 2103, Step 2104, Loss: 2.2364\n",
      "Resumed - Batch 2104, Step 2105, Loss: 2.8231\n",
      "Resumed - Batch 2105, Step 2106, Loss: 2.5067\n",
      "Resumed - Batch 2106, Step 2107, Loss: 2.7904\n",
      "Resumed - Batch 2107, Step 2108, Loss: 2.6578\n",
      "Resumed - Batch 2108, Step 2109, Loss: 2.8644\n",
      "Resumed - Batch 2109, Step 2110, Loss: 2.2424\n",
      "Resumed - Batch 2110, Step 2111, Loss: 2.6198\n",
      "Resumed - Batch 2111, Step 2112, Loss: 2.7371\n",
      "Resumed - Batch 2112, Step 2113, Loss: 2.2444\n",
      "Resumed - Batch 2113, Step 2114, Loss: 3.1006\n",
      "Resumed - Batch 2114, Step 2115, Loss: 2.6936\n",
      "Resumed - Batch 2115, Step 2116, Loss: 2.7782\n",
      "Resumed - Batch 2116, Step 2117, Loss: 2.5948\n",
      "Resumed - Batch 2117, Step 2118, Loss: 2.6483\n",
      "Resumed - Batch 2118, Step 2119, Loss: 2.7375\n",
      "Resumed - Batch 2119, Step 2120, Loss: 2.9025\n",
      "Resumed - Batch 2120, Step 2121, Loss: 2.6267\n",
      "Resumed - Batch 2121, Step 2122, Loss: 2.5384\n",
      "Resumed - Batch 2122, Step 2123, Loss: 2.6150\n",
      "Resumed - Batch 2123, Step 2124, Loss: 2.5683\n",
      "Resumed - Batch 2124, Step 2125, Loss: 2.7627\n",
      "Resumed - Batch 2125, Step 2126, Loss: 2.2280\n",
      "Resumed - Batch 2126, Step 2127, Loss: 2.8264\n",
      "Resumed - Batch 2127, Step 2128, Loss: 2.4678\n",
      "Resumed - Batch 2128, Step 2129, Loss: 2.9571\n",
      "Resumed - Batch 2129, Step 2130, Loss: 2.9492\n",
      "Resumed - Batch 2130, Step 2131, Loss: 2.7121\n",
      "Resumed - Batch 2131, Step 2132, Loss: 2.7874\n",
      "Resumed - Batch 2132, Step 2133, Loss: 2.7387\n",
      "Resumed - Batch 2133, Step 2134, Loss: 2.5648\n",
      "Resumed - Batch 2134, Step 2135, Loss: 2.7929\n",
      "Resumed - Batch 2135, Step 2136, Loss: 2.4533\n",
      "Resumed - Batch 2136, Step 2137, Loss: 2.4116\n",
      "Resumed - Batch 2137, Step 2138, Loss: 2.5757\n",
      "Resumed - Batch 2138, Step 2139, Loss: 2.7279\n",
      "Resumed - Batch 2139, Step 2140, Loss: 2.5068\n",
      "Resumed - Batch 2140, Step 2141, Loss: 2.7793\n",
      "Resumed - Batch 2141, Step 2142, Loss: 2.4118\n",
      "Resumed - Batch 2142, Step 2143, Loss: 2.5132\n",
      "Resumed - Batch 2143, Step 2144, Loss: 2.4578\n",
      "Resumed - Batch 2144, Step 2145, Loss: 2.4223\n",
      "Resumed - Batch 2145, Step 2146, Loss: 2.6344\n",
      "Resumed - Batch 2146, Step 2147, Loss: 2.5348\n",
      "Resumed - Batch 2147, Step 2148, Loss: 2.6182\n",
      "Resumed - Batch 2148, Step 2149, Loss: 2.4509\n",
      "Resumed - Batch 2149, Step 2150, Loss: 2.5445\n",
      "Resumed - Batch 2150, Step 2151, Loss: 2.5086\n",
      "Resumed - Batch 2151, Step 2152, Loss: 2.1238\n",
      "Resumed - Batch 2152, Step 2153, Loss: 2.3252\n",
      "Resumed - Batch 2153, Step 2154, Loss: 2.3903\n",
      "Resumed - Batch 2154, Step 2155, Loss: 2.4418\n",
      "Resumed - Batch 2155, Step 2156, Loss: 2.6745\n",
      "Resumed - Batch 2156, Step 2157, Loss: 2.1832\n",
      "Resumed - Batch 2157, Step 2158, Loss: 2.3349\n",
      "Resumed - Batch 2158, Step 2159, Loss: 2.3338\n",
      "Resumed - Batch 2159, Step 2160, Loss: 2.4457\n",
      "Resumed - Batch 2160, Step 2161, Loss: 2.4001\n",
      "Resumed - Batch 2161, Step 2162, Loss: 2.5742\n",
      "Resumed - Batch 2162, Step 2163, Loss: 2.4935\n",
      "Resumed - Batch 2163, Step 2164, Loss: 2.5872\n",
      "Resumed - Batch 2164, Step 2165, Loss: 2.3102\n",
      "Resumed - Batch 2165, Step 2166, Loss: 2.5838\n",
      "Resumed - Batch 2166, Step 2167, Loss: 2.2207\n",
      "Resumed - Batch 2167, Step 2168, Loss: 2.5483\n",
      "Resumed - Batch 2168, Step 2169, Loss: 2.4002\n",
      "Resumed - Batch 2169, Step 2170, Loss: 2.5846\n",
      "Resumed - Batch 2170, Step 2171, Loss: 2.3721\n",
      "Resumed - Batch 2171, Step 2172, Loss: 2.6848\n",
      "Resumed - Batch 2172, Step 2173, Loss: 2.5486\n",
      "Resumed - Batch 2173, Step 2174, Loss: 2.1954\n",
      "Resumed - Batch 2174, Step 2175, Loss: 2.6450\n",
      "Resumed - Batch 2175, Step 2176, Loss: 2.5551\n",
      "Resumed - Batch 2176, Step 2177, Loss: 2.2857\n",
      "Resumed - Batch 2177, Step 2178, Loss: 2.4337\n",
      "Resumed - Batch 2178, Step 2179, Loss: 2.9314\n",
      "Resumed - Batch 2179, Step 2180, Loss: 2.5709\n",
      "Resumed - Batch 2180, Step 2181, Loss: 2.4628\n",
      "Resumed - Batch 2181, Step 2182, Loss: 2.6925\n",
      "Resumed - Batch 2182, Step 2183, Loss: 2.5913\n",
      "Resumed - Batch 2183, Step 2184, Loss: 2.5070\n",
      "Resumed - Batch 2184, Step 2185, Loss: 2.4350\n",
      "Resumed - Batch 2185, Step 2186, Loss: 2.9030\n",
      "Resumed - Batch 2186, Step 2187, Loss: 2.5786\n",
      "Resumed - Batch 2187, Step 2188, Loss: 2.8402\n",
      "Resumed - Batch 2188, Step 2189, Loss: 2.8170\n",
      "Resumed - Batch 2189, Step 2190, Loss: 2.5770\n",
      "Resumed - Batch 2190, Step 2191, Loss: 2.5980\n",
      "Resumed - Batch 2191, Step 2192, Loss: 2.0882\n",
      "Resumed - Batch 2192, Step 2193, Loss: 2.7576\n",
      "Resumed - Batch 2193, Step 2194, Loss: 2.5926\n",
      "Resumed - Batch 2194, Step 2195, Loss: 2.6690\n",
      "Resumed - Batch 2195, Step 2196, Loss: 2.2971\n",
      "Resumed - Batch 2196, Step 2197, Loss: 2.7689\n",
      "Resumed - Batch 2197, Step 2198, Loss: 2.3901\n",
      "Resumed - Batch 2198, Step 2199, Loss: 2.7965\n",
      "Resumed - Batch 2199, Step 2200, Loss: 2.3774\n",
      "Resumed - Batch 2200, Step 2201, Loss: 2.4113\n",
      "Resumed - Batch 2201, Step 2202, Loss: 2.5650\n",
      "Resumed - Batch 2202, Step 2203, Loss: 2.2644\n",
      "Resumed - Batch 2203, Step 2204, Loss: 2.1991\n",
      "Resumed - Batch 2204, Step 2205, Loss: 2.6964\n",
      "Resumed - Batch 2205, Step 2206, Loss: 2.4954\n",
      "Resumed - Batch 2206, Step 2207, Loss: 2.5483\n",
      "Resumed - Batch 2207, Step 2208, Loss: 2.4001\n",
      "Resumed - Batch 2208, Step 2209, Loss: 2.6418\n",
      "Resumed - Batch 2209, Step 2210, Loss: 2.2999\n",
      "Resumed - Batch 2210, Step 2211, Loss: 2.0139\n",
      "Resumed - Batch 2211, Step 2212, Loss: 2.4604\n",
      "Resumed - Batch 2212, Step 2213, Loss: 2.6803\n",
      "Resumed - Batch 2213, Step 2214, Loss: 2.5438\n",
      "Resumed - Batch 2214, Step 2215, Loss: 2.0742\n",
      "Resumed - Batch 2215, Step 2216, Loss: 2.4691\n",
      "Resumed - Batch 2216, Step 2217, Loss: 2.6992\n",
      "Resumed - Batch 2217, Step 2218, Loss: 2.4986\n",
      "Resumed - Batch 2218, Step 2219, Loss: 2.4089\n",
      "Resumed - Batch 2219, Step 2220, Loss: 2.4881\n",
      "Resumed - Batch 2220, Step 2221, Loss: 2.4713\n",
      "Resumed - Batch 2221, Step 2222, Loss: 2.2801\n",
      "Resumed - Batch 2222, Step 2223, Loss: 2.4483\n",
      "Resumed - Batch 2223, Step 2224, Loss: 2.4114\n",
      "Resumed - Batch 2224, Step 2225, Loss: 2.7332\n",
      "Resumed - Batch 2225, Step 2226, Loss: 2.3853\n",
      "Resumed - Batch 2226, Step 2227, Loss: 2.2716\n",
      "Resumed - Batch 2227, Step 2228, Loss: 2.8464\n",
      "Resumed - Batch 2228, Step 2229, Loss: 2.5543\n",
      "Resumed - Batch 2229, Step 2230, Loss: 2.2284\n",
      "Resumed - Batch 2230, Step 2231, Loss: 2.4939\n",
      "Resumed - Batch 2231, Step 2232, Loss: 2.6677\n",
      "Resumed - Batch 2232, Step 2233, Loss: 2.2360\n",
      "Resumed - Batch 2233, Step 2234, Loss: 2.0508\n",
      "Resumed - Batch 2234, Step 2235, Loss: 2.2319\n",
      "Resumed - Batch 2235, Step 2236, Loss: 2.5151\n",
      "Resumed - Batch 2236, Step 2237, Loss: 2.5194\n",
      "Resumed - Batch 2237, Step 2238, Loss: 1.8561\n",
      "Resumed - Batch 2238, Step 2239, Loss: 2.3025\n",
      "Resumed - Batch 2239, Step 2240, Loss: 2.3277\n",
      "Resumed - Batch 2240, Step 2241, Loss: 2.3991\n",
      "Resumed - Batch 2241, Step 2242, Loss: 2.3182\n",
      "Resumed - Batch 2242, Step 2243, Loss: 2.3834\n",
      "Resumed - Batch 2243, Step 2244, Loss: 2.5198\n",
      "Resumed - Batch 2244, Step 2245, Loss: 2.4413\n",
      "Resumed - Batch 2245, Step 2246, Loss: 2.3534\n",
      "Resumed - Batch 2246, Step 2247, Loss: 2.3412\n",
      "Resumed - Batch 2247, Step 2248, Loss: 2.2121\n",
      "Resumed - Batch 2248, Step 2249, Loss: 2.6234\n",
      "Resumed - Batch 2249, Step 2250, Loss: 2.2812\n",
      "Resumed - Batch 2250, Step 2251, Loss: 2.6555\n",
      "Resumed - Batch 2251, Step 2252, Loss: 2.5842\n",
      "Resumed - Batch 2252, Step 2253, Loss: 2.7247\n",
      "Resumed - Batch 2253, Step 2254, Loss: 2.4317\n",
      "Resumed - Batch 2254, Step 2255, Loss: 2.1602\n",
      "Resumed - Batch 2255, Step 2256, Loss: 2.6409\n",
      "Resumed - Batch 2256, Step 2257, Loss: 2.3301\n",
      "Resumed - Batch 2257, Step 2258, Loss: 2.4796\n",
      "Resumed - Batch 2258, Step 2259, Loss: 2.5950\n",
      "Resumed - Batch 2259, Step 2260, Loss: 2.6102\n",
      "Resumed - Batch 2260, Step 2261, Loss: 2.9495\n",
      "Resumed - Batch 2261, Step 2262, Loss: 2.6008\n",
      "Resumed - Batch 2262, Step 2263, Loss: 2.3131\n",
      "Resumed - Batch 2263, Step 2264, Loss: 2.4383\n",
      "Resumed - Batch 2264, Step 2265, Loss: 2.6158\n",
      "Resumed - Batch 2265, Step 2266, Loss: 1.8342\n",
      "Resumed - Batch 2266, Step 2267, Loss: 2.5990\n",
      "Resumed - Batch 2267, Step 2268, Loss: 2.6411\n",
      "Resumed - Batch 2268, Step 2269, Loss: 2.6545\n",
      "Resumed - Batch 2269, Step 2270, Loss: 2.3590\n",
      "Resumed - Batch 2270, Step 2271, Loss: 2.2527\n",
      "Resumed - Batch 2271, Step 2272, Loss: 2.3431\n",
      "Resumed - Batch 2272, Step 2273, Loss: 2.3701\n",
      "Resumed - Batch 2273, Step 2274, Loss: 2.4600\n",
      "Resumed - Batch 2274, Step 2275, Loss: 2.3626\n",
      "Resumed - Batch 2275, Step 2276, Loss: 2.3440\n",
      "Resumed - Batch 2276, Step 2277, Loss: 2.3166\n",
      "Resumed - Batch 2277, Step 2278, Loss: 2.6410\n",
      "Resumed - Batch 2278, Step 2279, Loss: 2.1611\n",
      "Resumed - Batch 2279, Step 2280, Loss: 2.6650\n",
      "Resumed - Batch 2280, Step 2281, Loss: 1.9523\n",
      "Resumed - Batch 2281, Step 2282, Loss: 2.3767\n",
      "Resumed - Batch 2282, Step 2283, Loss: 2.2507\n",
      "Resumed - Batch 2283, Step 2284, Loss: 2.7510\n",
      "Resumed - Batch 2284, Step 2285, Loss: 2.6316\n",
      "Resumed - Batch 2285, Step 2286, Loss: 2.3862\n",
      "Resumed - Batch 2286, Step 2287, Loss: 2.5033\n",
      "Resumed - Batch 2287, Step 2288, Loss: 2.7544\n",
      "Resumed - Batch 2288, Step 2289, Loss: 2.4283\n",
      "Resumed - Batch 2289, Step 2290, Loss: 2.4118\n",
      "Resumed - Batch 2290, Step 2291, Loss: 2.9547\n",
      "Resumed - Batch 2291, Step 2292, Loss: 2.5410\n",
      "Resumed - Batch 2292, Step 2293, Loss: 2.1364\n",
      "Resumed - Batch 2293, Step 2294, Loss: 2.6117\n",
      "Resumed - Batch 2294, Step 2295, Loss: 2.3918\n",
      "Resumed - Batch 2295, Step 2296, Loss: 2.3808\n",
      "Resumed - Batch 2296, Step 2297, Loss: 2.5172\n",
      "Resumed - Batch 2297, Step 2298, Loss: 3.0009\n",
      "Resumed - Batch 2298, Step 2299, Loss: 2.2561\n",
      "Resumed - Batch 2299, Step 2300, Loss: 2.0743\n",
      "Resumed - Batch 2300, Step 2301, Loss: 2.4413\n",
      "Resumed - Batch 2301, Step 2302, Loss: 2.5414\n",
      "Resumed - Batch 2302, Step 2303, Loss: 2.4789\n",
      "Resumed - Batch 2303, Step 2304, Loss: 2.2141\n",
      "Resumed - Batch 2304, Step 2305, Loss: 2.0926\n",
      "Resumed - Batch 2305, Step 2306, Loss: 2.6596\n",
      "Resumed - Batch 2306, Step 2307, Loss: 2.7077\n",
      "Resumed - Batch 2307, Step 2308, Loss: 2.4050\n",
      "Resumed - Batch 2308, Step 2309, Loss: 2.0999\n",
      "Resumed - Batch 2309, Step 2310, Loss: 2.5669\n",
      "Resumed - Batch 2310, Step 2311, Loss: 2.8619\n",
      "Resumed - Batch 2311, Step 2312, Loss: 2.5718\n",
      "Resumed - Batch 2312, Step 2313, Loss: 2.1573\n",
      "Resumed - Batch 2313, Step 2314, Loss: 2.2766\n",
      "Resumed - Batch 2314, Step 2315, Loss: 2.4381\n",
      "Resumed - Batch 2315, Step 2316, Loss: 2.3889\n",
      "Resumed - Batch 2316, Step 2317, Loss: 2.3498\n",
      "Resumed - Batch 2317, Step 2318, Loss: 2.7433\n",
      "Resumed - Batch 2318, Step 2319, Loss: 2.2125\n",
      "Resumed - Batch 2319, Step 2320, Loss: 2.4195\n",
      "Resumed - Batch 2320, Step 2321, Loss: 2.7817\n",
      "Resumed - Batch 2321, Step 2322, Loss: 2.0618\n",
      "Resumed - Batch 2322, Step 2323, Loss: 2.5143\n",
      "Resumed - Batch 2323, Step 2324, Loss: 2.5989\n",
      "Resumed - Batch 2324, Step 2325, Loss: 2.5114\n",
      "Resumed - Batch 2325, Step 2326, Loss: 2.5332\n",
      "Resumed - Batch 2326, Step 2327, Loss: 2.4669\n",
      "Resumed - Batch 2327, Step 2328, Loss: 2.7627\n",
      "Resumed - Batch 2328, Step 2329, Loss: 2.8783\n",
      "Resumed - Batch 2329, Step 2330, Loss: 2.3850\n",
      "Resumed - Batch 2330, Step 2331, Loss: 2.5901\n",
      "Resumed - Batch 2331, Step 2332, Loss: 2.5298\n",
      "Resumed - Batch 2332, Step 2333, Loss: 2.1607\n",
      "Resumed - Batch 2333, Step 2334, Loss: 2.1973\n",
      "Resumed - Batch 2334, Step 2335, Loss: 2.6881\n",
      "Resumed - Batch 2335, Step 2336, Loss: 2.3308\n",
      "Resumed - Batch 2336, Step 2337, Loss: 2.4746\n",
      "Resumed - Batch 2337, Step 2338, Loss: 2.8445\n",
      "Resumed - Batch 2338, Step 2339, Loss: 2.7239\n",
      "Resumed - Batch 2339, Step 2340, Loss: 2.4897\n",
      "Resumed - Batch 2340, Step 2341, Loss: 2.6236\n",
      "Resumed - Batch 2341, Step 2342, Loss: 2.7407\n",
      "Resumed - Batch 2342, Step 2343, Loss: 2.6804\n",
      "Resumed - Batch 2343, Step 2344, Loss: 2.6035\n",
      "Resumed - Batch 2344, Step 2345, Loss: 2.3929\n",
      "Resumed - Batch 2345, Step 2346, Loss: 2.3336\n",
      "Resumed - Batch 2346, Step 2347, Loss: 2.4805\n",
      "Resumed - Batch 2347, Step 2348, Loss: 2.4053\n",
      "Resumed - Batch 2348, Step 2349, Loss: 2.0220\n",
      "Resumed - Batch 2349, Step 2350, Loss: 2.0435\n",
      "Resumed - Batch 2350, Step 2351, Loss: 2.2999\n",
      "Resumed - Batch 2351, Step 2352, Loss: 2.4464\n",
      "Resumed - Batch 2352, Step 2353, Loss: 2.3713\n",
      "Resumed - Batch 2353, Step 2354, Loss: 2.1981\n",
      "Resumed - Batch 2354, Step 2355, Loss: 2.2852\n",
      "Resumed - Batch 2355, Step 2356, Loss: 2.5012\n",
      "Resumed - Batch 2356, Step 2357, Loss: 2.3317\n",
      "Resumed - Batch 2357, Step 2358, Loss: 2.6380\n",
      "Resumed - Batch 2358, Step 2359, Loss: 2.7281\n",
      "Resumed - Batch 2359, Step 2360, Loss: 2.4414\n",
      "Resumed - Batch 2360, Step 2361, Loss: 2.8068\n",
      "Resumed - Batch 2361, Step 2362, Loss: 2.0485\n",
      "Resumed - Batch 2362, Step 2363, Loss: 2.0439\n",
      "Resumed - Batch 2363, Step 2364, Loss: 3.1491\n",
      "Resumed - Batch 2364, Step 2365, Loss: 2.2517\n",
      "Resumed - Batch 2365, Step 2366, Loss: 2.1969\n",
      "Resumed - Batch 2366, Step 2367, Loss: 2.0013\n",
      "Resumed - Batch 2367, Step 2368, Loss: 2.6052\n",
      "Resumed - Batch 2368, Step 2369, Loss: 2.3187\n",
      "Resumed - Batch 2369, Step 2370, Loss: 2.0323\n",
      "Resumed - Batch 2370, Step 2371, Loss: 2.4924\n",
      "Resumed - Batch 2371, Step 2372, Loss: 2.6061\n",
      "Resumed - Batch 2372, Step 2373, Loss: 2.9735\n",
      "Resumed - Batch 2373, Step 2374, Loss: 2.3267\n",
      "Resumed - Batch 2374, Step 2375, Loss: 2.3723\n",
      "Resumed - Batch 2375, Step 2376, Loss: 2.9246\n",
      "Resumed - Batch 2376, Step 2377, Loss: 2.4327\n",
      "Resumed - Batch 2377, Step 2378, Loss: 2.5426\n",
      "Resumed - Batch 2378, Step 2379, Loss: 2.5801\n",
      "Resumed - Batch 2379, Step 2380, Loss: 2.5026\n",
      "Resumed - Batch 2380, Step 2381, Loss: 2.2540\n",
      "Resumed - Batch 2381, Step 2382, Loss: 2.4657\n",
      "Resumed - Batch 2382, Step 2383, Loss: 2.5573\n",
      "Resumed - Batch 2383, Step 2384, Loss: 2.2967\n",
      "Resumed - Batch 2384, Step 2385, Loss: 2.5682\n",
      "Resumed - Batch 2385, Step 2386, Loss: 2.5077\n",
      "Resumed - Batch 2386, Step 2387, Loss: 2.6277\n",
      "Resumed - Batch 2387, Step 2388, Loss: 2.6398\n",
      "Resumed - Batch 2388, Step 2389, Loss: 2.5038\n",
      "Resumed - Batch 2389, Step 2390, Loss: 2.3340\n",
      "Resumed - Batch 2390, Step 2391, Loss: 2.4913\n",
      "Resumed - Batch 2391, Step 2392, Loss: 2.3941\n",
      "Resumed - Batch 2392, Step 2393, Loss: 2.3784\n",
      "Resumed - Batch 2393, Step 2394, Loss: 1.9742\n",
      "Resumed - Batch 2394, Step 2395, Loss: 2.6586\n",
      "Resumed - Batch 2395, Step 2396, Loss: 2.4745\n",
      "Resumed - Batch 2396, Step 2397, Loss: 2.2384\n",
      "Resumed - Batch 2397, Step 2398, Loss: 2.7991\n",
      "Resumed - Batch 2398, Step 2399, Loss: 2.6466\n",
      "Resumed - Batch 2399, Step 2400, Loss: 2.2407\n",
      "Resumed - Batch 2400, Step 2401, Loss: 2.4036\n",
      "Resumed - Batch 2401, Step 2402, Loss: 2.4526\n",
      "Resumed - Batch 2402, Step 2403, Loss: 2.4159\n",
      "Resumed - Batch 2403, Step 2404, Loss: 2.2055\n",
      "Resumed - Batch 2404, Step 2405, Loss: 2.9692\n",
      "Resumed - Batch 2405, Step 2406, Loss: 2.2593\n",
      "Resumed - Batch 2406, Step 2407, Loss: 2.3489\n",
      "Resumed - Batch 2407, Step 2408, Loss: 2.1354\n",
      "Resumed - Batch 2408, Step 2409, Loss: 2.2382\n",
      "Resumed - Batch 2409, Step 2410, Loss: 2.6346\n",
      "Resumed - Batch 2410, Step 2411, Loss: 2.3618\n",
      "Resumed - Batch 2411, Step 2412, Loss: 2.6128\n",
      "Resumed - Batch 2412, Step 2413, Loss: 2.4447\n",
      "Resumed - Batch 2413, Step 2414, Loss: 2.7186\n",
      "Resumed - Batch 2414, Step 2415, Loss: 2.5702\n",
      "Resumed - Batch 2415, Step 2416, Loss: 2.2428\n",
      "Resumed - Batch 2416, Step 2417, Loss: 2.6721\n",
      "Resumed - Batch 2417, Step 2418, Loss: 2.3927\n",
      "Resumed - Batch 2418, Step 2419, Loss: 2.3079\n",
      "Resumed - Batch 2419, Step 2420, Loss: 2.4387\n",
      "Resumed - Batch 2420, Step 2421, Loss: 2.2399\n",
      "Resumed - Batch 2421, Step 2422, Loss: 2.1649\n",
      "Resumed - Batch 2422, Step 2423, Loss: 2.9590\n",
      "Resumed - Batch 2423, Step 2424, Loss: 2.3676\n",
      "Resumed - Batch 2424, Step 2425, Loss: 2.4097\n",
      "Resumed - Batch 2425, Step 2426, Loss: 2.3925\n",
      "Resumed - Batch 2426, Step 2427, Loss: 2.4138\n",
      "Resumed - Batch 2427, Step 2428, Loss: 2.5908\n",
      "Resumed - Batch 2428, Step 2429, Loss: 2.8068\n",
      "Resumed - Batch 2429, Step 2430, Loss: 2.6402\n",
      "Resumed - Batch 2430, Step 2431, Loss: 2.5253\n",
      "Resumed - Batch 2431, Step 2432, Loss: 2.7962\n",
      "Resumed - Batch 2432, Step 2433, Loss: 2.2871\n",
      "Resumed - Batch 2433, Step 2434, Loss: 2.4727\n",
      "Resumed - Batch 2434, Step 2435, Loss: 2.2776\n",
      "Resumed - Batch 2435, Step 2436, Loss: 2.2461\n",
      "Resumed - Batch 2436, Step 2437, Loss: 2.3947\n",
      "Resumed - Batch 2437, Step 2438, Loss: 2.6058\n",
      "Resumed - Batch 2438, Step 2439, Loss: 2.6468\n",
      "Resumed - Batch 2439, Step 2440, Loss: 2.7124\n",
      "Resumed - Batch 2440, Step 2441, Loss: 2.3546\n",
      "Resumed - Batch 2441, Step 2442, Loss: 2.1423\n",
      "Resumed - Batch 2442, Step 2443, Loss: 2.9039\n",
      "Resumed - Batch 2443, Step 2444, Loss: 2.7963\n",
      "Resumed - Batch 2444, Step 2445, Loss: 2.0055\n",
      "Resumed - Batch 2445, Step 2446, Loss: 2.6050\n",
      "Resumed - Batch 2446, Step 2447, Loss: 2.6756\n",
      "Resumed - Batch 2447, Step 2448, Loss: 2.4382\n",
      "Resumed - Batch 2448, Step 2449, Loss: 2.4506\n",
      "Resumed - Batch 2449, Step 2450, Loss: 2.7069\n",
      "Resumed - Batch 2450, Step 2451, Loss: 2.5335\n",
      "Resumed - Batch 2451, Step 2452, Loss: 2.5255\n",
      "Resumed - Batch 2452, Step 2453, Loss: 2.1992\n",
      "Resumed - Batch 2453, Step 2454, Loss: 2.5331\n",
      "Resumed - Batch 2454, Step 2455, Loss: 2.7713\n",
      "Resumed - Batch 2455, Step 2456, Loss: 2.4742\n",
      "Resumed - Batch 2456, Step 2457, Loss: 2.9321\n",
      "Resumed - Batch 2457, Step 2458, Loss: 2.1694\n",
      "Resumed - Batch 2458, Step 2459, Loss: 2.2885\n",
      "Resumed - Batch 2459, Step 2460, Loss: 2.4685\n",
      "Resumed - Batch 2460, Step 2461, Loss: 2.3274\n",
      "Resumed - Batch 2461, Step 2462, Loss: 2.6936\n",
      "Resumed - Batch 2462, Step 2463, Loss: 2.3762\n",
      "Resumed - Batch 2463, Step 2464, Loss: 2.2191\n",
      "Resumed - Batch 2464, Step 2465, Loss: 2.1476\n",
      "Resumed - Batch 2465, Step 2466, Loss: 2.5072\n",
      "Resumed - Batch 2466, Step 2467, Loss: 2.7344\n",
      "Resumed - Batch 2467, Step 2468, Loss: 2.4557\n",
      "Resumed - Batch 2468, Step 2469, Loss: 2.5796\n",
      "Resumed - Batch 2469, Step 2470, Loss: 2.4907\n",
      "Resumed - Batch 2470, Step 2471, Loss: 2.7519\n",
      "Resumed - Batch 2471, Step 2472, Loss: 2.6032\n",
      "Resumed - Batch 2472, Step 2473, Loss: 2.0950\n",
      "Resumed - Batch 2473, Step 2474, Loss: 2.4604\n",
      "Resumed - Batch 2474, Step 2475, Loss: 2.6377\n",
      "Resumed - Batch 2475, Step 2476, Loss: 2.6856\n",
      "Resumed - Batch 2476, Step 2477, Loss: 2.6728\n",
      "Resumed - Batch 2477, Step 2478, Loss: 2.4990\n",
      "Resumed - Batch 2478, Step 2479, Loss: 2.4326\n",
      "Resumed - Batch 2479, Step 2480, Loss: 2.5358\n",
      "Resumed - Batch 2480, Step 2481, Loss: 2.3503\n",
      "Resumed - Batch 2481, Step 2482, Loss: 2.2973\n",
      "Resumed - Batch 2482, Step 2483, Loss: 2.3657\n",
      "Resumed - Batch 2483, Step 2484, Loss: 2.4712\n",
      "Resumed - Batch 2484, Step 2485, Loss: 2.5695\n",
      "Resumed - Batch 2485, Step 2486, Loss: 2.5742\n",
      "Resumed - Batch 2486, Step 2487, Loss: 2.5221\n",
      "Resumed - Batch 2487, Step 2488, Loss: 2.2074\n",
      "Resumed - Batch 2488, Step 2489, Loss: 2.3148\n",
      "Resumed - Batch 2489, Step 2490, Loss: 2.5770\n",
      "Resumed - Batch 2490, Step 2491, Loss: 2.3275\n",
      "Resumed - Batch 2491, Step 2492, Loss: 2.3486\n",
      "Resumed - Batch 2492, Step 2493, Loss: 2.3508\n",
      "Resumed - Batch 2493, Step 2494, Loss: 2.7472\n",
      "Resumed - Batch 2494, Step 2495, Loss: 2.2877\n",
      "Resumed - Batch 2495, Step 2496, Loss: 2.0910\n",
      "Resumed - Batch 2496, Step 2497, Loss: 2.5589\n",
      "Resumed - Batch 2497, Step 2498, Loss: 2.3124\n",
      "Resumed - Batch 2498, Step 2499, Loss: 2.2682\n",
      "Resumed - Batch 2499, Step 2500, Loss: 2.6284\n",
      "Resumed - Batch 2500, Step 2501, Loss: 2.3442\n",
      "Resumed - Batch 2501, Step 2502, Loss: 2.0050\n",
      "Resumed - Batch 2502, Step 2503, Loss: 2.3837\n",
      "Resumed - Batch 2503, Step 2504, Loss: 2.5945\n",
      "Resumed - Batch 2504, Step 2505, Loss: 2.5747\n",
      "Resumed - Batch 2505, Step 2506, Loss: 2.2494\n",
      "Resumed - Batch 2506, Step 2507, Loss: 2.3473\n",
      "Resumed - Batch 2507, Step 2508, Loss: 2.2429\n",
      "Resumed - Batch 2508, Step 2509, Loss: 2.3636\n",
      "Resumed - Batch 2509, Step 2510, Loss: 2.2953\n",
      "Resumed - Batch 2510, Step 2511, Loss: 2.4297\n",
      "Resumed - Batch 2511, Step 2512, Loss: 2.4713\n",
      "Resumed - Batch 2512, Step 2513, Loss: 2.4084\n",
      "Resumed - Batch 2513, Step 2514, Loss: 2.8570\n",
      "Resumed - Batch 2514, Step 2515, Loss: 2.4500\n",
      "Resumed - Batch 2515, Step 2516, Loss: 2.6063\n",
      "Resumed - Batch 2516, Step 2517, Loss: 2.7597\n",
      "Resumed - Batch 2517, Step 2518, Loss: 2.1963\n",
      "Resumed - Batch 2518, Step 2519, Loss: 2.2997\n",
      "Resumed - Batch 2519, Step 2520, Loss: 2.2910\n",
      "Resumed - Batch 2520, Step 2521, Loss: 2.2923\n",
      "Resumed - Batch 2521, Step 2522, Loss: 2.4854\n",
      "Resumed - Batch 2522, Step 2523, Loss: 2.3055\n",
      "Resumed - Batch 2523, Step 2524, Loss: 2.4880\n",
      "Resumed - Batch 2524, Step 2525, Loss: 2.3463\n",
      "Resumed - Batch 2525, Step 2526, Loss: 2.5308\n",
      "Resumed - Batch 2526, Step 2527, Loss: 2.4458\n",
      "Resumed - Batch 2527, Step 2528, Loss: 2.5548\n",
      "Resumed - Batch 2528, Step 2529, Loss: 2.3843\n",
      "Resumed - Batch 2529, Step 2530, Loss: 2.8303\n",
      "Resumed - Batch 2530, Step 2531, Loss: 2.3634\n",
      "Resumed - Batch 2531, Step 2532, Loss: 2.4528\n",
      "Resumed - Batch 2532, Step 2533, Loss: 2.3453\n",
      "Resumed - Batch 2533, Step 2534, Loss: 2.2775\n",
      "Resumed - Batch 2534, Step 2535, Loss: 2.3665\n",
      "Resumed - Batch 2535, Step 2536, Loss: 2.3920\n",
      "Resumed - Batch 2536, Step 2537, Loss: 2.5944\n",
      "Resumed - Batch 2537, Step 2538, Loss: 1.9024\n",
      "Resumed - Batch 2538, Step 2539, Loss: 2.8224\n",
      "Resumed - Batch 2539, Step 2540, Loss: 2.4601\n",
      "Resumed - Batch 2540, Step 2541, Loss: 2.2613\n",
      "Resumed - Batch 2541, Step 2542, Loss: 2.7411\n",
      "Resumed - Batch 2542, Step 2543, Loss: 2.2440\n",
      "Resumed - Batch 2543, Step 2544, Loss: 2.6049\n",
      "Resumed - Batch 2544, Step 2545, Loss: 2.4730\n",
      "Resumed - Batch 2545, Step 2546, Loss: 2.4928\n",
      "Resumed - Batch 2546, Step 2547, Loss: 2.3234\n",
      "Resumed - Batch 2547, Step 2548, Loss: 2.3402\n",
      "Resumed - Batch 2548, Step 2549, Loss: 2.6285\n",
      "Resumed - Batch 2549, Step 2550, Loss: 2.4693\n",
      "Resumed - Batch 2550, Step 2551, Loss: 2.9399\n",
      "Resumed - Batch 2551, Step 2552, Loss: 2.1607\n",
      "Resumed - Batch 2552, Step 2553, Loss: 2.5604\n",
      "Resumed - Batch 2553, Step 2554, Loss: 2.2898\n",
      "Resumed - Batch 2554, Step 2555, Loss: 2.6932\n",
      "Resumed - Batch 2555, Step 2556, Loss: 2.7212\n",
      "Resumed - Batch 2556, Step 2557, Loss: 2.5932\n",
      "Resumed - Batch 2557, Step 2558, Loss: 2.4038\n",
      "Resumed - Batch 2558, Step 2559, Loss: 2.3507\n",
      "Resumed - Batch 2559, Step 2560, Loss: 2.6369\n",
      "Resumed - Batch 2560, Step 2561, Loss: 2.4327\n",
      "Resumed - Batch 2561, Step 2562, Loss: 2.5889\n",
      "Resumed - Batch 2562, Step 2563, Loss: 2.5993\n",
      "Resumed - Batch 2563, Step 2564, Loss: 2.5839\n",
      "Resumed - Batch 2564, Step 2565, Loss: 2.5130\n",
      "Resumed - Batch 2565, Step 2566, Loss: 2.5177\n",
      "Resumed - Batch 2566, Step 2567, Loss: 2.8592\n",
      "Resumed - Batch 2567, Step 2568, Loss: 2.6477\n",
      "Resumed - Batch 2568, Step 2569, Loss: 2.2464\n",
      "Resumed - Batch 2569, Step 2570, Loss: 2.6912\n",
      "Resumed - Batch 2570, Step 2571, Loss: 2.2557\n",
      "Resumed - Batch 2571, Step 2572, Loss: 2.4786\n",
      "Resumed - Batch 2572, Step 2573, Loss: 2.5394\n",
      "Resumed - Batch 2573, Step 2574, Loss: 2.4935\n",
      "Resumed - Batch 2574, Step 2575, Loss: 2.3840\n",
      "Resumed - Batch 2575, Step 2576, Loss: 2.3594\n",
      "Resumed - Batch 2576, Step 2577, Loss: 2.5076\n",
      "Resumed - Batch 2577, Step 2578, Loss: 2.2124\n",
      "Resumed - Batch 2578, Step 2579, Loss: 2.6044\n",
      "Resumed - Batch 2579, Step 2580, Loss: 2.6892\n",
      "Resumed - Batch 2580, Step 2581, Loss: 2.4315\n",
      "Resumed - Batch 2581, Step 2582, Loss: 2.4526\n",
      "Resumed - Batch 2582, Step 2583, Loss: 2.4094\n",
      "Resumed - Batch 2583, Step 2584, Loss: 2.4014\n",
      "Resumed - Batch 2584, Step 2585, Loss: 2.3696\n",
      "Resumed - Batch 2585, Step 2586, Loss: 2.6111\n",
      "Resumed - Batch 2586, Step 2587, Loss: 2.5178\n",
      "Resumed - Batch 2587, Step 2588, Loss: 2.3565\n",
      "Resumed - Batch 2588, Step 2589, Loss: 2.1877\n",
      "Resumed - Batch 2589, Step 2590, Loss: 2.3735\n",
      "Resumed - Batch 2590, Step 2591, Loss: 2.2133\n",
      "Resumed - Batch 2591, Step 2592, Loss: 2.6103\n",
      "Resumed - Batch 2592, Step 2593, Loss: 2.3181\n",
      "Resumed - Batch 2593, Step 2594, Loss: 2.7938\n",
      "Resumed - Batch 2594, Step 2595, Loss: 2.3341\n",
      "Resumed - Batch 2595, Step 2596, Loss: 2.6523\n",
      "Resumed - Batch 2596, Step 2597, Loss: 2.6653\n",
      "Resumed - Batch 2597, Step 2598, Loss: 2.6044\n",
      "Resumed - Batch 2598, Step 2599, Loss: 2.6061\n",
      "Resumed - Batch 2599, Step 2600, Loss: 2.3747\n",
      "Resumed - Batch 2600, Step 2601, Loss: 2.7652\n",
      "Resumed - Batch 2601, Step 2602, Loss: 2.2014\n",
      "Resumed - Batch 2602, Step 2603, Loss: 2.3613\n",
      "Resumed - Batch 2603, Step 2604, Loss: 2.2441\n",
      "Resumed - Batch 2604, Step 2605, Loss: 2.3232\n",
      "Resumed - Batch 2605, Step 2606, Loss: 2.1341\n",
      "Resumed - Batch 2606, Step 2607, Loss: 2.3925\n",
      "Resumed - Batch 2607, Step 2608, Loss: 2.4238\n",
      "Resumed - Batch 2608, Step 2609, Loss: 2.4019\n",
      "Resumed - Batch 2609, Step 2610, Loss: 2.1007\n",
      "Resumed - Batch 2610, Step 2611, Loss: 2.5118\n",
      "Resumed - Batch 2611, Step 2612, Loss: 2.3821\n",
      "Resumed - Batch 2612, Step 2613, Loss: 2.3416\n",
      "Resumed - Batch 2613, Step 2614, Loss: 2.5231\n",
      "Resumed - Batch 2614, Step 2615, Loss: 2.4258\n",
      "Resumed - Batch 2615, Step 2616, Loss: 2.3764\n",
      "Resumed - Batch 2616, Step 2617, Loss: 2.3529\n",
      "Resumed - Batch 2617, Step 2618, Loss: 2.4055\n",
      "Resumed - Batch 2618, Step 2619, Loss: 2.5853\n",
      "Resumed - Batch 2619, Step 2620, Loss: 1.9795\n",
      "Resumed - Batch 2620, Step 2621, Loss: 2.2625\n",
      "Resumed - Batch 2621, Step 2622, Loss: 2.4999\n",
      "Resumed - Batch 2622, Step 2623, Loss: 2.1835\n",
      "Resumed - Batch 2623, Step 2624, Loss: 2.2970\n",
      "Resumed - Batch 2624, Step 2625, Loss: 2.2601\n",
      "Resumed - Batch 2625, Step 2626, Loss: 2.3672\n",
      "Resumed - Batch 2626, Step 2627, Loss: 2.3751\n",
      "Resumed - Batch 2627, Step 2628, Loss: 2.5704\n",
      "Resumed - Batch 2628, Step 2629, Loss: 2.3327\n",
      "Resumed - Batch 2629, Step 2630, Loss: 2.4145\n",
      "Resumed - Batch 2630, Step 2631, Loss: 2.5962\n",
      "Resumed - Batch 2631, Step 2632, Loss: 2.5041\n",
      "Resumed - Batch 2632, Step 2633, Loss: 2.1799\n",
      "Resumed - Batch 2633, Step 2634, Loss: 2.6757\n",
      "Resumed - Batch 2634, Step 2635, Loss: 2.3822\n",
      "Resumed - Batch 2635, Step 2636, Loss: 2.4414\n",
      "Resumed - Batch 2636, Step 2637, Loss: 2.1193\n",
      "Resumed - Batch 2637, Step 2638, Loss: 2.3862\n",
      "Resumed - Batch 2638, Step 2639, Loss: 2.3790\n",
      "Resumed - Batch 2639, Step 2640, Loss: 2.5514\n",
      "Resumed - Batch 2640, Step 2641, Loss: 2.6935\n",
      "Resumed - Batch 2641, Step 2642, Loss: 2.2813\n",
      "Resumed - Batch 2642, Step 2643, Loss: 2.2152\n",
      "Resumed - Batch 2643, Step 2644, Loss: 2.4436\n",
      "Resumed - Batch 2644, Step 2645, Loss: 1.9990\n",
      "Resumed - Batch 2645, Step 2646, Loss: 2.1399\n",
      "Resumed - Batch 2646, Step 2647, Loss: 2.2983\n",
      "Resumed - Batch 2647, Step 2648, Loss: 2.1723\n",
      "Resumed - Batch 2648, Step 2649, Loss: 2.2935\n",
      "Resumed - Batch 2649, Step 2650, Loss: 2.5252\n",
      "Resumed - Batch 2650, Step 2651, Loss: 2.5211\n",
      "Resumed - Batch 2651, Step 2652, Loss: 2.4382\n",
      "Resumed - Batch 2652, Step 2653, Loss: 2.3303\n",
      "Resumed - Batch 2653, Step 2654, Loss: 2.6186\n",
      "Resumed - Batch 2654, Step 2655, Loss: 2.2762\n",
      "Resumed - Batch 2655, Step 2656, Loss: 2.3976\n",
      "Resumed - Batch 2656, Step 2657, Loss: 2.2740\n",
      "Resumed - Batch 2657, Step 2658, Loss: 2.6528\n",
      "Resumed - Batch 2658, Step 2659, Loss: 2.2022\n",
      "Resumed - Batch 2659, Step 2660, Loss: 2.4525\n",
      "Resumed - Batch 2660, Step 2661, Loss: 2.5316\n",
      "Resumed - Batch 2661, Step 2662, Loss: 2.6350\n",
      "Resumed - Batch 2662, Step 2663, Loss: 2.6455\n",
      "Resumed - Batch 2663, Step 2664, Loss: 2.2713\n",
      "Resumed - Batch 2664, Step 2665, Loss: 2.7378\n",
      "Resumed - Batch 2665, Step 2666, Loss: 2.3625\n",
      "Resumed - Batch 2666, Step 2667, Loss: 2.7742\n",
      "Resumed - Batch 2667, Step 2668, Loss: 2.7935\n",
      "Resumed - Batch 2668, Step 2669, Loss: 2.3775\n",
      "Resumed - Batch 2669, Step 2670, Loss: 2.4303\n",
      "Resumed - Batch 2670, Step 2671, Loss: 2.5051\n",
      "Resumed - Batch 2671, Step 2672, Loss: 2.3311\n",
      "Resumed - Batch 2672, Step 2673, Loss: 2.2792\n",
      "Resumed - Batch 2673, Step 2674, Loss: 2.6678\n",
      "Resumed - Batch 2674, Step 2675, Loss: 2.3860\n",
      "Resumed - Batch 2675, Step 2676, Loss: 2.3667\n",
      "Resumed - Batch 2676, Step 2677, Loss: 2.6751\n",
      "Resumed - Batch 2677, Step 2678, Loss: 2.6487\n",
      "Resumed - Batch 2678, Step 2679, Loss: 2.4840\n",
      "Resumed - Batch 2679, Step 2680, Loss: 1.9204\n",
      "Resumed - Batch 2680, Step 2681, Loss: 2.6044\n",
      "Resumed - Batch 2681, Step 2682, Loss: 2.5385\n",
      "Resumed - Batch 2682, Step 2683, Loss: 2.6506\n",
      "Resumed - Batch 2683, Step 2684, Loss: 2.6933\n",
      "Resumed - Batch 2684, Step 2685, Loss: 2.5413\n",
      "Resumed - Batch 2685, Step 2686, Loss: 2.2355\n",
      "Resumed - Batch 2686, Step 2687, Loss: 2.3194\n",
      "Resumed - Batch 2687, Step 2688, Loss: 2.4254\n",
      "Resumed - Batch 2688, Step 2689, Loss: 2.2274\n",
      "Resumed - Batch 2689, Step 2690, Loss: 2.3520\n",
      "Resumed - Batch 2690, Step 2691, Loss: 2.3377\n",
      "Resumed - Batch 2691, Step 2692, Loss: 2.4856\n",
      "Resumed - Batch 2692, Step 2693, Loss: 2.5424\n",
      "Resumed - Batch 2693, Step 2694, Loss: 2.4071\n",
      "Resumed - Batch 2694, Step 2695, Loss: 2.2202\n",
      "Resumed - Batch 2695, Step 2696, Loss: 2.5482\n",
      "Resumed - Batch 2696, Step 2697, Loss: 2.1523\n",
      "Resumed - Batch 2697, Step 2698, Loss: 2.4166\n",
      "Resumed - Batch 2698, Step 2699, Loss: 2.5793\n",
      "Resumed - Batch 2699, Step 2700, Loss: 2.3364\n",
      "Resumed - Batch 2700, Step 2701, Loss: 2.4874\n",
      "Resumed - Batch 2701, Step 2702, Loss: 2.0655\n",
      "Resumed - Batch 2702, Step 2703, Loss: 3.0093\n",
      "Resumed - Batch 2703, Step 2704, Loss: 2.4523\n",
      "Resumed - Batch 2704, Step 2705, Loss: 2.5884\n",
      "Resumed - Batch 2705, Step 2706, Loss: 2.5739\n",
      "Resumed - Batch 2706, Step 2707, Loss: 2.4782\n",
      "Resumed - Batch 2707, Step 2708, Loss: 2.4142\n",
      "Resumed - Batch 2708, Step 2709, Loss: 2.1155\n",
      "Resumed - Batch 2709, Step 2710, Loss: 2.6011\n",
      "Resumed - Batch 2710, Step 2711, Loss: 2.2559\n",
      "Resumed - Batch 2711, Step 2712, Loss: 2.4014\n",
      "Resumed - Batch 2712, Step 2713, Loss: 2.3472\n",
      "Resumed - Batch 2713, Step 2714, Loss: 2.4939\n",
      "Resumed - Batch 2714, Step 2715, Loss: 2.1253\n",
      "Resumed - Batch 2715, Step 2716, Loss: 2.1688\n",
      "Resumed - Batch 2716, Step 2717, Loss: 2.3059\n",
      "Resumed - Batch 2717, Step 2718, Loss: 2.1404\n",
      "Resumed - Batch 2718, Step 2719, Loss: 2.5145\n",
      "Resumed - Batch 2719, Step 2720, Loss: 2.4858\n",
      "Resumed - Batch 2720, Step 2721, Loss: 2.7511\n",
      "Resumed - Batch 2721, Step 2722, Loss: 2.5264\n",
      "Resumed - Batch 2722, Step 2723, Loss: 2.3493\n",
      "Resumed - Batch 2723, Step 2724, Loss: 2.4643\n",
      "Resumed - Batch 2724, Step 2725, Loss: 2.4880\n",
      "Resumed - Batch 2725, Step 2726, Loss: 2.6741\n",
      "Resumed - Batch 2726, Step 2727, Loss: 2.4912\n",
      "Resumed - Batch 2727, Step 2728, Loss: 2.3489\n",
      "Resumed - Batch 2728, Step 2729, Loss: 2.8280\n",
      "Resumed - Batch 2729, Step 2730, Loss: 2.3632\n",
      "Resumed - Batch 2730, Step 2731, Loss: 2.5510\n",
      "Resumed - Batch 2731, Step 2732, Loss: 2.3245\n",
      "Resumed - Batch 2732, Step 2733, Loss: 2.4931\n",
      "Resumed - Batch 2733, Step 2734, Loss: 2.0688\n",
      "Resumed - Batch 2734, Step 2735, Loss: 2.4076\n",
      "Resumed - Batch 2735, Step 2736, Loss: 2.5678\n",
      "Resumed - Batch 2736, Step 2737, Loss: 2.3564\n",
      "Resumed - Batch 2737, Step 2738, Loss: 2.3298\n",
      "Resumed - Batch 2738, Step 2739, Loss: 2.3266\n",
      "Resumed - Batch 2739, Step 2740, Loss: 2.6343\n",
      "Resumed - Batch 2740, Step 2741, Loss: 2.5167\n",
      "Resumed - Batch 2741, Step 2742, Loss: 2.4657\n",
      "Resumed - Batch 2742, Step 2743, Loss: 2.7414\n",
      "Resumed - Batch 2743, Step 2744, Loss: 2.5908\n",
      "Resumed - Batch 2744, Step 2745, Loss: 2.5556\n",
      "Resumed - Batch 2745, Step 2746, Loss: 2.5391\n",
      "Resumed - Batch 2746, Step 2747, Loss: 2.4414\n",
      "Resumed - Batch 2747, Step 2748, Loss: 2.3360\n",
      "Resumed - Batch 2748, Step 2749, Loss: 2.1648\n",
      "Resumed - Batch 2749, Step 2750, Loss: 2.3060\n",
      "Resumed - Batch 2750, Step 2751, Loss: 2.3868\n",
      "Resumed - Batch 2751, Step 2752, Loss: 2.3620\n",
      "Resumed - Batch 2752, Step 2753, Loss: 2.1340\n",
      "Resumed - Batch 2753, Step 2754, Loss: 2.4728\n",
      "Resumed - Batch 2754, Step 2755, Loss: 2.6172\n",
      "Resumed - Batch 2755, Step 2756, Loss: 2.1404\n",
      "Resumed - Batch 2756, Step 2757, Loss: 2.0796\n",
      "Resumed - Batch 2757, Step 2758, Loss: 2.3745\n",
      "Resumed - Batch 2758, Step 2759, Loss: 2.6202\n",
      "Resumed - Batch 2759, Step 2760, Loss: 2.6976\n",
      "Resumed - Batch 2760, Step 2761, Loss: 2.1846\n",
      "Resumed - Batch 2761, Step 2762, Loss: 2.6818\n",
      "Resumed - Batch 2762, Step 2763, Loss: 2.1962\n",
      "Resumed - Batch 2763, Step 2764, Loss: 2.5928\n",
      "Resumed - Batch 2764, Step 2765, Loss: 2.1328\n",
      "Resumed - Batch 2765, Step 2766, Loss: 2.4283\n",
      "Resumed - Batch 2766, Step 2767, Loss: 2.5351\n",
      "Resumed - Batch 2767, Step 2768, Loss: 2.4205\n",
      "Resumed - Batch 2768, Step 2769, Loss: 2.6626\n",
      "Resumed - Batch 2769, Step 2770, Loss: 2.3696\n",
      "Resumed - Batch 2770, Step 2771, Loss: 2.2979\n",
      "Resumed - Batch 2771, Step 2772, Loss: 2.5066\n",
      "Resumed - Batch 2772, Step 2773, Loss: 2.3527\n",
      "Resumed - Batch 2773, Step 2774, Loss: 2.8127\n",
      "Resumed - Batch 2774, Step 2775, Loss: 2.3843\n",
      "Resumed - Batch 2775, Step 2776, Loss: 2.0817\n",
      "Resumed - Batch 2776, Step 2777, Loss: 2.5016\n",
      "Resumed - Batch 2777, Step 2778, Loss: 2.0884\n",
      "Resumed - Batch 2778, Step 2779, Loss: 1.9850\n",
      "Resumed - Batch 2779, Step 2780, Loss: 2.3762\n",
      "Resumed - Batch 2780, Step 2781, Loss: 2.5170\n",
      "Resumed - Batch 2781, Step 2782, Loss: 2.2700\n",
      "Resumed - Batch 2782, Step 2783, Loss: 2.1020\n",
      "Resumed - Batch 2783, Step 2784, Loss: 2.0536\n",
      "Resumed - Batch 2784, Step 2785, Loss: 2.5096\n",
      "Resumed - Batch 2785, Step 2786, Loss: 2.2977\n",
      "Resumed - Batch 2786, Step 2787, Loss: 2.2831\n",
      "Resumed - Batch 2787, Step 2788, Loss: 2.7038\n",
      "Resumed - Batch 2788, Step 2789, Loss: 2.0292\n",
      "Resumed - Batch 2789, Step 2790, Loss: 2.3522\n",
      "Resumed - Batch 2790, Step 2791, Loss: 2.5327\n",
      "Resumed - Batch 2791, Step 2792, Loss: 2.1427\n",
      "Resumed - Batch 2792, Step 2793, Loss: 2.3511\n",
      "Resumed - Batch 2793, Step 2794, Loss: 2.2079\n",
      "Resumed - Batch 2794, Step 2795, Loss: 2.1616\n",
      "Resumed - Batch 2795, Step 2796, Loss: 2.6975\n",
      "Resumed - Batch 2796, Step 2797, Loss: 2.1346\n",
      "Resumed - Batch 2797, Step 2798, Loss: 2.1615\n",
      "Resumed - Batch 2798, Step 2799, Loss: 2.3781\n",
      "Resumed - Batch 2799, Step 2800, Loss: 2.0062\n",
      "Resumed - Batch 2800, Step 2801, Loss: 2.3071\n",
      "Resumed - Batch 2801, Step 2802, Loss: 2.1772\n",
      "Resumed - Batch 2802, Step 2803, Loss: 2.5765\n",
      "Resumed - Batch 2803, Step 2804, Loss: 2.4759\n",
      "Resumed - Batch 2804, Step 2805, Loss: 2.2919\n",
      "Resumed - Batch 2805, Step 2806, Loss: 2.1469\n",
      "Resumed - Batch 2806, Step 2807, Loss: 2.4201\n",
      "Resumed - Batch 2807, Step 2808, Loss: 2.2958\n",
      "Resumed - Batch 2808, Step 2809, Loss: 2.5981\n",
      "Resumed - Batch 2809, Step 2810, Loss: 2.0093\n",
      "Resumed - Batch 2810, Step 2811, Loss: 2.2689\n",
      "Resumed - Batch 2811, Step 2812, Loss: 2.7551\n",
      "Resumed - Batch 2812, Step 2813, Loss: 2.7340\n",
      "Resumed - Batch 2813, Step 2814, Loss: 2.2008\n",
      "Resumed - Batch 2814, Step 2815, Loss: 2.5544\n",
      "Resumed - Batch 2815, Step 2816, Loss: 2.1816\n",
      "Resumed - Batch 2816, Step 2817, Loss: 2.1130\n",
      "Resumed - Batch 2817, Step 2818, Loss: 2.7045\n",
      "Resumed - Batch 2818, Step 2819, Loss: 2.2749\n",
      "Resumed - Batch 2819, Step 2820, Loss: 2.1365\n",
      "Resumed - Batch 2820, Step 2821, Loss: 2.1421\n",
      "Resumed - Batch 2821, Step 2822, Loss: 2.2677\n",
      "Resumed - Batch 2822, Step 2823, Loss: 2.3957\n",
      "Resumed - Batch 2823, Step 2824, Loss: 2.1821\n",
      "Resumed - Batch 2824, Step 2825, Loss: 2.5258\n",
      "Resumed - Batch 2825, Step 2826, Loss: 2.4737\n",
      "Resumed - Batch 2826, Step 2827, Loss: 2.4838\n",
      "Resumed - Batch 2827, Step 2828, Loss: 2.3519\n",
      "Resumed - Batch 2828, Step 2829, Loss: 2.2940\n",
      "Resumed - Batch 2829, Step 2830, Loss: 2.3910\n",
      "Resumed - Batch 2830, Step 2831, Loss: 2.2104\n",
      "Resumed - Batch 2831, Step 2832, Loss: 2.6242\n",
      "Resumed - Batch 2832, Step 2833, Loss: 2.3674\n",
      "Resumed - Batch 2833, Step 2834, Loss: 2.2131\n",
      "Resumed - Batch 2834, Step 2835, Loss: 2.2145\n",
      "Resumed - Batch 2835, Step 2836, Loss: 2.5730\n",
      "Resumed - Batch 2836, Step 2837, Loss: 2.2416\n",
      "Resumed - Batch 2837, Step 2838, Loss: 2.3182\n",
      "Resumed - Batch 2838, Step 2839, Loss: 2.5394\n",
      "Resumed - Batch 2839, Step 2840, Loss: 2.6718\n",
      "Resumed - Batch 2840, Step 2841, Loss: 2.6571\n",
      "Resumed - Batch 2841, Step 2842, Loss: 1.7066\n",
      "Resumed - Batch 2842, Step 2843, Loss: 2.2768\n",
      "Resumed - Batch 2843, Step 2844, Loss: 2.4855\n",
      "Resumed - Batch 2844, Step 2845, Loss: 2.5944\n",
      "Resumed - Batch 2845, Step 2846, Loss: 2.0591\n",
      "Resumed - Batch 2846, Step 2847, Loss: 2.1431\n",
      "Resumed - Batch 2847, Step 2848, Loss: 2.6022\n",
      "Resumed - Batch 2848, Step 2849, Loss: 2.3943\n",
      "Resumed - Batch 2849, Step 2850, Loss: 2.5548\n",
      "Resumed - Batch 2850, Step 2851, Loss: 2.6827\n",
      "Resumed - Batch 2851, Step 2852, Loss: 2.3480\n",
      "Resumed - Batch 2852, Step 2853, Loss: 2.5201\n",
      "Resumed - Batch 2853, Step 2854, Loss: 2.2584\n",
      "Resumed - Batch 2854, Step 2855, Loss: 2.3770\n",
      "Resumed - Batch 2855, Step 2856, Loss: 2.3971\n",
      "Resumed - Batch 2856, Step 2857, Loss: 2.3598\n",
      "Resumed - Batch 2857, Step 2858, Loss: 2.2384\n",
      "Resumed - Batch 2858, Step 2859, Loss: 2.6325\n",
      "Resumed - Batch 2859, Step 2860, Loss: 2.2367\n",
      "Resumed - Batch 2860, Step 2861, Loss: 2.4761\n",
      "Resumed - Batch 2861, Step 2862, Loss: 2.4620\n",
      "Resumed - Batch 2862, Step 2863, Loss: 2.4096\n",
      "Resumed - Batch 2863, Step 2864, Loss: 2.3570\n",
      "Resumed - Batch 2864, Step 2865, Loss: 2.6969\n",
      "Resumed - Batch 2865, Step 2866, Loss: 1.8614\n",
      "Resumed - Batch 2866, Step 2867, Loss: 1.7707\n",
      "Resumed - Batch 2867, Step 2868, Loss: 2.0422\n",
      "Resumed - Batch 2868, Step 2869, Loss: 2.6216\n",
      "Resumed - Batch 2869, Step 2870, Loss: 1.9455\n",
      "Resumed - Batch 2870, Step 2871, Loss: 2.2858\n",
      "Resumed - Batch 2871, Step 2872, Loss: 2.3543\n",
      "Resumed - Batch 2872, Step 2873, Loss: 2.0786\n",
      "Resumed - Batch 2873, Step 2874, Loss: 2.3077\n",
      "Resumed - Batch 2874, Step 2875, Loss: 2.1666\n",
      "Resumed - Batch 2875, Step 2876, Loss: 2.3387\n",
      "Resumed - Batch 2876, Step 2877, Loss: 2.2236\n",
      "Resumed - Batch 2877, Step 2878, Loss: 2.0938\n",
      "Resumed - Batch 2878, Step 2879, Loss: 2.4229\n",
      "Resumed - Batch 2879, Step 2880, Loss: 2.5822\n",
      "Resumed - Batch 2880, Step 2881, Loss: 2.2857\n",
      "Resumed - Batch 2881, Step 2882, Loss: 2.3902\n",
      "Resumed - Batch 2882, Step 2883, Loss: 2.7952\n",
      "Resumed - Batch 2883, Step 2884, Loss: 2.5998\n",
      "Resumed - Batch 2884, Step 2885, Loss: 2.1252\n",
      "Resumed - Batch 2885, Step 2886, Loss: 2.3652\n",
      "Resumed - Batch 2886, Step 2887, Loss: 2.7453\n",
      "Resumed - Batch 2887, Step 2888, Loss: 2.3814\n",
      "Resumed - Batch 2888, Step 2889, Loss: 2.4022\n",
      "Resumed - Batch 2889, Step 2890, Loss: 2.1021\n",
      "Resumed - Batch 2890, Step 2891, Loss: 2.3357\n",
      "Resumed - Batch 2891, Step 2892, Loss: 2.0467\n",
      "Resumed - Batch 2892, Step 2893, Loss: 2.1787\n",
      "Resumed - Batch 2893, Step 2894, Loss: 2.2253\n",
      "Resumed - Batch 2894, Step 2895, Loss: 2.0945\n",
      "Resumed - Batch 2895, Step 2896, Loss: 2.3979\n",
      "Resumed - Batch 2896, Step 2897, Loss: 2.2516\n",
      "Resumed - Batch 2897, Step 2898, Loss: 2.2206\n",
      "Resumed - Batch 2898, Step 2899, Loss: 2.4536\n",
      "Resumed - Batch 2899, Step 2900, Loss: 2.3956\n",
      "Resumed - Batch 2900, Step 2901, Loss: 2.2211\n",
      "Resumed - Batch 2901, Step 2902, Loss: 2.3727\n",
      "Resumed - Batch 2902, Step 2903, Loss: 2.3877\n",
      "Resumed - Batch 2903, Step 2904, Loss: 2.3307\n",
      "Resumed - Batch 2904, Step 2905, Loss: 2.5465\n",
      "Resumed - Batch 2905, Step 2906, Loss: 2.2379\n",
      "Resumed - Batch 2906, Step 2907, Loss: 2.3957\n",
      "Resumed - Batch 2907, Step 2908, Loss: 2.3642\n",
      "Resumed - Batch 2908, Step 2909, Loss: 2.0186\n",
      "Resumed - Batch 2909, Step 2910, Loss: 2.4362\n",
      "Resumed - Batch 2910, Step 2911, Loss: 2.0369\n",
      "Resumed - Batch 2911, Step 2912, Loss: 2.4617\n",
      "Resumed - Batch 2912, Step 2913, Loss: 2.3614\n",
      "Resumed - Batch 2913, Step 2914, Loss: 2.6244\n",
      "Resumed - Batch 2914, Step 2915, Loss: 2.0130\n",
      "Resumed - Batch 2915, Step 2916, Loss: 2.3830\n",
      "Resumed - Batch 2916, Step 2917, Loss: 2.2708\n",
      "Resumed - Batch 2917, Step 2918, Loss: 2.5271\n",
      "Resumed - Batch 2918, Step 2919, Loss: 2.4589\n",
      "Resumed - Batch 2919, Step 2920, Loss: 2.3941\n",
      "Resumed - Batch 2920, Step 2921, Loss: 2.4509\n",
      "Resumed - Batch 2921, Step 2922, Loss: 2.3138\n",
      "Resumed - Batch 2922, Step 2923, Loss: 2.3435\n",
      "Resumed - Batch 2923, Step 2924, Loss: 2.4234\n",
      "Resumed - Batch 2924, Step 2925, Loss: 2.0404\n",
      "Resumed - Batch 2925, Step 2926, Loss: 2.1060\n",
      "Resumed - Batch 2926, Step 2927, Loss: 1.8007\n",
      "Resumed - Batch 2927, Step 2928, Loss: 2.2555\n",
      "Resumed - Batch 2928, Step 2929, Loss: 2.3752\n",
      "Resumed - Batch 2929, Step 2930, Loss: 2.5440\n",
      "Resumed - Batch 2930, Step 2931, Loss: 2.3230\n",
      "Resumed - Batch 2931, Step 2932, Loss: 2.0449\n",
      "Resumed - Batch 2932, Step 2933, Loss: 2.4705\n",
      "Resumed - Batch 2933, Step 2934, Loss: 2.4228\n",
      "Resumed - Batch 2934, Step 2935, Loss: 1.8681\n",
      "Resumed - Batch 2935, Step 2936, Loss: 2.4675\n",
      "Resumed - Batch 2936, Step 2937, Loss: 2.1725\n",
      "Resumed - Batch 2937, Step 2938, Loss: 2.2904\n",
      "Resumed - Batch 2938, Step 2939, Loss: 2.4912\n",
      "Resumed - Batch 2939, Step 2940, Loss: 2.5082\n",
      "Resumed - Batch 2940, Step 2941, Loss: 2.2973\n",
      "Resumed - Batch 2941, Step 2942, Loss: 2.3956\n",
      "Resumed - Batch 2942, Step 2943, Loss: 2.5625\n",
      "Resumed - Batch 2943, Step 2944, Loss: 2.6024\n",
      "Resumed - Batch 2944, Step 2945, Loss: 2.3238\n",
      "Resumed - Batch 2945, Step 2946, Loss: 1.9145\n",
      "Resumed - Batch 2946, Step 2947, Loss: 2.4244\n",
      "Resumed - Batch 2947, Step 2948, Loss: 2.5911\n",
      "Resumed - Batch 2948, Step 2949, Loss: 2.2262\n",
      "Resumed - Batch 2949, Step 2950, Loss: 2.4740\n",
      "Resumed - Batch 2950, Step 2951, Loss: 2.1412\n",
      "Resumed - Batch 2951, Step 2952, Loss: 2.4979\n",
      "Resumed - Batch 2952, Step 2953, Loss: 2.2379\n",
      "Resumed - Batch 2953, Step 2954, Loss: 2.3867\n",
      "Resumed - Batch 2954, Step 2955, Loss: 2.2944\n",
      "Resumed - Batch 2955, Step 2956, Loss: 2.3117\n",
      "Resumed - Batch 2956, Step 2957, Loss: 1.9675\n",
      "Resumed - Batch 2957, Step 2958, Loss: 2.5239\n",
      "Resumed - Batch 2958, Step 2959, Loss: 2.0887\n",
      "Resumed - Batch 2959, Step 2960, Loss: 2.1996\n",
      "Resumed - Batch 2960, Step 2961, Loss: 2.0344\n",
      "Resumed - Batch 2961, Step 2962, Loss: 2.4314\n",
      "Resumed - Batch 2962, Step 2963, Loss: 2.5837\n",
      "Resumed - Batch 2963, Step 2964, Loss: 2.1613\n",
      "Resumed - Batch 2964, Step 2965, Loss: 2.2115\n",
      "Resumed - Batch 2965, Step 2966, Loss: 2.3816\n",
      "Resumed - Batch 2966, Step 2967, Loss: 2.3636\n",
      "Resumed - Batch 2967, Step 2968, Loss: 2.1150\n",
      "Resumed - Batch 2968, Step 2969, Loss: 2.0685\n",
      "Resumed - Batch 2969, Step 2970, Loss: 1.8505\n",
      "Resumed - Batch 2970, Step 2971, Loss: 2.2161\n",
      "Resumed - Batch 2971, Step 2972, Loss: 2.2198\n",
      "Resumed - Batch 2972, Step 2973, Loss: 2.0276\n",
      "Resumed - Batch 2973, Step 2974, Loss: 2.2630\n",
      "Resumed - Batch 2974, Step 2975, Loss: 2.4466\n",
      "Resumed - Batch 2975, Step 2976, Loss: 2.7614\n",
      "Resumed - Batch 2976, Step 2977, Loss: 2.3734\n",
      "Resumed - Batch 2977, Step 2978, Loss: 2.3125\n",
      "Resumed - Batch 2978, Step 2979, Loss: 2.5588\n",
      "Resumed - Batch 2979, Step 2980, Loss: 2.2037\n",
      "Resumed - Batch 2980, Step 2981, Loss: 2.5984\n",
      "Resumed - Batch 2981, Step 2982, Loss: 2.4700\n",
      "Resumed - Batch 2982, Step 2983, Loss: 2.0442\n",
      "Resumed - Batch 2983, Step 2984, Loss: 2.0264\n",
      "Resumed - Batch 2984, Step 2985, Loss: 2.4808\n",
      "Resumed - Batch 2985, Step 2986, Loss: 2.4202\n",
      "Resumed - Batch 2986, Step 2987, Loss: 2.2349\n",
      "Resumed - Batch 2987, Step 2988, Loss: 2.2589\n",
      "Resumed - Batch 2988, Step 2989, Loss: 2.4228\n",
      "Resumed - Batch 2989, Step 2990, Loss: 2.3321\n",
      "Resumed - Batch 2990, Step 2991, Loss: 2.4254\n",
      "Resumed - Batch 2991, Step 2992, Loss: 2.1446\n",
      "Resumed - Batch 2992, Step 2993, Loss: 2.1898\n",
      "Resumed - Batch 2993, Step 2994, Loss: 2.1916\n",
      "Resumed - Batch 2994, Step 2995, Loss: 2.5482\n",
      "Resumed - Batch 2995, Step 2996, Loss: 1.6764\n",
      "Resumed - Batch 2996, Step 2997, Loss: 2.2623\n",
      "Resumed - Batch 2997, Step 2998, Loss: 2.4282\n",
      "Resumed - Batch 2998, Step 2999, Loss: 2.1802\n",
      "Resumed - Batch 2999, Step 3000, Loss: 2.4424\n",
      "Resumed - Batch 3000, Step 3001, Loss: 2.3407\n",
      "Resumed - Batch 3001, Step 3002, Loss: 2.4587\n",
      "Resumed - Batch 3002, Step 3003, Loss: 2.6786\n",
      "Resumed - Batch 3003, Step 3004, Loss: 2.3281\n",
      "Resumed - Batch 3004, Step 3005, Loss: 2.6597\n",
      "Resumed - Batch 3005, Step 3006, Loss: 2.2186\n",
      "Resumed - Batch 3006, Step 3007, Loss: 2.4298\n",
      "Resumed - Batch 3007, Step 3008, Loss: 2.1522\n",
      "Resumed - Batch 3008, Step 3009, Loss: 2.3511\n",
      "Resumed - Batch 3009, Step 3010, Loss: 2.1868\n",
      "Resumed - Batch 3010, Step 3011, Loss: 2.3734\n",
      "Resumed - Batch 3011, Step 3012, Loss: 2.7245\n",
      "Resumed - Batch 3012, Step 3013, Loss: 2.3645\n",
      "Resumed - Batch 3013, Step 3014, Loss: 2.3252\n",
      "Resumed - Batch 3014, Step 3015, Loss: 2.3384\n",
      "Resumed - Batch 3015, Step 3016, Loss: 2.3231\n",
      "Resumed - Batch 3016, Step 3017, Loss: 2.2991\n",
      "Resumed - Batch 3017, Step 3018, Loss: 2.1305\n",
      "Resumed - Batch 3018, Step 3019, Loss: 2.0845\n",
      "Resumed - Batch 3019, Step 3020, Loss: 2.4453\n",
      "Resumed - Batch 3020, Step 3021, Loss: 2.5426\n",
      "Resumed - Batch 3021, Step 3022, Loss: 2.3545\n",
      "Resumed - Batch 3022, Step 3023, Loss: 2.3655\n",
      "Resumed - Batch 3023, Step 3024, Loss: 2.4659\n",
      "Resumed - Batch 3024, Step 3025, Loss: 2.3962\n",
      "Resumed - Batch 3025, Step 3026, Loss: 2.1036\n",
      "Resumed - Batch 3026, Step 3027, Loss: 2.2660\n",
      "Resumed - Batch 3027, Step 3028, Loss: 2.4392\n",
      "Resumed - Batch 3028, Step 3029, Loss: 1.8968\n",
      "Resumed - Batch 3029, Step 3030, Loss: 2.2361\n",
      "Resumed - Batch 3030, Step 3031, Loss: 2.0071\n",
      "Resumed - Batch 3031, Step 3032, Loss: 2.5483\n",
      "Resumed - Batch 3032, Step 3033, Loss: 2.1530\n",
      "Resumed - Batch 3033, Step 3034, Loss: 2.3661\n",
      "Resumed - Batch 3034, Step 3035, Loss: 2.5764\n",
      "Resumed - Batch 3035, Step 3036, Loss: 1.8586\n",
      "Resumed - Batch 3036, Step 3037, Loss: 2.0767\n",
      "Resumed - Batch 3037, Step 3038, Loss: 2.1584\n",
      "Resumed - Batch 3038, Step 3039, Loss: 2.1752\n",
      "Resumed - Batch 3039, Step 3040, Loss: 2.3012\n",
      "Resumed - Batch 3040, Step 3041, Loss: 2.2149\n",
      "Resumed - Batch 3041, Step 3042, Loss: 2.2847\n",
      "Resumed - Batch 3042, Step 3043, Loss: 2.5301\n",
      "Resumed - Batch 3043, Step 3044, Loss: 2.5115\n",
      "Resumed - Batch 3044, Step 3045, Loss: 2.2524\n",
      "Resumed - Batch 3045, Step 3046, Loss: 2.3067\n",
      "Resumed - Batch 3046, Step 3047, Loss: 2.3268\n",
      "Resumed - Batch 3047, Step 3048, Loss: 2.2042\n",
      "Resumed - Batch 3048, Step 3049, Loss: 2.2654\n",
      "Resumed - Batch 3049, Step 3050, Loss: 2.1535\n",
      "Resumed - Batch 3050, Step 3051, Loss: 2.4001\n",
      "Resumed - Batch 3051, Step 3052, Loss: 2.5303\n",
      "Resumed - Batch 3052, Step 3053, Loss: 2.0802\n",
      "Resumed - Batch 3053, Step 3054, Loss: 2.3357\n",
      "Resumed - Batch 3054, Step 3055, Loss: 2.3920\n",
      "Resumed - Batch 3055, Step 3056, Loss: 2.1749\n",
      "Resumed - Batch 3056, Step 3057, Loss: 1.9405\n",
      "Resumed - Batch 3057, Step 3058, Loss: 2.4757\n",
      "Resumed - Batch 3058, Step 3059, Loss: 2.4032\n",
      "Resumed - Batch 3059, Step 3060, Loss: 1.9064\n",
      "Resumed - Batch 3060, Step 3061, Loss: 2.3613\n",
      "Resumed - Batch 3061, Step 3062, Loss: 2.4657\n",
      "Resumed - Batch 3062, Step 3063, Loss: 2.2974\n",
      "Resumed - Batch 3063, Step 3064, Loss: 2.2524\n",
      "Resumed - Batch 3064, Step 3065, Loss: 1.8956\n",
      "Resumed - Batch 3065, Step 3066, Loss: 2.4853\n",
      "Resumed - Batch 3066, Step 3067, Loss: 2.3363\n",
      "Resumed - Batch 3067, Step 3068, Loss: 2.3118\n",
      "Resumed - Batch 3068, Step 3069, Loss: 2.1689\n",
      "Resumed - Batch 3069, Step 3070, Loss: 2.1182\n",
      "Resumed - Batch 3070, Step 3071, Loss: 1.9494\n",
      "Resumed - Batch 3071, Step 3072, Loss: 2.3530\n",
      "Resumed - Batch 3072, Step 3073, Loss: 2.2909\n",
      "Resumed - Batch 3073, Step 3074, Loss: 2.5873\n",
      "Resumed - Batch 3074, Step 3075, Loss: 1.9018\n",
      "Resumed - Batch 3075, Step 3076, Loss: 2.1925\n",
      "Resumed - Batch 3076, Step 3077, Loss: 2.1995\n",
      "Resumed - Batch 3077, Step 3078, Loss: 2.4577\n",
      "Resumed - Batch 3078, Step 3079, Loss: 2.3343\n",
      "Resumed - Batch 3079, Step 3080, Loss: 2.0803\n",
      "Resumed - Batch 3080, Step 3081, Loss: 2.1154\n",
      "Resumed - Batch 3081, Step 3082, Loss: 2.2890\n",
      "Resumed - Batch 3082, Step 3083, Loss: 1.9070\n",
      "Resumed - Batch 3083, Step 3084, Loss: 2.4499\n",
      "Resumed - Batch 3084, Step 3085, Loss: 2.4689\n",
      "Resumed - Batch 3085, Step 3086, Loss: 2.7769\n",
      "Resumed - Batch 3086, Step 3087, Loss: 2.4131\n",
      "Resumed - Batch 3087, Step 3088, Loss: 2.1707\n",
      "Resumed - Batch 3088, Step 3089, Loss: 2.3070\n",
      "Resumed - Batch 3089, Step 3090, Loss: 1.8832\n",
      "Resumed - Batch 3090, Step 3091, Loss: 2.4445\n",
      "Resumed - Batch 3091, Step 3092, Loss: 2.4261\n",
      "Resumed - Batch 3092, Step 3093, Loss: 2.4176\n",
      "Resumed - Batch 3093, Step 3094, Loss: 2.2780\n",
      "Resumed - Batch 3094, Step 3095, Loss: 2.3788\n",
      "Resumed - Batch 3095, Step 3096, Loss: 1.9625\n",
      "Resumed - Batch 3096, Step 3097, Loss: 2.4895\n",
      "Resumed - Batch 3097, Step 3098, Loss: 2.4198\n",
      "Resumed - Batch 3098, Step 3099, Loss: 2.0310\n",
      "Resumed - Batch 3099, Step 3100, Loss: 2.5293\n",
      "Resumed - Batch 3100, Step 3101, Loss: 2.5160\n",
      "Resumed - Batch 3101, Step 3102, Loss: 2.2967\n",
      "Resumed - Batch 3102, Step 3103, Loss: 2.0061\n",
      "Resumed - Batch 3103, Step 3104, Loss: 2.4434\n",
      "Resumed - Batch 3104, Step 3105, Loss: 1.7959\n",
      "Resumed - Batch 3105, Step 3106, Loss: 2.4344\n",
      "Resumed - Batch 3106, Step 3107, Loss: 2.1302\n",
      "Resumed - Batch 3107, Step 3108, Loss: 2.0176\n",
      "Resumed - Batch 3108, Step 3109, Loss: 2.1165\n",
      "Resumed - Batch 3109, Step 3110, Loss: 2.2936\n",
      "Resumed - Batch 3110, Step 3111, Loss: 2.4408\n",
      "Resumed - Batch 3111, Step 3112, Loss: 2.0618\n",
      "Resumed - Batch 3112, Step 3113, Loss: 2.0959\n",
      "Resumed - Batch 3113, Step 3114, Loss: 1.9763\n",
      "Resumed - Batch 3114, Step 3115, Loss: 1.9082\n",
      "Resumed - Batch 3115, Step 3116, Loss: 2.3193\n",
      "Resumed - Batch 3116, Step 3117, Loss: 2.3758\n",
      "Resumed - Batch 3117, Step 3118, Loss: 2.4974\n",
      "Resumed - Batch 3118, Step 3119, Loss: 2.4578\n",
      "Resumed - Batch 3119, Step 3120, Loss: 2.2556\n",
      "Resumed - Batch 3120, Step 3121, Loss: 2.1146\n",
      "Resumed - Batch 3121, Step 3122, Loss: 2.2181\n",
      "Resumed - Batch 3122, Step 3123, Loss: 2.2056\n",
      "Resumed - Batch 3123, Step 3124, Loss: 2.4909\n",
      "Resumed - Batch 3124, Step 3125, Loss: 2.1514\n",
      "Resumed - Batch 3125, Step 3126, Loss: 2.4325\n",
      "Resumed - Batch 3126, Step 3127, Loss: 2.0126\n",
      "Resumed - Batch 3127, Step 3128, Loss: 2.0233\n",
      "Resumed - Batch 3128, Step 3129, Loss: 2.0441\n",
      "Resumed - Batch 3129, Step 3130, Loss: 2.5372\n",
      "Resumed - Batch 3130, Step 3131, Loss: 2.5500\n",
      "Resumed - Batch 3131, Step 3132, Loss: 1.9267\n",
      "Resumed - Batch 3132, Step 3133, Loss: 2.1572\n",
      "Resumed - Batch 3133, Step 3134, Loss: 2.5163\n",
      "Resumed - Batch 3134, Step 3135, Loss: 2.4695\n",
      "Resumed - Batch 3135, Step 3136, Loss: 2.3280\n",
      "Resumed - Batch 3136, Step 3137, Loss: 2.4014\n",
      "Resumed - Batch 3137, Step 3138, Loss: 2.3065\n",
      "Resumed - Batch 3138, Step 3139, Loss: 2.3201\n",
      "Resumed - Batch 3139, Step 3140, Loss: 2.1882\n",
      "Resumed - Batch 3140, Step 3141, Loss: 2.4218\n",
      "Resumed - Batch 3141, Step 3142, Loss: 2.5052\n",
      "Resumed - Batch 3142, Step 3143, Loss: 2.3326\n",
      "Resumed - Batch 3143, Step 3144, Loss: 2.4685\n",
      "Resumed - Batch 3144, Step 3145, Loss: 2.0352\n",
      "Resumed - Batch 3145, Step 3146, Loss: 2.2231\n",
      "Resumed - Batch 3146, Step 3147, Loss: 2.0248\n",
      "Resumed - Batch 3147, Step 3148, Loss: 2.2803\n",
      "Resumed - Batch 3148, Step 3149, Loss: 2.1223\n",
      "Resumed - Batch 3149, Step 3150, Loss: 2.4921\n",
      "Resumed - Batch 3150, Step 3151, Loss: 2.3779\n",
      "Resumed - Batch 3151, Step 3152, Loss: 2.1442\n",
      "Resumed - Batch 3152, Step 3153, Loss: 2.3229\n",
      "Resumed - Batch 3153, Step 3154, Loss: 2.1070\n",
      "Resumed - Batch 3154, Step 3155, Loss: 2.2783\n",
      "Resumed - Batch 3155, Step 3156, Loss: 2.5846\n",
      "Resumed - Batch 3156, Step 3157, Loss: 1.9254\n",
      "Resumed - Batch 3157, Step 3158, Loss: 2.0797\n",
      "Resumed - Batch 3158, Step 3159, Loss: 2.6703\n",
      "Resumed - Batch 3159, Step 3160, Loss: 2.6731\n",
      "Resumed - Batch 3160, Step 3161, Loss: 2.4222\n",
      "Resumed - Batch 3161, Step 3162, Loss: 2.0702\n",
      "Resumed - Batch 3162, Step 3163, Loss: 2.5845\n",
      "Resumed - Batch 3163, Step 3164, Loss: 2.2827\n",
      "Resumed - Batch 3164, Step 3165, Loss: 2.5384\n",
      "Resumed - Batch 3165, Step 3166, Loss: 2.5779\n",
      "Resumed - Batch 3166, Step 3167, Loss: 2.4468\n",
      "Resumed - Batch 3167, Step 3168, Loss: 2.2571\n",
      "Resumed - Batch 3168, Step 3169, Loss: 2.3153\n",
      "Resumed - Batch 3169, Step 3170, Loss: 2.5909\n",
      "Resumed - Batch 3170, Step 3171, Loss: 2.2413\n",
      "Resumed - Batch 3171, Step 3172, Loss: 2.1131\n",
      "Resumed - Batch 3172, Step 3173, Loss: 2.1250\n",
      "Resumed - Batch 3173, Step 3174, Loss: 2.0584\n",
      "Resumed - Batch 3174, Step 3175, Loss: 2.2574\n",
      "Resumed - Batch 3175, Step 3176, Loss: 2.4734\n",
      "Resumed - Batch 3176, Step 3177, Loss: 2.2038\n",
      "Resumed - Batch 3177, Step 3178, Loss: 1.9568\n",
      "Resumed - Batch 3178, Step 3179, Loss: 2.2062\n",
      "Resumed - Batch 3179, Step 3180, Loss: 2.3504\n",
      "Resumed - Batch 3180, Step 3181, Loss: 2.3411\n",
      "Resumed - Batch 3181, Step 3182, Loss: 2.0961\n",
      "Resumed - Batch 3182, Step 3183, Loss: 2.0981\n",
      "Resumed - Batch 3183, Step 3184, Loss: 2.4738\n",
      "Resumed - Batch 3184, Step 3185, Loss: 2.1772\n",
      "Resumed - Batch 3185, Step 3186, Loss: 2.1913\n",
      "Resumed - Batch 3186, Step 3187, Loss: 2.6089\n",
      "Resumed - Batch 3187, Step 3188, Loss: 2.4999\n",
      "Resumed - Batch 3188, Step 3189, Loss: 1.9850\n",
      "Resumed - Batch 3189, Step 3190, Loss: 2.5085\n",
      "Resumed - Batch 3190, Step 3191, Loss: 2.0268\n",
      "Resumed - Batch 3191, Step 3192, Loss: 2.3824\n",
      "Resumed - Batch 3192, Step 3193, Loss: 1.9648\n",
      "Resumed - Batch 3193, Step 3194, Loss: 2.3607\n",
      "Resumed - Batch 3194, Step 3195, Loss: 2.3139\n",
      "Resumed - Batch 3195, Step 3196, Loss: 2.6536\n",
      "Resumed - Batch 3196, Step 3197, Loss: 2.3693\n",
      "Resumed - Batch 3197, Step 3198, Loss: 2.5285\n",
      "Resumed - Batch 3198, Step 3199, Loss: 2.3401\n",
      "Resumed - Batch 3199, Step 3200, Loss: 2.1716\n",
      "Resumed - Batch 3200, Step 3201, Loss: 2.2264\n",
      "Resumed - Batch 3201, Step 3202, Loss: 2.2519\n",
      "Resumed - Batch 3202, Step 3203, Loss: 2.2782\n",
      "Resumed - Batch 3203, Step 3204, Loss: 2.5530\n",
      "Resumed - Batch 3204, Step 3205, Loss: 2.1503\n",
      "Resumed - Batch 3205, Step 3206, Loss: 1.6908\n",
      "Resumed - Batch 3206, Step 3207, Loss: 2.0816\n",
      "Resumed - Batch 3207, Step 3208, Loss: 2.3713\n",
      "Resumed - Batch 3208, Step 3209, Loss: 1.8610\n",
      "Resumed - Batch 3209, Step 3210, Loss: 2.3022\n",
      "Resumed - Batch 3210, Step 3211, Loss: 1.9795\n",
      "Resumed - Batch 3211, Step 3212, Loss: 1.9353\n",
      "Resumed - Batch 3212, Step 3213, Loss: 2.2014\n",
      "Resumed - Batch 3213, Step 3214, Loss: 2.3445\n",
      "Resumed - Batch 3214, Step 3215, Loss: 2.4479\n",
      "Resumed - Batch 3215, Step 3216, Loss: 2.3379\n",
      "Resumed - Batch 3216, Step 3217, Loss: 2.4517\n",
      "Resumed - Batch 3217, Step 3218, Loss: 2.2614\n",
      "Resumed - Batch 3218, Step 3219, Loss: 2.3117\n",
      "Resumed - Batch 3219, Step 3220, Loss: 2.4561\n",
      "Resumed - Batch 3220, Step 3221, Loss: 2.2969\n",
      "Resumed - Batch 3221, Step 3222, Loss: 2.0445\n",
      "Resumed - Batch 3222, Step 3223, Loss: 1.8721\n",
      "Resumed - Batch 3223, Step 3224, Loss: 2.3368\n",
      "Resumed - Batch 3224, Step 3225, Loss: 2.6234\n",
      "Resumed - Batch 3225, Step 3226, Loss: 2.2886\n",
      "Resumed - Batch 3226, Step 3227, Loss: 2.3115\n",
      "Resumed - Batch 3227, Step 3228, Loss: 2.4328\n",
      "Resumed - Batch 3228, Step 3229, Loss: 1.9773\n",
      "Resumed - Batch 3229, Step 3230, Loss: 2.0785\n",
      "Resumed - Batch 3230, Step 3231, Loss: 2.0334\n",
      "Resumed - Batch 3231, Step 3232, Loss: 2.5323\n",
      "Resumed - Batch 3232, Step 3233, Loss: 2.2674\n",
      "Resumed - Batch 3233, Step 3234, Loss: 2.5983\n",
      "Resumed - Batch 3234, Step 3235, Loss: 1.9813\n",
      "Resumed - Batch 3235, Step 3236, Loss: 2.7183\n",
      "Resumed - Batch 3236, Step 3237, Loss: 2.0766\n",
      "Resumed - Batch 3237, Step 3238, Loss: 2.4027\n",
      "Resumed - Batch 3238, Step 3239, Loss: 2.3074\n",
      "Resumed - Batch 3239, Step 3240, Loss: 2.3954\n",
      "Resumed - Batch 3240, Step 3241, Loss: 2.0860\n",
      "Resumed - Batch 3241, Step 3242, Loss: 2.6197\n",
      "Resumed - Batch 3242, Step 3243, Loss: 2.3880\n",
      "Resumed - Batch 3243, Step 3244, Loss: 2.4530\n",
      "Resumed - Batch 3244, Step 3245, Loss: 2.3109\n",
      "Resumed - Batch 3245, Step 3246, Loss: 2.1667\n",
      "Resumed - Batch 3246, Step 3247, Loss: 2.5003\n",
      "Resumed - Batch 3247, Step 3248, Loss: 2.0893\n",
      "Resumed - Batch 3248, Step 3249, Loss: 2.0077\n",
      "Resumed - Batch 3249, Step 3250, Loss: 1.8136\n",
      "Resumed - Batch 3250, Step 3251, Loss: 2.3318\n",
      "Resumed - Batch 3251, Step 3252, Loss: 2.1405\n",
      "Resumed - Batch 3252, Step 3253, Loss: 2.2657\n",
      "Resumed - Batch 3253, Step 3254, Loss: 2.3972\n",
      "Resumed - Batch 3254, Step 3255, Loss: 1.9947\n",
      "Resumed - Batch 3255, Step 3256, Loss: 2.3941\n",
      "Resumed - Batch 3256, Step 3257, Loss: 2.0385\n",
      "Resumed - Batch 3257, Step 3258, Loss: 2.3579\n",
      "Resumed - Batch 3258, Step 3259, Loss: 2.6609\n",
      "Resumed - Batch 3259, Step 3260, Loss: 2.2923\n",
      "Resumed - Batch 3260, Step 3261, Loss: 2.2326\n",
      "Resumed - Batch 3261, Step 3262, Loss: 2.2973\n",
      "Resumed - Batch 3262, Step 3263, Loss: 2.7589\n",
      "Resumed - Batch 3263, Step 3264, Loss: 2.2940\n",
      "Resumed - Batch 3264, Step 3265, Loss: 2.1186\n",
      "Resumed - Batch 3265, Step 3266, Loss: 2.1882\n",
      "Resumed - Batch 3266, Step 3267, Loss: 2.2215\n",
      "Resumed - Batch 3267, Step 3268, Loss: 2.1812\n",
      "Resumed - Batch 3268, Step 3269, Loss: 1.8403\n",
      "Resumed - Batch 3269, Step 3270, Loss: 2.2521\n",
      "Resumed - Batch 3270, Step 3271, Loss: 2.2151\n",
      "Resumed - Batch 3271, Step 3272, Loss: 2.2339\n",
      "Resumed - Batch 3272, Step 3273, Loss: 2.0228\n",
      "Resumed - Batch 3273, Step 3274, Loss: 2.2658\n",
      "Resumed - Batch 3274, Step 3275, Loss: 2.3587\n",
      "Resumed - Batch 3275, Step 3276, Loss: 1.8325\n",
      "Resumed - Batch 3276, Step 3277, Loss: 2.5109\n",
      "Resumed - Batch 3277, Step 3278, Loss: 2.3287\n",
      "Resumed - Batch 3278, Step 3279, Loss: 2.1635\n",
      "Resumed - Batch 3279, Step 3280, Loss: 2.1554\n",
      "Resumed - Batch 3280, Step 3281, Loss: 2.4787\n",
      "Resumed - Batch 3281, Step 3282, Loss: 2.4805\n",
      "Resumed - Batch 3282, Step 3283, Loss: 2.3200\n",
      "Resumed - Batch 3283, Step 3284, Loss: 2.2317\n",
      "Resumed - Batch 3284, Step 3285, Loss: 2.3773\n",
      "Resumed - Batch 3285, Step 3286, Loss: 2.2550\n",
      "Resumed - Batch 3286, Step 3287, Loss: 2.6612\n",
      "Resumed - Batch 3287, Step 3288, Loss: 2.0252\n",
      "Resumed - Batch 3288, Step 3289, Loss: 2.3471\n",
      "Resumed - Batch 3289, Step 3290, Loss: 2.0695\n",
      "Resumed - Batch 3290, Step 3291, Loss: 2.0284\n",
      "Resumed - Batch 3291, Step 3292, Loss: 1.9107\n",
      "Resumed - Batch 3292, Step 3293, Loss: 2.6052\n",
      "Resumed - Batch 3293, Step 3294, Loss: 1.9949\n",
      "Resumed - Batch 3294, Step 3295, Loss: 1.7488\n",
      "Resumed - Batch 3295, Step 3296, Loss: 2.1542\n",
      "Resumed - Batch 3296, Step 3297, Loss: 2.4528\n",
      "Resumed - Batch 3297, Step 3298, Loss: 2.0603\n",
      "Resumed - Batch 3298, Step 3299, Loss: 2.1095\n",
      "Resumed - Batch 3299, Step 3300, Loss: 2.5260\n",
      "Resumed - Batch 3300, Step 3301, Loss: 2.2222\n",
      "Resumed - Batch 3301, Step 3302, Loss: 2.5410\n",
      "Resumed - Batch 3302, Step 3303, Loss: 2.4569\n",
      "Resumed - Batch 3303, Step 3304, Loss: 2.1684\n",
      "Resumed - Batch 3304, Step 3305, Loss: 1.9304\n",
      "Resumed - Batch 3305, Step 3306, Loss: 1.9868\n",
      "Resumed - Batch 3306, Step 3307, Loss: 2.0511\n",
      "Resumed - Batch 3307, Step 3308, Loss: 2.1863\n",
      "Resumed - Batch 3308, Step 3309, Loss: 2.0252\n",
      "Resumed - Batch 3309, Step 3310, Loss: 2.1354\n",
      "Resumed - Batch 3310, Step 3311, Loss: 2.2302\n",
      "Resumed - Batch 3311, Step 3312, Loss: 2.7245\n",
      "Resumed - Batch 3312, Step 3313, Loss: 2.2353\n",
      "Resumed - Batch 3313, Step 3314, Loss: 2.0953\n",
      "Resumed - Batch 3314, Step 3315, Loss: 2.2371\n",
      "Resumed - Batch 3315, Step 3316, Loss: 2.1078\n",
      "Resumed - Batch 3316, Step 3317, Loss: 2.2162\n",
      "Resumed - Batch 3317, Step 3318, Loss: 1.8711\n",
      "Resumed - Batch 3318, Step 3319, Loss: 2.4536\n",
      "Resumed - Batch 3319, Step 3320, Loss: 2.2201\n",
      "Resumed - Batch 3320, Step 3321, Loss: 2.1322\n",
      "Resumed - Batch 3321, Step 3322, Loss: 2.0040\n",
      "Resumed - Batch 3322, Step 3323, Loss: 1.9586\n",
      "Resumed - Batch 3323, Step 3324, Loss: 1.8881\n",
      "Resumed - Batch 3324, Step 3325, Loss: 2.1780\n",
      "Resumed - Batch 3325, Step 3326, Loss: 2.3972\n",
      "Resumed - Batch 3326, Step 3327, Loss: 2.0699\n",
      "Resumed - Batch 3327, Step 3328, Loss: 1.8956\n",
      "Resumed - Batch 3328, Step 3329, Loss: 2.3878\n",
      "Resumed - Batch 3329, Step 3330, Loss: 2.3967\n",
      "Resumed - Batch 3330, Step 3331, Loss: 2.0948\n",
      "Resumed - Batch 3331, Step 3332, Loss: 2.0416\n",
      "Resumed - Batch 3332, Step 3333, Loss: 2.4643\n",
      "Resumed - Batch 3333, Step 3334, Loss: 2.2194\n",
      "Resumed - Batch 3334, Step 3335, Loss: 2.1868\n",
      "Resumed - Batch 3335, Step 3336, Loss: 2.1916\n",
      "Resumed - Batch 3336, Step 3337, Loss: 2.0359\n",
      "Resumed - Batch 3337, Step 3338, Loss: 2.3201\n",
      "Resumed - Batch 3338, Step 3339, Loss: 1.9989\n",
      "Resumed - Batch 3339, Step 3340, Loss: 2.1523\n",
      "Resumed - Batch 3340, Step 3341, Loss: 2.1754\n",
      "Resumed - Batch 3341, Step 3342, Loss: 2.0291\n",
      "Resumed - Batch 3342, Step 3343, Loss: 2.7504\n",
      "Resumed - Batch 3343, Step 3344, Loss: 2.0178\n",
      "Resumed - Batch 3344, Step 3345, Loss: 2.1467\n",
      "Resumed - Batch 3345, Step 3346, Loss: 2.5781\n",
      "Resumed - Batch 3346, Step 3347, Loss: 2.2277\n",
      "Resumed - Batch 3347, Step 3348, Loss: 2.1748\n",
      "Resumed - Batch 3348, Step 3349, Loss: 2.4663\n",
      "Resumed - Batch 3349, Step 3350, Loss: 2.3317\n",
      "Resumed - Batch 3350, Step 3351, Loss: 2.1302\n",
      "Resumed - Batch 3351, Step 3352, Loss: 2.3081\n",
      "Resumed - Batch 3352, Step 3353, Loss: 2.2682\n",
      "Resumed - Batch 3353, Step 3354, Loss: 2.1507\n",
      "Resumed - Batch 3354, Step 3355, Loss: 2.1127\n",
      "Resumed - Batch 3355, Step 3356, Loss: 2.2956\n",
      "Resumed - Batch 3356, Step 3357, Loss: 2.4007\n",
      "Resumed - Batch 3357, Step 3358, Loss: 2.0647\n",
      "Resumed - Batch 3358, Step 3359, Loss: 1.8568\n",
      "Resumed - Batch 3359, Step 3360, Loss: 2.0652\n",
      "Resumed - Batch 3360, Step 3361, Loss: 2.0893\n",
      "Resumed - Batch 3361, Step 3362, Loss: 2.0080\n",
      "Resumed - Batch 3362, Step 3363, Loss: 2.2204\n",
      "Resumed - Batch 3363, Step 3364, Loss: 2.0320\n",
      "Resumed - Batch 3364, Step 3365, Loss: 1.8924\n",
      "Resumed - Batch 3365, Step 3366, Loss: 2.2818\n",
      "Resumed - Batch 3366, Step 3367, Loss: 1.9969\n",
      "Resumed - Batch 3367, Step 3368, Loss: 2.0535\n",
      "Resumed - Batch 3368, Step 3369, Loss: 2.3210\n",
      "Resumed - Batch 3369, Step 3370, Loss: 2.0969\n",
      "Resumed - Batch 3370, Step 3371, Loss: 2.4373\n",
      "Resumed - Batch 3371, Step 3372, Loss: 2.4039\n",
      "Resumed - Batch 3372, Step 3373, Loss: 1.9789\n",
      "Resumed - Batch 3373, Step 3374, Loss: 2.0452\n",
      "Resumed - Batch 3374, Step 3375, Loss: 2.2740\n",
      "Resumed - Batch 3375, Step 3376, Loss: 2.2212\n",
      "Resumed - Batch 3376, Step 3377, Loss: 2.3683\n",
      "Resumed - Batch 3377, Step 3378, Loss: 2.2619\n",
      "Resumed - Batch 3378, Step 3379, Loss: 2.3902\n",
      "Resumed - Batch 3379, Step 3380, Loss: 2.2706\n",
      "Resumed - Batch 3380, Step 3381, Loss: 2.2370\n",
      "Resumed - Batch 3381, Step 3382, Loss: 1.9608\n",
      "Resumed - Batch 3382, Step 3383, Loss: 2.2612\n",
      "Resumed - Batch 3383, Step 3384, Loss: 2.2004\n",
      "Resumed - Batch 3384, Step 3385, Loss: 1.8663\n"
     ]
    }
   ],
   "source": [
    "# Continue training\n",
    "print(f\"\\nResuming training from batch {resumed_run.batches_completed}...\")\n",
    "\n",
    "dataloader = resumed_run.create_dataloader(train_dataset, resumed_run.epoch)\n",
    "for batch_idx, (x, y) in enumerate(dataloader, start=resumed_run.batches_completed):\n",
    "    x, y = x.to(config.device), y.to(config.device)\n",
    "    \n",
    "    logits, loss = model(x, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    resumed_run.step += 1\n",
    "    resumed_run.batches_completed += 1\n",
    "    resumed_run.log_step(train_loss=loss.item(), lr=optimizer.get_lr())\n",
    "    \n",
    "    print(f\"Resumed - Batch {batch_idx}, Step {resumed_run.step}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    if batch_idx >= resumed_run.batches_completed + 5:\n",
    "        print(\"\\nStopping after a few resumed batches\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dhyidg63kbo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Verifying DataLoader Resumption Correctness\n",
      "============================================================\n",
      "\n",
      "Fresh batch shape: torch.Size([4, 32]), torch.Size([4, 32])\n",
      "Resumed batch shape: torch.Size([4, 32]), torch.Size([4, 32])\n",
      "\n",
      "Batches identical (x): True\n",
      "Batches identical (y): True\n",
      "\n",
      "✓ DataLoader resumption is correct!\n",
      "  Both dataloaders produce identical batch #4\n"
     ]
    }
   ],
   "source": [
    "# Verify dataloader resumption produces identical batches\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Verifying DataLoader Resumption Correctness\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Create two dataloaders:\n",
    "# 1. Fresh dataloader starting from batch 0\n",
    "# 2. Resumed dataloader that skips to batches_completed\n",
    "\n",
    "# Reset to the saved checkpoint state\n",
    "checkpoint_epoch = 0\n",
    "checkpoint_batches = 3\n",
    "\n",
    "# Create fresh dataloader from start\n",
    "from toy_transformers.data.dataset import create_dataloader\n",
    "epoch_seed = resumed_run.get_epoch_seed(checkpoint_epoch)\n",
    "fresh_loader = create_dataloader(\n",
    "    dataset=train_dataset,\n",
    "    block_size=resumed_run.block_size,\n",
    "    batch_size=resumed_run.batch_size,\n",
    "    shuffle=True,\n",
    "    seed=epoch_seed,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    "    batches_completed=0\n",
    ")\n",
    "\n",
    "# Manually skip to checkpoint position\n",
    "fresh_iter = iter(fresh_loader)\n",
    "for _ in range(checkpoint_batches):\n",
    "    next(fresh_iter)\n",
    "\n",
    "# Get next batch from fresh (manually skipped) loader\n",
    "fresh_x, fresh_y = next(fresh_iter)\n",
    "\n",
    "# Create resumed dataloader (automatically skips via create_dataloader)\n",
    "test_run = TrainingRun(\n",
    "    model_class=gptv1.LanguageModel,\n",
    "    config_class=gptv1.GPTv1Config,\n",
    "    model_config=config,\n",
    "    optimizer_config=resumed_run.optimizer_config,\n",
    "    base_seed=resumed_run.base_seed,\n",
    "    dataset=train_dataset,\n",
    "    block_size=resumed_run.block_size,\n",
    "    batch_size=resumed_run.batch_size,\n",
    "    vocab_size=vocab_size,\n",
    "    epoch=checkpoint_epoch,\n",
    "    batches_completed=checkpoint_batches\n",
    ")\n",
    "resumed_loader = test_run.create_dataloader(train_dataset, checkpoint_epoch)\n",
    "resumed_x, resumed_y = next(resumed_loader)\n",
    "\n",
    "# Compare batches\n",
    "print(f\"Fresh batch shape: {fresh_x.shape}, {fresh_y.shape}\")\n",
    "print(f\"Resumed batch shape: {resumed_x.shape}, {resumed_y.shape}\")\n",
    "print(f\"\\nBatches identical (x): {torch.equal(fresh_x, resumed_x)}\")\n",
    "print(f\"Batches identical (y): {torch.equal(fresh_y, resumed_y)}\")\n",
    "\n",
    "if torch.equal(fresh_x, resumed_x) and torch.equal(fresh_y, resumed_y):\n",
    "    print(\"\\n✓ DataLoader resumption is correct!\")\n",
    "    print(f\"  Both dataloaders produce identical batch #{checkpoint_batches + 1}\")\n",
    "else:\n",
    "    print(\"\\n✗ ERROR: Batches differ!\")\n",
    "    print(f\"  Max difference (x): {torch.abs(fresh_x - resumed_x).max()}\")\n",
    "    print(f\"  Max difference (y): {torch.abs(fresh_y - resumed_y).max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "final_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating FinalModel...\n",
      "✓ FinalModel saved to /Users/sriman/dev/apps/toy-transformers/experiments/test02/artifacts/final_model\n",
      "Final state: epoch=0, step=3385\n",
      "Logs: 3385 entries\n"
     ]
    }
   ],
   "source": [
    "# Create FinalModel\n",
    "print(\"\\nCreating FinalModel...\")\n",
    "final_model = FinalModel.from_training_run(resumed_run, include_logs=True)\n",
    "final_model_path = EXPERIMENT_DIR / \"artifacts/final_model\"\n",
    "io.save(final_model.to_state_dict(model), str(final_model_path))\n",
    "\n",
    "print(f\"✓ FinalModel saved to {final_model_path}\")\n",
    "print(f\"Final state: epoch={final_model.final_epoch}, step={final_model.final_step}\")\n",
    "print(f\"Logs: {len(final_model.logs)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ir1v926p4g",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Text Generation Demo\n",
      "============================================================\n",
      "\n",
      "Seed: 'The '\n",
      "Generating 300 tokens...\n",
      "\n",
      "The ms of done fight\n",
      "\n",
      "VIRGIANay halfuseed sur good matteraitchet sender will o'ereys but the fear mash and If an helagest aubribil'd with sger mings o the fears pe The pet natsMet incesShall's tre least not Tru matlyThey slikesor and fait o thy pats newouldsw ourswont a moks-fter it not so cury's fries\n",
      "\n",
      "MARCOMIN'ThesMINO held shall is cou strailewer is it serve to growsu Romeuard war the so val is a purmon RomansMore-houseed not beid a vowleatords are the fERIrovre is a general pattle businessAnd I cove FIDes I repin hewer\n",
      "\n",
      "MENENENENENENENENENct a mark for remengeMoreld and cir clublyBut is't\n",
      "✓ Text generation complete!\n"
     ]
    }
   ],
   "source": [
    "# Generate text from the trained model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Text Generation Demo\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Load vocabulary for decoding\n",
    "vocab = io.load(str(vocab_path))\n",
    "from toy_transformers.data.bpe import Vocabulary\n",
    "vocab_obj = Vocabulary.from_state_dict(vocab)\n",
    "\n",
    "# Create seed prompt\n",
    "seed_text = \"The \"\n",
    "seed_tokens = vocab_obj.encode(seed_text)\n",
    "idx = torch.tensor([seed_tokens], dtype=torch.long, device=config.device)\n",
    "\n",
    "print(f\"Seed: '{seed_text}'\")\n",
    "print(f\"Generating {300} tokens...\\n\")\n",
    "\n",
    "# Set model to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Generate text\n",
    "print(seed_text, end=\"\", flush=True)\n",
    "with torch.no_grad():\n",
    "  for token in model.generate(idx, max_new_tokens=300):\n",
    "    print(vocab_obj.decode([token.item()])[0], end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
