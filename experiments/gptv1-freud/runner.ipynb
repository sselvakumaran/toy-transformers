{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2597617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch \n",
    "from pathlib import Path\n",
    "\n",
    "def get_project_info() -> Path:\n",
    "  current = Path.cwd().resolve()\n",
    "  root = current\n",
    "  for parent in [current, *current.parents]:\n",
    "    if (parent / \"toy_transformers\").exists():\n",
    "      root = parent\n",
    "      break\n",
    "  return root, current\n",
    "\n",
    "if 'ROOT_DIR' not in globals():\n",
    "\tROOT_DIR, EXPERIMENT_DIR = get_project_info()\n",
    "\tif str(ROOT_DIR) not in sys.path:\n",
    "\t\tsys.path.append(str(ROOT_DIR))\n",
    "\tif Path.cwd() != ROOT_DIR:\n",
    "\t\tos.chdir(ROOT_DIR)\n",
    "\n",
    "from toy_transformers.models import gptv1\n",
    "from toy_transformers import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca1310df",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 256\n",
    "BATCH_SIZE = 16\n",
    "MODE = tokenization.TokenizationMode.STR\n",
    "DEVICE = \"mps\"\n",
    "\n",
    "config = gptv1.GPTv1Config(\n",
    "\tvocab_size=VOCAB_SIZE,\n",
    "\tblock_size=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23787b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = EXPERIMENT_DIR / f\"vocab_{VOCAB_SIZE}.json\"\n",
    "raw_data_path = ROOT_DIR / \"data/gutenberg/freud-interpretation-of-dreams.txt\"\n",
    "\n",
    "if not vocab_path.exists():\n",
    "\traw_data = open(raw_data_path, \"r\")\n",
    "\tvocab = tokenization.create_bpe(\n",
    "\t\traw_data, \n",
    "\t\tVOCAB_SIZE, MODE\n",
    "\t)\n",
    "\tvocab.save(vocab_path)\n",
    "else:\n",
    "\tvocab = tokenization.Vocabulary.load(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbcae302",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(\n",
    "\tvocab.encode(open(raw_data_path, \"r\").read()),\n",
    "\tdtype=torch.long\n",
    ").to(device=DEVICE)\n",
    "\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "block_size, batch_size = config.block_size, BATCH_SIZE\n",
    "def get_batch(split):\n",
    "  data = train_data if split == 'train' else val_data\n",
    "  idxs = torch.randint(len(data) - block_size, (batch_size,), device=DEVICE)\n",
    "  x = torch.stack([data[i:i+block_size] for i in idxs])\n",
    "  y = torch.stack([data[i+1:i+block_size+1] for i in idxs])\n",
    "  return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_val_loss(model):\n",
    "  model.eval()\n",
    "  X, Y = get_batch(\"val\")\n",
    "  _, loss = model(X, Y)\n",
    "  model.train()\n",
    "  return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfc13aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "m = gptv1.LanguageModel(config).to(device=DEVICE)\n",
    "m.compile()\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=3e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "  optimizer,\n",
    "  mode='min',\n",
    "  factor=0.1,\n",
    "  patience=10\n",
    ")\n",
    "\n",
    "import time\n",
    "from torch.amp import autocast\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a2e7c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0217 19:07:53.402000 1495 torch/_inductor/utils.py:1436] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "ename": "InductorError",
     "evalue": "SyntaxError: failed to compile \n    #include <c10/metal/utils.h>\n    #include <c10/metal/reduction_utils.h>\n    [[max_total_threads_per_threadgroup(288)]]\n    kernel void generated_kernel(\n        device float* out_ptr2,\n        device float* out_ptr5,\n        device half* out_ptr6,\n        constant half* in_ptr0,\n        constant half* in_ptr1,\n        constant half* in_ptr2,\n        constant half* in_ptr3,\n        constant half* in_ptr4,\n        constant half* in_ptr5,\n        constant half* in_ptr6,\n        constant half* in_ptr7,\n        constant half* in_ptr8,\n        constant half* in_ptr9,\n        constant half* in_ptr10,\n        constant half* in_ptr11,\n        constant half* in_ptr12,\n        constant half* in_ptr13,\n        constant half* in_ptr14,\n        constant half* in_ptr15,\n        constant half* in_ptr16,\n        constant half* in_ptr17,\n        constant half* in_ptr18,\n        constant half* in_ptr19,\n        constant half* in_ptr20,\n        constant half* in_ptr21,\n        constant half* in_ptr22,\n        constant half* in_ptr23,\n        constant float* in_ptr24,\n        constant float* in_ptr25,\n        constant float* in_ptr26,\n        constant float* in_ptr27,\n        constant bool* in_ptr28,\n        uint2 thread_pos [[thread_position_in_grid]],\n        uint2 group_pos [[thread_position_in_threadgroup]]\n    ) {\n        auto xindex = thread_pos.x;\n        auto r0_index = thread_pos.y;\n        int r0_1 = r0_index;\n        int x0 = xindex;\n        threadgroup float tmp_acc_0[9];\n        threadgroup float tmp_acc_1[9];\n        auto tmp0 = static_cast<float>(in_ptr0[r0_1 + 288*x0]);\n        auto tmp2 = static_cast<float>(in_ptr1[r0_1 + 288*x0]);\n        auto tmp5 = static_cast<float>(in_ptr2[r0_1 + 288*x0]);\n        auto tmp8 = static_cast<float>(in_ptr3[r0_1 + 288*x0]);\n        auto tmp11 = static_cast<float>(in_ptr4[r0_1 + 288*x0]);\n        auto tmp14 = static_cast<float>(in_ptr5[r0_1 + 288*x0]);\n        auto tmp17 = static_cast<float>(in_ptr6[r0_1 + 288*x0]);\n        auto tmp20 = static_cast<float>(in_ptr7[r0_1 + 288*x0]);\n        auto tmp23 = static_cast<float>(in_ptr8[r0_1 + 288*x0]);\n        auto tmp26 = static_cast<float>(in_ptr9[r0_1 + 288*x0]);\n        auto tmp29 = static_cast<float>(in_ptr10[r0_1 + 288*x0]);\n        auto tmp32 = static_cast<float>(in_ptr11[r0_1 + 288*x0]);\n        auto tmp35 = static_cast<float>(in_ptr12[r0_1 + 288*x0]);\n        auto tmp38 = static_cast<float>(in_ptr13[r0_1 + 288*x0]);\n        auto tmp41 = static_cast<float>(in_ptr14[r0_1 + 288*x0]);\n        auto tmp44 = static_cast<float>(in_ptr15[r0_1 + 288*x0]);\n        auto tmp47 = static_cast<float>(in_ptr16[r0_1 + 288*x0]);\n        auto tmp50 = static_cast<float>(in_ptr17[r0_1 + 288*x0]);\n        auto tmp53 = static_cast<float>(in_ptr18[r0_1 + 288*x0]);\n        auto tmp56 = static_cast<float>(in_ptr19[r0_1 + 288*x0]);\n        auto tmp59 = static_cast<float>(in_ptr20[r0_1 + 288*x0]);\n        auto tmp62 = static_cast<float>(in_ptr21[r0_1 + 288*x0]);\n        auto tmp65 = static_cast<float>(in_ptr22[r0_1 + 288*x0]);\n        auto tmp68 = static_cast<float>(in_ptr23[r0_1 + 288*x0]);\n        auto tmp71 = in_ptr24[r0_1];\n        auto tmp74 = in_ptr25[r0_1 + 288*x0];\n        auto tmp1 = static_cast<float>(tmp0);\n        auto tmp3 = static_cast<float>(tmp2);\n        auto tmp4 = tmp1 + tmp3;\n        auto tmp6 = static_cast<float>(tmp5);\n        auto tmp7 = tmp4 + tmp6;\n        auto tmp9 = static_cast<float>(tmp8);\n        auto tmp10 = tmp7 + tmp9;\n        auto tmp12 = static_cast<float>(tmp11);\n        auto tmp13 = tmp10 + tmp12;\n        auto tmp15 = static_cast<float>(tmp14);\n        auto tmp16 = tmp13 + tmp15;\n        auto tmp18 = static_cast<float>(tmp17);\n        auto tmp19 = tmp16 + tmp18;\n        auto tmp21 = static_cast<float>(tmp20);\n        auto tmp22 = tmp19 + tmp21;\n        auto tmp24 = static_cast<float>(tmp23);\n        auto tmp25 = tmp22 + tmp24;\n        auto tmp27 = static_cast<float>(tmp26);\n        auto tmp28 = tmp25 + tmp27;\n        auto tmp30 = static_cast<float>(tmp29);\n        auto tmp31 = tmp28 + tmp30;\n        auto tmp33 = static_cast<float>(tmp32);\n        auto tmp34 = tmp31 + tmp33;\n        auto tmp36 = static_cast<float>(tmp35);\n        auto tmp37 = tmp34 + tmp36;\n        auto tmp39 = static_cast<float>(tmp38);\n        auto tmp40 = tmp37 + tmp39;\n        auto tmp42 = static_cast<float>(tmp41);\n        auto tmp43 = tmp40 + tmp42;\n        auto tmp45 = static_cast<float>(tmp44);\n        auto tmp46 = tmp43 + tmp45;\n        auto tmp48 = static_cast<float>(tmp47);\n        auto tmp49 = tmp46 + tmp48;\n        auto tmp51 = static_cast<float>(tmp50);\n        auto tmp52 = tmp49 + tmp51;\n        auto tmp54 = static_cast<float>(tmp53);\n        auto tmp55 = tmp52 + tmp54;\n        auto tmp57 = static_cast<float>(tmp56);\n        auto tmp58 = tmp55 + tmp57;\n        auto tmp60 = static_cast<float>(tmp59);\n        auto tmp61 = tmp58 + tmp60;\n        auto tmp63 = static_cast<float>(tmp62);\n        auto tmp64 = tmp61 + tmp63;\n        auto tmp66 = static_cast<float>(tmp65);\n        auto tmp67 = tmp64 + tmp66;\n        auto tmp69 = static_cast<float>(tmp68);\n        auto tmp70 = tmp67 + tmp69;\n        out_ptr2[r0_1 + 288*x0] = static_cast<float>(tmp70);\n        auto tmp72 = tmp70 * tmp71;\n        auto tmp75 = tmp72 * tmp74;\n        auto tmp73 = c10::metal::threadgroup_sum(tmp_acc_0, tmp72, r0_index * 1, 288);\n        auto tmp76 = c10::metal::threadgroup_sum(tmp_acc_1, tmp75, r0_index * 1, 288);\n        auto tmp77 = in_ptr26[r0_1 + 288*x0];\n        auto tmp78 = in_ptr27[x0];\n        auto tmp87 = in_ptr28[r0_1 + 288*x0];\n        auto tmp79 = 288.0;\n        auto tmp80 = tmp72 * tmp79;\n        auto tmp81 = tmp80 - tmp73;\n        auto tmp82 = tmp74 * tmp76;\n        auto tmp83 = tmp81 - tmp82;\n        auto tmp84 = tmp78 * tmp83;\n        auto tmp85 = tmp77 + tmp84;\n        out_ptr5[r0_1 + 288*x0] = static_cast<float>(tmp85);\n        auto tmp86 = static_cast<half>(tmp85);\n        auto tmp88 = static_cast<half>(tmp87);\n        auto tmp89 = 1.25;\n        auto tmp90 = tmp88 * tmp89;\n        auto tmp91 = tmp86 * tmp90;\n        out_ptr6[r0_1 + 288*x0] = static_cast<half>(tmp91);\n    }\n with program_source:589:24: error: no 'buffer' resource location available for 'in_ptr28'\n        constant bool* in_ptr28,\n                       ^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInductorError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \tlogits, loss = m(xb, yb)\n\u001b[32m      5\u001b[39m optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m optimizer.step()\n\u001b[32m      8\u001b[39m train_loss, val_loss = loss.item(), \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/autograd/function.py:311\u001b[39m, in \u001b[36mBackwardCFunction.apply\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    306\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mImplementing both \u001b[39m\u001b[33m'\u001b[39m\u001b[33mbackward\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mvjp\u001b[39m\u001b[33m'\u001b[39m\u001b[33m for a custom \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    307\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFunction is not allowed. You should only implement one \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    308\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mof them.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    309\u001b[39m     )\n\u001b[32m    310\u001b[39m user_fn = vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function.vjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:2259\u001b[39m, in \u001b[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.backward\u001b[39m\u001b[34m(ctx, *flat_args)\u001b[39m\n\u001b[32m   2257\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m CompiledFunction._double_backward(ctx, impl_fn, all_args)\n\u001b[32m   2258\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:2245\u001b[39m, in \u001b[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.backward.<locals>.impl_fn\u001b[39m\u001b[34m(double_ctx)\u001b[39m\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mimpl_fn\u001b[39m(double_ctx=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     out = \u001b[43mCompiledFunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_backward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2246\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _backward_epilogue_functional(\n\u001b[32m   2247\u001b[39m         CompiledFunction.metadata,\n\u001b[32m   2248\u001b[39m         CompiledFunction.maybe_subclass_metadata,\n\u001b[32m   2249\u001b[39m         out,\n\u001b[32m   2250\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:2347\u001b[39m, in \u001b[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction._backward_impl\u001b[39m\u001b[34m(ctx, all_args)\u001b[39m\n\u001b[32m   2345\u001b[39m CompileEventLogger.compilation_metric(is_forward=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   2346\u001b[39m \u001b[38;5;66;03m# See Note: [Backward graph lazy lowering]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2347\u001b[39m CompiledFunction.compiled_bw = \u001b[43maot_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbw_compiler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbw_module\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplaceholder_list\u001b[49m\n\u001b[32m   2349\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2350\u001b[39m \u001b[38;5;66;03m# Maybe save cache entry\u001b[39;00m\n\u001b[32m   2351\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m try_save_cache_entry \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:483\u001b[39m, in \u001b[36mSerializableAOTDispatchCompiler.__call__\u001b[39m\u001b[34m(self, gm, example_inputs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    479\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    480\u001b[39m     gm: torch.fx.GraphModule,\n\u001b[32m    481\u001b[39m     example_inputs: Sequence[InputType],\n\u001b[32m    482\u001b[39m ) -> OutputCode:\n\u001b[32m--> \u001b[39m\u001b[32m483\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_dynamo/backends/common.py:76\u001b[39m, in \u001b[36mAotAutograd.__call__.<locals>.wrap_bw_compiler.<locals>._wrapped_bw_compiler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrapped_bw_compiler\u001b[39m(*args, **kwargs):\n\u001b[32m     71\u001b[39m     \u001b[38;5;66;03m# Note [Wrapping bw_compiler in disable]\u001b[39;00m\n\u001b[32m     72\u001b[39m     \u001b[38;5;66;03m# The two disables here:\u001b[39;00m\n\u001b[32m     73\u001b[39m     \u001b[38;5;66;03m# - stop TorchDynamo from trying to compile the bw_compiler function itself\u001b[39;00m\n\u001b[32m     74\u001b[39m     \u001b[38;5;66;03m# - stop TorchDynamo from trying to compile our the generated backwards pass bw_compiler produces\u001b[39;00m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disable(\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m         \u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbw_compiler_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreason\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdo not trace backward compiler function\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     79\u001b[39m         reason=\u001b[33m\"\u001b[39m\u001b[33mdo not trace generated backwards pass\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     80\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:929\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    927\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m.callback))\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m929\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    931\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_utils_internal.py:97\u001b[39m, in \u001b[36mcompile_time_strobelight_meta.<locals>.compile_time_strobelight_meta_inner.<locals>.wrapper_function\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# This is not needed but we have it here to avoid having profile_compile_time\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# in stack traces when profiling is not enabled.\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m StrobelightCompileTimeProfiler.enabled:\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m StrobelightCompileTimeProfiler.profile_compile_time(\n\u001b[32m    100\u001b[39m     function, phase_name, *args, **kwargs\n\u001b[32m    101\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:2335\u001b[39m, in \u001b[36mcompile_fx.<locals>.bw_compiler\u001b[39m\u001b[34m(gm, example_inputs)\u001b[39m\n\u001b[32m   2329\u001b[39m fixed = count_tangents(gm)\n\u001b[32m   2330\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[32m   2331\u001b[39m     config.patch(get_cpp_wrapper_config())\n\u001b[32m   2332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config.cpp_wrapper\n\u001b[32m   2333\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext()\n\u001b[32m   2334\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m2335\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2336\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2337\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2338\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstatic_input_idxs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfixed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2339\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcudagraphs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcudagraphs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2340\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_backward\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2341\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgraph_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgraph_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2342\u001b[39m \u001b[43m        \u001b[49m\u001b[43mboxed_forward_device_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforward_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2343\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:745\u001b[39m, in \u001b[36mcompile_fx_inner\u001b[39m\u001b[34m(gm, example_inputs, **kwargs)\u001b[39m\n\u001b[32m    740\u001b[39m stack.enter_context(DebugContext())\n\u001b[32m    741\u001b[39m CompileEventLogger.pt2_compile(\n\u001b[32m    742\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33minductor_compile\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    743\u001b[39m     is_backward=kwargs[\u001b[33m\"\u001b[39m\u001b[33mis_backward\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    744\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m745\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrap_compiler_debug\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_compile_fx_inner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompiler_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minductor\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    747\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    749\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_dynamo/repro/after_aot.py:124\u001b[39m, in \u001b[36mwrap_compiler_debug.<locals>.debug_wrapper\u001b[39m\u001b[34m(gm, example_inputs, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m config.repro_after \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mdynamo\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33maot\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# Call the compiler_fn - which is either aot_autograd or inductor\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;66;03m# with fake inputs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     inner_compiled_fn = \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    126\u001b[39m     \u001b[38;5;66;03m# TODO: Failures here are troublesome because no real inputs,\u001b[39;00m\n\u001b[32m    127\u001b[39m     \u001b[38;5;66;03m# need a different serialization strategy\u001b[39;00m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config.repro_after == \u001b[33m\"\u001b[39m\u001b[33maot\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:923\u001b[39m, in \u001b[36m_compile_fx_inner\u001b[39m\u001b[34m(gm, example_inputs, **graph_kwargs)\u001b[39m\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m923\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InductorError(e, currentframe()).with_traceback(\n\u001b[32m    924\u001b[39m         e.__traceback__\n\u001b[32m    925\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    927\u001b[39m     TritonBundler.end_compile()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:907\u001b[39m, in \u001b[36m_compile_fx_inner\u001b[39m\u001b[34m(gm, example_inputs, **graph_kwargs)\u001b[39m\n\u001b[32m    905\u001b[39m TritonBundler.begin_compile()\n\u001b[32m    906\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m907\u001b[39m     mb_compiled_graph = \u001b[43mfx_codegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgraph_kwargs\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    910\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m mb_compiled_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    911\u001b[39m     mb_compiled_graph._time_taken_ns = time.time_ns() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1578\u001b[39m, in \u001b[36mfx_codegen_and_compile\u001b[39m\u001b[34m(gm, example_inputs, inputs_to_check, **graph_kwargs)\u001b[39m\n\u001b[32m   1573\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scheme, _OutOfProcessFxCompile), (\n\u001b[32m   1574\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masync is only valid with an out-of-process compile mode\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1575\u001b[39m     )\n\u001b[32m   1576\u001b[39m     scheme = _AsyncFxCompile(scheme)\n\u001b[32m-> \u001b[39m\u001b[32m1578\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscheme\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcodegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1456\u001b[39m, in \u001b[36m_InProcessFxCompile.codegen_and_compile\u001b[39m\u001b[34m(self, gm, example_inputs, inputs_to_check, graph_kwargs)\u001b[39m\n\u001b[32m   1438\u001b[39m         compiled_fn = AotCodeCompiler.compile(\n\u001b[32m   1439\u001b[39m             graph,\n\u001b[32m   1440\u001b[39m             wrapper_code.value,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1453\u001b[39m             ],\n\u001b[32m   1454\u001b[39m         )\n\u001b[32m   1455\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1456\u001b[39m     compiled_module = \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1457\u001b[39m     compiled_fn = compiled_module.call\n\u001b[32m   1458\u001b[39m     compiled_fn_runner = \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m   1459\u001b[39m         compiled_module, \u001b[33m\"\u001b[39m\u001b[33mrunner\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1460\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_inductor/graph.py:2293\u001b[39m, in \u001b[36mGraphLowering.compile_to_module\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2286\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompile_to_module\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> CompiledModule:\n\u001b[32m   2287\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[32m   2288\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mGraphLowering.compile_to_module\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2289\u001b[39m         phase_name=\u001b[33m\"\u001b[39m\u001b[33mcode_gen\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2290\u001b[39m         log_pt2_compile_event=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   2291\u001b[39m         dynamo_compile_column_us=\u001b[33m\"\u001b[39m\u001b[33minductor_code_gen_cumulative_compile_time_us\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2292\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m2293\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_inductor/graph.py:2303\u001b[39m, in \u001b[36mGraphLowering._compile_to_module\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2298\u001b[39m wrapper_code, _ = (\n\u001b[32m   2299\u001b[39m     \u001b[38;5;28mself\u001b[39m.codegen_with_cpp_wrapper() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cpp_wrapper \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.codegen()\n\u001b[32m   2300\u001b[39m )\n\u001b[32m   2302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(wrapper_code, ValueWithLineMap):\n\u001b[32m-> \u001b[39m\u001b[32m2303\u001b[39m     mod = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compile_to_module_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper_code\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2304\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(wrapper_code, FileBackedGraphModule):\n\u001b[32m   2305\u001b[39m     mod = wrapper_code\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_inductor/graph.py:2371\u001b[39m, in \u001b[36mGraphLowering._compile_to_module_lines\u001b[39m\u001b[34m(self, wrapper_code)\u001b[39m\n\u001b[32m   2365\u001b[39m     trace_structured(\n\u001b[32m   2366\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minductor_output_code\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2367\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m: {\u001b[33m\"\u001b[39m\u001b[33mfilename\u001b[39m\u001b[33m\"\u001b[39m: path},\n\u001b[32m   2368\u001b[39m         payload_fn=\u001b[38;5;28;01mlambda\u001b[39;00m: wrapper_code.value,\n\u001b[32m   2369\u001b[39m     )\n\u001b[32m   2370\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[33m\"\u001b[39m\u001b[33mPyCodeCache.load_by_key_path\u001b[39m\u001b[33m\"\u001b[39m, log_pt2_compile_event=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2371\u001b[39m     mod = \u001b[43mPyCodeCache\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_by_key_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2372\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2373\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2374\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlinemap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlinemap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   2375\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtorchbind_constants\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2376\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2377\u001b[39m \u001b[38;5;28mself\u001b[39m.cache_key = key\n\u001b[32m   2378\u001b[39m \u001b[38;5;28mself\u001b[39m.cache_path = path\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_inductor/codecache.py:3296\u001b[39m, in \u001b[36mPyCodeCache.load_by_key_path\u001b[39m\u001b[34m(cls, key, path, linemap, attrs)\u001b[39m\n\u001b[32m   3293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.modules_no_attr[path]\n\u001b[32m   3295\u001b[39m in_toplevel = in_toplevel_process()\n\u001b[32m-> \u001b[39m\u001b[32m3296\u001b[39m mod = \u001b[43m_reload_python_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_sys_modules\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_toplevel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3298\u001b[39m \u001b[38;5;66;03m# unzip into separate lines/nodes lists\u001b[39;00m\n\u001b[32m   3299\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m in_toplevel:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_inductor/runtime/compile_tasks.py:31\u001b[39m, in \u001b[36m_reload_python_module\u001b[39m\u001b[34m(key, path, set_sys_modules)\u001b[39m\n\u001b[32m     29\u001b[39m mod.\u001b[34m__file__\u001b[39m = path\n\u001b[32m     30\u001b[39m mod.key = key  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__dict__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__dict__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m set_sys_modules:\n\u001b[32m     33\u001b[39m     sys.modules[mod.\u001b[34m__name__\u001b[39m] = mod\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/folders/2k/7nzhgb9s67b36s310p79c0_00000gp/T/torchinductor_sselva890@cable.comcast.com/xj/cxjekpmnwvjcrzj4pht55ys3ec7chtesven7sbcx56rjc625tyky.py:543\u001b[39m\n\u001b[32m    458\u001b[39m mps_lib_9 = compile_mps_shader(\u001b[33m'''\u001b[39m\n\u001b[32m    459\u001b[39m \u001b[33m    #include <c10/metal/utils.h>\u001b[39m\n\u001b[32m    460\u001b[39m \u001b[33m    #include <c10/metal/reduction_utils.h>\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    474\u001b[39m \u001b[33m    }\u001b[39m\n\u001b[32m    475\u001b[39m \u001b[33m'''\u001b[39m)\n\u001b[32m    478\u001b[39m \u001b[38;5;66;03m# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy, aten.add, aten.native_layer_norm_backward, aten.native_dropout_backward, aten.linear_backward]\u001b[39;00m\n\u001b[32m    479\u001b[39m \u001b[38;5;66;03m# Source node to ATen node mapping:\u001b[39;00m\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Graph fragment:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m \u001b[38;5;66;03m#   %mul_254 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_1128, %mul_253), kwargs = {})\u001b[39;00m\n\u001b[32m    542\u001b[39m \u001b[38;5;66;03m#   %linear_backward_28 : [num_users=3] = call_function[target=torch.ops.aten.linear_backward.default](args = (%relu_4, %mul_254, %convert_element_type_795, [True, True, True]), kwargs = {})\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m mps_lib_10 = \u001b[43mcompile_mps_shader\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'''\u001b[39;49m\n\u001b[32m    544\u001b[39m \u001b[33;43m    #include <c10/metal/utils.h>\u001b[39;49m\n\u001b[32m    545\u001b[39m \u001b[33;43m    #include <c10/metal/reduction_utils.h>\u001b[39;49m\n\u001b[32m    546\u001b[39m \u001b[33;43m    [[max_total_threads_per_threadgroup(288)]]\u001b[39;49m\n\u001b[32m    547\u001b[39m \u001b[33;43m    kernel void generated_kernel(\u001b[39;49m\n\u001b[32m    548\u001b[39m \u001b[33;43m        device float* out_ptr2,\u001b[39;49m\n\u001b[32m    549\u001b[39m \u001b[33;43m        device float* out_ptr5,\u001b[39;49m\n\u001b[32m    550\u001b[39m \u001b[33;43m        device half* out_ptr6,\u001b[39;49m\n\u001b[32m    551\u001b[39m \u001b[33;43m        constant half* in_ptr0,\u001b[39;49m\n\u001b[32m    552\u001b[39m \u001b[33;43m        constant half* in_ptr1,\u001b[39;49m\n\u001b[32m    553\u001b[39m \u001b[33;43m        constant half* in_ptr2,\u001b[39;49m\n\u001b[32m    554\u001b[39m \u001b[33;43m        constant half* in_ptr3,\u001b[39;49m\n\u001b[32m    555\u001b[39m \u001b[33;43m        constant half* in_ptr4,\u001b[39;49m\n\u001b[32m    556\u001b[39m \u001b[33;43m        constant half* in_ptr5,\u001b[39;49m\n\u001b[32m    557\u001b[39m \u001b[33;43m        constant half* in_ptr6,\u001b[39;49m\n\u001b[32m    558\u001b[39m \u001b[33;43m        constant half* in_ptr7,\u001b[39;49m\n\u001b[32m    559\u001b[39m \u001b[33;43m        constant half* in_ptr8,\u001b[39;49m\n\u001b[32m    560\u001b[39m \u001b[33;43m        constant half* in_ptr9,\u001b[39;49m\n\u001b[32m    561\u001b[39m \u001b[33;43m        constant half* in_ptr10,\u001b[39;49m\n\u001b[32m    562\u001b[39m \u001b[33;43m        constant half* in_ptr11,\u001b[39;49m\n\u001b[32m    563\u001b[39m \u001b[33;43m        constant half* in_ptr12,\u001b[39;49m\n\u001b[32m    564\u001b[39m \u001b[33;43m        constant half* in_ptr13,\u001b[39;49m\n\u001b[32m    565\u001b[39m \u001b[33;43m        constant half* in_ptr14,\u001b[39;49m\n\u001b[32m    566\u001b[39m \u001b[33;43m        constant half* in_ptr15,\u001b[39;49m\n\u001b[32m    567\u001b[39m \u001b[33;43m        constant half* in_ptr16,\u001b[39;49m\n\u001b[32m    568\u001b[39m \u001b[33;43m        constant half* in_ptr17,\u001b[39;49m\n\u001b[32m    569\u001b[39m \u001b[33;43m        constant half* in_ptr18,\u001b[39;49m\n\u001b[32m    570\u001b[39m \u001b[33;43m        constant half* in_ptr19,\u001b[39;49m\n\u001b[32m    571\u001b[39m \u001b[33;43m        constant half* in_ptr20,\u001b[39;49m\n\u001b[32m    572\u001b[39m \u001b[33;43m        constant half* in_ptr21,\u001b[39;49m\n\u001b[32m    573\u001b[39m \u001b[33;43m        constant half* in_ptr22,\u001b[39;49m\n\u001b[32m    574\u001b[39m \u001b[33;43m        constant half* in_ptr23,\u001b[39;49m\n\u001b[32m    575\u001b[39m \u001b[33;43m        constant float* in_ptr24,\u001b[39;49m\n\u001b[32m    576\u001b[39m \u001b[33;43m        constant float* in_ptr25,\u001b[39;49m\n\u001b[32m    577\u001b[39m \u001b[33;43m        constant float* in_ptr26,\u001b[39;49m\n\u001b[32m    578\u001b[39m \u001b[33;43m        constant float* in_ptr27,\u001b[39;49m\n\u001b[32m    579\u001b[39m \u001b[33;43m        constant bool* in_ptr28,\u001b[39;49m\n\u001b[32m    580\u001b[39m \u001b[33;43m        uint2 thread_pos [[thread_position_in_grid]],\u001b[39;49m\n\u001b[32m    581\u001b[39m \u001b[33;43m        uint2 group_pos [[thread_position_in_threadgroup]]\u001b[39;49m\n\u001b[32m    582\u001b[39m \u001b[33;43m    ) \u001b[39;49m\u001b[33;43m{\u001b[39;49m\n\u001b[32m    583\u001b[39m \u001b[33;43m        auto xindex = thread_pos.x;\u001b[39;49m\n\u001b[32m    584\u001b[39m \u001b[33;43m        auto r0_index = thread_pos.y;\u001b[39;49m\n\u001b[32m    585\u001b[39m \u001b[33;43m        int r0_1 = r0_index;\u001b[39;49m\n\u001b[32m    586\u001b[39m \u001b[33;43m        int x0 = xindex;\u001b[39;49m\n\u001b[32m    587\u001b[39m \u001b[33;43m        threadgroup float tmp_acc_0[9];\u001b[39;49m\n\u001b[32m    588\u001b[39m \u001b[33;43m        threadgroup float tmp_acc_1[9];\u001b[39;49m\n\u001b[32m    589\u001b[39m \u001b[33;43m        auto tmp0 = static_cast<float>(in_ptr0[r0_1 + 288*x0]);\u001b[39;49m\n\u001b[32m    590\u001b[39m \u001b[33;43m        auto tmp2 = static_cast<float>(in_ptr1[r0_1 + 288*x0]);\u001b[39;49m\n\u001b[32m    591\u001b[39m \u001b[33;43m        auto tmp5 = static_cast<float>(in_ptr2[r0_1 + 288*x0]);\u001b[39;49m\n\u001b[32m    592\u001b[39m \u001b[33;43m        auto tmp8 = static_cast<float>(in_ptr3[r0_1 + 288*x0]);\u001b[39;49m\n\u001b[32m    593\u001b[39m \u001b[33;43m        auto tmp11 = static_cast<float>(in_ptr4[r0_1 + 288*x0]);\u001b[39;49m\n\u001b[32m    594\u001b[39m \u001b[33;43m        auto tmp14 = static_cast<float>(in_ptr5[r0_1 + 288*x0]);\u001b[39;49m\n\u001b[32m    595\u001b[39m \u001b[33;43m        auto tmp17 = static_cast<float>(in_ptr6[r0_1 + 288*x0]);\u001b[39;49m\n\u001b[32m    596\u001b[39m \u001b[33;43m        auto tmp20 = static_cast<float>(in_ptr7[r0_1 + 288*x0]);\u001b[39;49m\n\u001b[32m    597\u001b[39m \u001b[33;43m        auto tmp23 = static_cast<float>(in_ptr8[r0_1 + 288*x0]);\u001b[39;49m\n\u001b[32m    598\u001b[39m \u001b[33;43m        auto tmp26 = static_cast<float>(in_ptr9[r0_1 + 288*x0]);\u001b[39;49m\n\u001b[32m    599\u001b[39m \u001b[33;43m        auto tmp29 = static_cast<float>(in_ptr10[r0_1 + 288*x0]);\u001b[39;49m\n\u001b[32m    600\u001b[39m \u001b[33;43m        auto tmp32 = static_cast<float>(in_ptr11[r0_1 + 288*x0]);\u001b[39;49m\n\u001b[32m    601\u001b[39m \u001b[33;43m        auto tmp35 = static_cast<float>(in_ptr12[r0_1 + 288*x0]);\u001b[39;49m\n\u001b[32m    602\u001b[39m \u001b[33;43m        auto tmp38 = static_cast<float>(in_ptr13[r0_1 + 288*x0]);\u001b[39;49m\n\u001b[32m    603\u001b[39m \u001b[33;43m        auto tmp41 = static_cast<float>(in_ptr14[r0_1 + 288*x0]);\u001b[39;49m\n\u001b[32m    604\u001b[39m \u001b[33;43m        auto tmp44 = static_cast<float>(in_ptr15[r0_1 + 288*x0]);\u001b[39;49m\n\u001b[32m    605\u001b[39m \u001b[33;43m        auto tmp47 = static_cast<float>(in_ptr16[r0_1 + 288*x0]);\u001b[39;49m\n\u001b[32m    606\u001b[39m \u001b[33;43m        auto tmp50 = static_cast<float>(in_ptr17[r0_1 + 288*x0]);\u001b[39;49m\n\u001b[32m    607\u001b[39m \u001b[33;43m        auto tmp53 = static_cast<float>(in_ptr18[r0_1 + 288*x0]);\u001b[39;49m\n\u001b[32m    608\u001b[39m \u001b[33;43m        auto tmp56 = static_cast<float>(in_ptr19[r0_1 + 288*x0]);\u001b[39;49m\n\u001b[32m    609\u001b[39m \u001b[33;43m        auto tmp59 = static_cast<float>(in_ptr20[r0_1 + 288*x0]);\u001b[39;49m\n\u001b[32m    610\u001b[39m \u001b[33;43m        auto tmp62 = static_cast<float>(in_ptr21[r0_1 + 288*x0]);\u001b[39;49m\n\u001b[32m    611\u001b[39m \u001b[33;43m        auto tmp65 = static_cast<float>(in_ptr22[r0_1 + 288*x0]);\u001b[39;49m\n\u001b[32m    612\u001b[39m \u001b[33;43m        auto tmp68 = static_cast<float>(in_ptr23[r0_1 + 288*x0]);\u001b[39;49m\n\u001b[32m    613\u001b[39m \u001b[33;43m        auto tmp71 = in_ptr24[r0_1];\u001b[39;49m\n\u001b[32m    614\u001b[39m \u001b[33;43m        auto tmp74 = in_ptr25[r0_1 + 288*x0];\u001b[39;49m\n\u001b[32m    615\u001b[39m \u001b[33;43m        auto tmp1 = static_cast<float>(tmp0);\u001b[39;49m\n\u001b[32m    616\u001b[39m \u001b[33;43m        auto tmp3 = static_cast<float>(tmp2);\u001b[39;49m\n\u001b[32m    617\u001b[39m \u001b[33;43m        auto tmp4 = tmp1 + tmp3;\u001b[39;49m\n\u001b[32m    618\u001b[39m \u001b[33;43m        auto tmp6 = static_cast<float>(tmp5);\u001b[39;49m\n\u001b[32m    619\u001b[39m \u001b[33;43m        auto tmp7 = tmp4 + tmp6;\u001b[39;49m\n\u001b[32m    620\u001b[39m \u001b[33;43m        auto tmp9 = static_cast<float>(tmp8);\u001b[39;49m\n\u001b[32m    621\u001b[39m \u001b[33;43m        auto tmp10 = tmp7 + tmp9;\u001b[39;49m\n\u001b[32m    622\u001b[39m \u001b[33;43m        auto tmp12 = static_cast<float>(tmp11);\u001b[39;49m\n\u001b[32m    623\u001b[39m \u001b[33;43m        auto tmp13 = tmp10 + tmp12;\u001b[39;49m\n\u001b[32m    624\u001b[39m \u001b[33;43m        auto tmp15 = static_cast<float>(tmp14);\u001b[39;49m\n\u001b[32m    625\u001b[39m \u001b[33;43m        auto tmp16 = tmp13 + tmp15;\u001b[39;49m\n\u001b[32m    626\u001b[39m \u001b[33;43m        auto tmp18 = static_cast<float>(tmp17);\u001b[39;49m\n\u001b[32m    627\u001b[39m \u001b[33;43m        auto tmp19 = tmp16 + tmp18;\u001b[39;49m\n\u001b[32m    628\u001b[39m \u001b[33;43m        auto tmp21 = static_cast<float>(tmp20);\u001b[39;49m\n\u001b[32m    629\u001b[39m \u001b[33;43m        auto tmp22 = tmp19 + tmp21;\u001b[39;49m\n\u001b[32m    630\u001b[39m \u001b[33;43m        auto tmp24 = static_cast<float>(tmp23);\u001b[39;49m\n\u001b[32m    631\u001b[39m \u001b[33;43m        auto tmp25 = tmp22 + tmp24;\u001b[39;49m\n\u001b[32m    632\u001b[39m \u001b[33;43m        auto tmp27 = static_cast<float>(tmp26);\u001b[39;49m\n\u001b[32m    633\u001b[39m \u001b[33;43m        auto tmp28 = tmp25 + tmp27;\u001b[39;49m\n\u001b[32m    634\u001b[39m \u001b[33;43m        auto tmp30 = static_cast<float>(tmp29);\u001b[39;49m\n\u001b[32m    635\u001b[39m \u001b[33;43m        auto tmp31 = tmp28 + tmp30;\u001b[39;49m\n\u001b[32m    636\u001b[39m \u001b[33;43m        auto tmp33 = static_cast<float>(tmp32);\u001b[39;49m\n\u001b[32m    637\u001b[39m \u001b[33;43m        auto tmp34 = tmp31 + tmp33;\u001b[39;49m\n\u001b[32m    638\u001b[39m \u001b[33;43m        auto tmp36 = static_cast<float>(tmp35);\u001b[39;49m\n\u001b[32m    639\u001b[39m \u001b[33;43m        auto tmp37 = tmp34 + tmp36;\u001b[39;49m\n\u001b[32m    640\u001b[39m \u001b[33;43m        auto tmp39 = static_cast<float>(tmp38);\u001b[39;49m\n\u001b[32m    641\u001b[39m \u001b[33;43m        auto tmp40 = tmp37 + tmp39;\u001b[39;49m\n\u001b[32m    642\u001b[39m \u001b[33;43m        auto tmp42 = static_cast<float>(tmp41);\u001b[39;49m\n\u001b[32m    643\u001b[39m \u001b[33;43m        auto tmp43 = tmp40 + tmp42;\u001b[39;49m\n\u001b[32m    644\u001b[39m \u001b[33;43m        auto tmp45 = static_cast<float>(tmp44);\u001b[39;49m\n\u001b[32m    645\u001b[39m \u001b[33;43m        auto tmp46 = tmp43 + tmp45;\u001b[39;49m\n\u001b[32m    646\u001b[39m \u001b[33;43m        auto tmp48 = static_cast<float>(tmp47);\u001b[39;49m\n\u001b[32m    647\u001b[39m \u001b[33;43m        auto tmp49 = tmp46 + tmp48;\u001b[39;49m\n\u001b[32m    648\u001b[39m \u001b[33;43m        auto tmp51 = static_cast<float>(tmp50);\u001b[39;49m\n\u001b[32m    649\u001b[39m \u001b[33;43m        auto tmp52 = tmp49 + tmp51;\u001b[39;49m\n\u001b[32m    650\u001b[39m \u001b[33;43m        auto tmp54 = static_cast<float>(tmp53);\u001b[39;49m\n\u001b[32m    651\u001b[39m \u001b[33;43m        auto tmp55 = tmp52 + tmp54;\u001b[39;49m\n\u001b[32m    652\u001b[39m \u001b[33;43m        auto tmp57 = static_cast<float>(tmp56);\u001b[39;49m\n\u001b[32m    653\u001b[39m \u001b[33;43m        auto tmp58 = tmp55 + tmp57;\u001b[39;49m\n\u001b[32m    654\u001b[39m \u001b[33;43m        auto tmp60 = static_cast<float>(tmp59);\u001b[39;49m\n\u001b[32m    655\u001b[39m \u001b[33;43m        auto tmp61 = tmp58 + tmp60;\u001b[39;49m\n\u001b[32m    656\u001b[39m \u001b[33;43m        auto tmp63 = static_cast<float>(tmp62);\u001b[39;49m\n\u001b[32m    657\u001b[39m \u001b[33;43m        auto tmp64 = tmp61 + tmp63;\u001b[39;49m\n\u001b[32m    658\u001b[39m \u001b[33;43m        auto tmp66 = static_cast<float>(tmp65);\u001b[39;49m\n\u001b[32m    659\u001b[39m \u001b[33;43m        auto tmp67 = tmp64 + tmp66;\u001b[39;49m\n\u001b[32m    660\u001b[39m \u001b[33;43m        auto tmp69 = static_cast<float>(tmp68);\u001b[39;49m\n\u001b[32m    661\u001b[39m \u001b[33;43m        auto tmp70 = tmp67 + tmp69;\u001b[39;49m\n\u001b[32m    662\u001b[39m \u001b[33;43m        out_ptr2[r0_1 + 288*x0] = static_cast<float>(tmp70);\u001b[39;49m\n\u001b[32m    663\u001b[39m \u001b[33;43m        auto tmp72 = tmp70 * tmp71;\u001b[39;49m\n\u001b[32m    664\u001b[39m \u001b[33;43m        auto tmp75 = tmp72 * tmp74;\u001b[39;49m\n\u001b[32m    665\u001b[39m \u001b[33;43m        auto tmp73 = c10::metal::threadgroup_sum(tmp_acc_0, tmp72, r0_index * 1, 288);\u001b[39;49m\n\u001b[32m    666\u001b[39m \u001b[33;43m        auto tmp76 = c10::metal::threadgroup_sum(tmp_acc_1, tmp75, r0_index * 1, 288);\u001b[39;49m\n\u001b[32m    667\u001b[39m \u001b[33;43m        auto tmp77 = in_ptr26[r0_1 + 288*x0];\u001b[39;49m\n\u001b[32m    668\u001b[39m \u001b[33;43m        auto tmp78 = in_ptr27[x0];\u001b[39;49m\n\u001b[32m    669\u001b[39m \u001b[33;43m        auto tmp87 = in_ptr28[r0_1 + 288*x0];\u001b[39;49m\n\u001b[32m    670\u001b[39m \u001b[33;43m        auto tmp79 = 288.0;\u001b[39;49m\n\u001b[32m    671\u001b[39m \u001b[33;43m        auto tmp80 = tmp72 * tmp79;\u001b[39;49m\n\u001b[32m    672\u001b[39m \u001b[33;43m        auto tmp81 = tmp80 - tmp73;\u001b[39;49m\n\u001b[32m    673\u001b[39m \u001b[33;43m        auto tmp82 = tmp74 * tmp76;\u001b[39;49m\n\u001b[32m    674\u001b[39m \u001b[33;43m        auto tmp83 = tmp81 - tmp82;\u001b[39;49m\n\u001b[32m    675\u001b[39m \u001b[33;43m        auto tmp84 = tmp78 * tmp83;\u001b[39;49m\n\u001b[32m    676\u001b[39m \u001b[33;43m        auto tmp85 = tmp77 + tmp84;\u001b[39;49m\n\u001b[32m    677\u001b[39m \u001b[33;43m        out_ptr5[r0_1 + 288*x0] = static_cast<float>(tmp85);\u001b[39;49m\n\u001b[32m    678\u001b[39m \u001b[33;43m        auto tmp86 = static_cast<half>(tmp85);\u001b[39;49m\n\u001b[32m    679\u001b[39m \u001b[33;43m        auto tmp88 = static_cast<half>(tmp87);\u001b[39;49m\n\u001b[32m    680\u001b[39m \u001b[33;43m        auto tmp89 = 1.25;\u001b[39;49m\n\u001b[32m    681\u001b[39m \u001b[33;43m        auto tmp90 = tmp88 * tmp89;\u001b[39;49m\n\u001b[32m    682\u001b[39m \u001b[33;43m        auto tmp91 = tmp86 * tmp90;\u001b[39;49m\n\u001b[32m    683\u001b[39m \u001b[33;43m        out_ptr6[r0_1 + 288*x0] = static_cast<half>(tmp91);\u001b[39;49m\n\u001b[32m    684\u001b[39m \u001b[33;43m    }\u001b[39;49m\n\u001b[32m    685\u001b[39m \u001b[33;43m'''\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[38;5;66;03m# Topologically Sorted Source Nodes: [], Original ATen: [aten.native_layer_norm_backward]\u001b[39;00m\n\u001b[32m    689\u001b[39m \u001b[38;5;66;03m# Source node to ATen node mapping:\u001b[39;00m\n\u001b[32m    690\u001b[39m \u001b[38;5;66;03m# Graph fragment:\u001b[39;00m\n\u001b[32m    691\u001b[39m \u001b[38;5;66;03m#   %mul_252 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_63, %mul_160), kwargs = {})\u001b[39;00m\n\u001b[32m    692\u001b[39m \u001b[38;5;66;03m#   %sum_71 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_252, [0, 1]), kwargs = {})\u001b[39;00m\n\u001b[32m    693\u001b[39m \u001b[38;5;66;03m#   %sum_72 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%add_63, [0, 1]), kwargs = {})\u001b[39;00m\n\u001b[32m    694\u001b[39m mps_lib_11 = compile_mps_shader(\u001b[33m'''\u001b[39m\n\u001b[32m    695\u001b[39m \u001b[33m    #include <c10/metal/utils.h>\u001b[39m\n\u001b[32m    696\u001b[39m \u001b[33m    #include <c10/metal/reduction_utils.h>\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    721\u001b[39m \u001b[33m    }\u001b[39m\n\u001b[32m    722\u001b[39m \u001b[33m'''\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_inductor/runtime/runtime_utils.py:184\u001b[39m, in \u001b[36mcompile_mps_shader\u001b[39m\u001b[34m(source)\u001b[39m\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.mps.compile_shader(source)\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mSyntaxError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mSyntaxError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfailed to compile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr.msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n",
      "\u001b[31mInductorError\u001b[39m: SyntaxError: failed to compile \n    #include <c10/metal/utils.h>\n    #include <c10/metal/reduction_utils.h>\n    [[max_total_threads_per_threadgroup(288)]]\n    kernel void generated_kernel(\n        device float* out_ptr2,\n        device float* out_ptr5,\n        device half* out_ptr6,\n        constant half* in_ptr0,\n        constant half* in_ptr1,\n        constant half* in_ptr2,\n        constant half* in_ptr3,\n        constant half* in_ptr4,\n        constant half* in_ptr5,\n        constant half* in_ptr6,\n        constant half* in_ptr7,\n        constant half* in_ptr8,\n        constant half* in_ptr9,\n        constant half* in_ptr10,\n        constant half* in_ptr11,\n        constant half* in_ptr12,\n        constant half* in_ptr13,\n        constant half* in_ptr14,\n        constant half* in_ptr15,\n        constant half* in_ptr16,\n        constant half* in_ptr17,\n        constant half* in_ptr18,\n        constant half* in_ptr19,\n        constant half* in_ptr20,\n        constant half* in_ptr21,\n        constant half* in_ptr22,\n        constant half* in_ptr23,\n        constant float* in_ptr24,\n        constant float* in_ptr25,\n        constant float* in_ptr26,\n        constant float* in_ptr27,\n        constant bool* in_ptr28,\n        uint2 thread_pos [[thread_position_in_grid]],\n        uint2 group_pos [[thread_position_in_threadgroup]]\n    ) {\n        auto xindex = thread_pos.x;\n        auto r0_index = thread_pos.y;\n        int r0_1 = r0_index;\n        int x0 = xindex;\n        threadgroup float tmp_acc_0[9];\n        threadgroup float tmp_acc_1[9];\n        auto tmp0 = static_cast<float>(in_ptr0[r0_1 + 288*x0]);\n        auto tmp2 = static_cast<float>(in_ptr1[r0_1 + 288*x0]);\n        auto tmp5 = static_cast<float>(in_ptr2[r0_1 + 288*x0]);\n        auto tmp8 = static_cast<float>(in_ptr3[r0_1 + 288*x0]);\n        auto tmp11 = static_cast<float>(in_ptr4[r0_1 + 288*x0]);\n        auto tmp14 = static_cast<float>(in_ptr5[r0_1 + 288*x0]);\n        auto tmp17 = static_cast<float>(in_ptr6[r0_1 + 288*x0]);\n        auto tmp20 = static_cast<float>(in_ptr7[r0_1 + 288*x0]);\n        auto tmp23 = static_cast<float>(in_ptr8[r0_1 + 288*x0]);\n        auto tmp26 = static_cast<float>(in_ptr9[r0_1 + 288*x0]);\n        auto tmp29 = static_cast<float>(in_ptr10[r0_1 + 288*x0]);\n        auto tmp32 = static_cast<float>(in_ptr11[r0_1 + 288*x0]);\n        auto tmp35 = static_cast<float>(in_ptr12[r0_1 + 288*x0]);\n        auto tmp38 = static_cast<float>(in_ptr13[r0_1 + 288*x0]);\n        auto tmp41 = static_cast<float>(in_ptr14[r0_1 + 288*x0]);\n        auto tmp44 = static_cast<float>(in_ptr15[r0_1 + 288*x0]);\n        auto tmp47 = static_cast<float>(in_ptr16[r0_1 + 288*x0]);\n        auto tmp50 = static_cast<float>(in_ptr17[r0_1 + 288*x0]);\n        auto tmp53 = static_cast<float>(in_ptr18[r0_1 + 288*x0]);\n        auto tmp56 = static_cast<float>(in_ptr19[r0_1 + 288*x0]);\n        auto tmp59 = static_cast<float>(in_ptr20[r0_1 + 288*x0]);\n        auto tmp62 = static_cast<float>(in_ptr21[r0_1 + 288*x0]);\n        auto tmp65 = static_cast<float>(in_ptr22[r0_1 + 288*x0]);\n        auto tmp68 = static_cast<float>(in_ptr23[r0_1 + 288*x0]);\n        auto tmp71 = in_ptr24[r0_1];\n        auto tmp74 = in_ptr25[r0_1 + 288*x0];\n        auto tmp1 = static_cast<float>(tmp0);\n        auto tmp3 = static_cast<float>(tmp2);\n        auto tmp4 = tmp1 + tmp3;\n        auto tmp6 = static_cast<float>(tmp5);\n        auto tmp7 = tmp4 + tmp6;\n        auto tmp9 = static_cast<float>(tmp8);\n        auto tmp10 = tmp7 + tmp9;\n        auto tmp12 = static_cast<float>(tmp11);\n        auto tmp13 = tmp10 + tmp12;\n        auto tmp15 = static_cast<float>(tmp14);\n        auto tmp16 = tmp13 + tmp15;\n        auto tmp18 = static_cast<float>(tmp17);\n        auto tmp19 = tmp16 + tmp18;\n        auto tmp21 = static_cast<float>(tmp20);\n        auto tmp22 = tmp19 + tmp21;\n        auto tmp24 = static_cast<float>(tmp23);\n        auto tmp25 = tmp22 + tmp24;\n        auto tmp27 = static_cast<float>(tmp26);\n        auto tmp28 = tmp25 + tmp27;\n        auto tmp30 = static_cast<float>(tmp29);\n        auto tmp31 = tmp28 + tmp30;\n        auto tmp33 = static_cast<float>(tmp32);\n        auto tmp34 = tmp31 + tmp33;\n        auto tmp36 = static_cast<float>(tmp35);\n        auto tmp37 = tmp34 + tmp36;\n        auto tmp39 = static_cast<float>(tmp38);\n        auto tmp40 = tmp37 + tmp39;\n        auto tmp42 = static_cast<float>(tmp41);\n        auto tmp43 = tmp40 + tmp42;\n        auto tmp45 = static_cast<float>(tmp44);\n        auto tmp46 = tmp43 + tmp45;\n        auto tmp48 = static_cast<float>(tmp47);\n        auto tmp49 = tmp46 + tmp48;\n        auto tmp51 = static_cast<float>(tmp50);\n        auto tmp52 = tmp49 + tmp51;\n        auto tmp54 = static_cast<float>(tmp53);\n        auto tmp55 = tmp52 + tmp54;\n        auto tmp57 = static_cast<float>(tmp56);\n        auto tmp58 = tmp55 + tmp57;\n        auto tmp60 = static_cast<float>(tmp59);\n        auto tmp61 = tmp58 + tmp60;\n        auto tmp63 = static_cast<float>(tmp62);\n        auto tmp64 = tmp61 + tmp63;\n        auto tmp66 = static_cast<float>(tmp65);\n        auto tmp67 = tmp64 + tmp66;\n        auto tmp69 = static_cast<float>(tmp68);\n        auto tmp70 = tmp67 + tmp69;\n        out_ptr2[r0_1 + 288*x0] = static_cast<float>(tmp70);\n        auto tmp72 = tmp70 * tmp71;\n        auto tmp75 = tmp72 * tmp74;\n        auto tmp73 = c10::metal::threadgroup_sum(tmp_acc_0, tmp72, r0_index * 1, 288);\n        auto tmp76 = c10::metal::threadgroup_sum(tmp_acc_1, tmp75, r0_index * 1, 288);\n        auto tmp77 = in_ptr26[r0_1 + 288*x0];\n        auto tmp78 = in_ptr27[x0];\n        auto tmp87 = in_ptr28[r0_1 + 288*x0];\n        auto tmp79 = 288.0;\n        auto tmp80 = tmp72 * tmp79;\n        auto tmp81 = tmp80 - tmp73;\n        auto tmp82 = tmp74 * tmp76;\n        auto tmp83 = tmp81 - tmp82;\n        auto tmp84 = tmp78 * tmp83;\n        auto tmp85 = tmp77 + tmp84;\n        out_ptr5[r0_1 + 288*x0] = static_cast<float>(tmp85);\n        auto tmp86 = static_cast<half>(tmp85);\n        auto tmp88 = static_cast<half>(tmp87);\n        auto tmp89 = 1.25;\n        auto tmp90 = tmp88 * tmp89;\n        auto tmp91 = tmp86 * tmp90;\n        out_ptr6[r0_1 + 288*x0] = static_cast<half>(tmp91);\n    }\n with program_source:589:24: error: no 'buffer' resource location available for 'in_ptr28'\n        constant bool* in_ptr28,\n                       ^\n"
     ]
    }
   ],
   "source": [
    "for steps in range(4000):\n",
    "\txb, yb = get_batch('train')\n",
    "\twith autocast(device_type=DEVICE, dtype=torch.float16):\n",
    "\t\tlogits, loss = m(xb, yb)\n",
    "\toptimizer.zero_grad(set_to_none=True)\n",
    "\tloss.backward()\n",
    "\toptimizer.step()\n",
    "\ttrain_loss, val_loss = loss.item(), None\n",
    "\tif steps % 50 == 0:\n",
    "\t\tval_loss = estimate_val_loss(m)\n",
    "\t\tscheduler.step(val_loss)\n",
    "\t\n",
    "\tif steps % 25 == 0:\n",
    "\t\tprint(steps, train_loss, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de620972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 36, 126, 145, 129,  47]], device='mps:0')\n",
      "The mind upill Inauentally noses which arawary noking a clf acou reccrfLef are implapodreams po with repecturness\n",
      "\n",
      "Iftersompar"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(idx)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mThe mind \u001b[39m\u001b[33m\"\u001b[39m, end=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m\t\u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflush\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:59\u001b[39m, in \u001b[36m_wrap_generator.<locals>.generator_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     56\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     57\u001b[39m             \u001b[38;5;66;03m# Pass the last request to the generator and get its response\u001b[39;00m\n\u001b[32m     58\u001b[39m             \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m                 response = gen.send(request)\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# We let the exceptions raised above by the generator's `.throw` or\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# `.send` methods bubble up to our caller, except for StopIteration\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# The generator informed us that it is done: take whatever its\u001b[39;00m\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# returned value (if any) was and indicate that we're done too\u001b[39;00m\n\u001b[32m     66\u001b[39m     \u001b[38;5;66;03m# by returning it (see docs for python's return-statement).\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/toy_transformers/models/gptv1.py:159\u001b[39m, in \u001b[36mLanguageModel.generate\u001b[39m\u001b[34m(self, idx, max_new_tokens)\u001b[39m\n\u001b[32m    157\u001b[39m tok_embed = \u001b[38;5;28mself\u001b[39m.token_embedding_table(idx_cond)\n\u001b[32m    158\u001b[39m pos_embed = \u001b[38;5;28mself\u001b[39m.position_embedding_table(torch.arange(T, device=device))\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtok_embed\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_embed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.lm_head(x)\n\u001b[32m    162\u001b[39m logits = logits[:, -\u001b[32m1\u001b[39m, :]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/nn/modules/container.py:244\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/toy_transformers/models/gptv1.py:93\u001b[39m, in \u001b[36mBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m   x = torch.add(x, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msa\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     94\u001b[39m   x = torch.add(x, \u001b[38;5;28mself\u001b[39m.ffwd(\u001b[38;5;28mself\u001b[39m.ln2(x)))\n\u001b[32m     95\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/toy_transformers/models/gptv1.py:60\u001b[39m, in \u001b[36mMultiHeadAttention.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m   out = torch.cat(\u001b[43m[\u001b[49m\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mheads\u001b[49m\u001b[43m]\u001b[49m, dim = -\u001b[32m1\u001b[39m)\n\u001b[32m     61\u001b[39m   out = \u001b[38;5;28mself\u001b[39m.dropout(\u001b[38;5;28mself\u001b[39m.proj(out))\n\u001b[32m     62\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/toy_transformers/models/gptv1.py:60\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m   out = torch.cat([\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.heads], dim = -\u001b[32m1\u001b[39m)\n\u001b[32m     61\u001b[39m   out = \u001b[38;5;28mself\u001b[39m.dropout(\u001b[38;5;28mself\u001b[39m.proj(out))\n\u001b[32m     62\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/toy_transformers/models/gptv1.py:45\u001b[39m, in \u001b[36mHead.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     40\u001b[39m weights = q @ k.transpose(-\u001b[32m2\u001b[39m, -\u001b[32m1\u001b[39m) * \u001b[38;5;28mself\u001b[39m.attention_scalar\n\u001b[32m     41\u001b[39m weights = F.softmax(\n\u001b[32m     42\u001b[39m   weights.masked_fill(\u001b[38;5;28mself\u001b[39m.tril[:T, :T] == \u001b[32m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m-inf\u001b[39m\u001b[33m'\u001b[39m)),\n\u001b[32m     43\u001b[39m   dim=-\u001b[32m1\u001b[39m\n\u001b[32m     44\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m out = weights @ v\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70\u001b[39m, in \u001b[36mDropout.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/nn/functional.py:1422\u001b[39m, in \u001b[36mdropout\u001b[39m\u001b[34m(input, p, training, inplace)\u001b[39m\n\u001b[32m   1419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p < \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p > \u001b[32m1.0\u001b[39m:\n\u001b[32m   1420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1421\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m     _VF.dropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1423\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "idx = torch.tensor([vocab.encode(\"The mind \")], dtype=torch.long, device=DEVICE)\n",
    "print(idx)\n",
    "print(\"The mind \", end=\"\", flush=True)\n",
    "for token in m.generate(idx, max_new_tokens=200):\n",
    "\tprint(vocab.decode([token.item()])[0], end=\"\", flush=True)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
