{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c45d589",
   "metadata": {},
   "source": [
    "## user-end goals\n",
    "- [ ] use fineweb-edu, sample-10B dataset\n",
    "- [ ] join documents with <BOS>\n",
    "- [ ] add max document length (2x context length) - split at reasonable point\n",
    "## model changes\n",
    "- [ ] (maybe) add GQA\n",
    "- [ ] remove bias in linear layers\n",
    "- [ ] document masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1336159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import sys\n",
    "import pyarrow\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_project_info() -> tuple[Path, Path]:\n",
    "  current = Path.cwd().resolve()\n",
    "  root = current\n",
    "  for parent in [current, *current.parents]:\n",
    "    if (parent / \"toy_transformers\").exists():\n",
    "      root = parent\n",
    "      break\n",
    "  return root, current\n",
    "\n",
    "if 'ROOT_DIR' not in globals():\n",
    "\tROOT_DIR, EXPERIMENT_DIR = get_project_info()\n",
    "\tif str(ROOT_DIR) not in sys.path:\n",
    "\t\tsys.path.append(str(ROOT_DIR))\n",
    "\tif Path.cwd() != ROOT_DIR:\n",
    "\t\tos.chdir(ROOT_DIR)\n",
    "\n",
    "DATA_DIR = ROOT_DIR / \"data/fineweb-edu/sample/10BT\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "from toy_transformers.tokenization import create_bpe, bulk_encode, Vocabulary, TokenizationMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "707e4350",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_BYTES = 100 * 1024 * 1024\n",
    "API_URL = \"https://huggingface.co/api/datasets/HuggingFaceFW/fineweb-edu/parquet/sample-10BT/train\"\n",
    "\n",
    "response = requests.get(API_URL)\n",
    "response.raise_for_status()\n",
    "SHARD_URLS = response.json()\n",
    "\n",
    "def download_shard(url, dst: Path):\n",
    "\tif dst.exists():\n",
    "\t\treturn\n",
    "\t\n",
    "\ttmp = dst.with_suffix(\".tmp\")\n",
    "\ttry:\n",
    "\t\twith requests.get(url, stream=True, headers={\"User-Agent\": \"python\"}) as r:\n",
    "\t\t\tr.raise_for_status()\n",
    "\n",
    "\t\t\ttotal = int(r.headers.get(\"Content-Length\", 0))\n",
    "\t\t\twith open(tmp, \"wb\") as f, tqdm(total=total, unit=\"B\", unit_scale=True, desc=dst.name) as bar:\n",
    "\t\t\t\tfor chunk in r.iter_content(chunk_size=1024*1024):\n",
    "\t\t\t\t\tif chunk:\n",
    "\t\t\t\t\t\tf.write(chunk)\n",
    "\t\t\t\t\t\tbar.update(len(chunk))\n",
    "\t\ttmp.rename(dst)\n",
    "\n",
    "\texcept Exception as e:\n",
    "\t\tif tmp.exists():\n",
    "\t\t\ttmp.unlink()\n",
    "\t\traise e\n",
    "\n",
    "def stream_raw_ds(columns=None, shards=None, batch_size_bytes=BATCH_SIZE_BYTES):\n",
    "\tidxs = shards if shards is not None else range(len(SHARD_URLS))\n",
    "\n",
    "\tbatch_tables = []\n",
    "\tbatch_bytes = 0\n",
    "\n",
    "\tfor i in idxs:\n",
    "\t\tdst = DATA_DIR / f\"{i:02d}.parquet\"\n",
    "\t\tdownload_shard(SHARD_URLS[i], dst)\n",
    "\n",
    "\t\tpf = pq.ParquetFile(dst)\n",
    "\n",
    "\t\tfor rg in range(pf.metadata.num_row_groups):\n",
    "\t\t\ttable = pf.read_row_group(rg, columns=columns)\n",
    "\t\t\tbatch_tables.append(table)\n",
    "\t\t\tbatch_bytes += table.nbytes\n",
    "\n",
    "\t\t\tif batch_bytes >= batch_size_bytes:\n",
    "\t\t\t\tyield pyarrow.concat_tables(batch_tables)\n",
    "\t\t\t\tbatch_tables = []\n",
    "\t\t\t\tbatch_bytes = 0\n",
    "\n",
    "\tif batch_tables:\n",
    "\t\tyield pyarrow.concat_tables(batch_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3120960",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1 << 15\n",
    "BOS = \"<BOS>\"\n",
    "PAD = \"<PAD>\"\n",
    "SPECIAL_TOKENS = [BOS, PAD]\n",
    "\n",
    "VOCAB_PATH = EXPERIMENT_DIR / \"data/vocab_32k.json\"\n",
    "OUTPUT_DIR = EXPERIMENT_DIR / \"data/encoded\"\n",
    "\n",
    "def stream_texts(shards=None):\n",
    "\tfor batch in stream_raw_ds(columns=[\"text\"], shards=shards):\n",
    "\t\tyield (BOS + BOS.join(batch[\"text\"].to_pylist())).encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fc391b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocessing: 162shard [05:29,  2.03s/shard]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting merging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BPE Training: 100%|██████████| 32510/32510 [02:36<00:00, 208.29it/s] \n"
     ]
    }
   ],
   "source": [
    "vocab = create_bpe(\n",
    "\tdata_iter=stream_texts(shards=[0, 1, 2, 3, 4]),\n",
    "\tvocab_size=VOCAB_SIZE,\n",
    "\tmode=TokenizationMode.BYTES,\n",
    "\tspecial_tokens=SPECIAL_TOKENS\n",
    ")\n",
    "\n",
    "vocab.save(EXPERIMENT_DIR / \"data/vocab_32k.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7951eba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary.load(EXPERIMENT_DIR / \"data/vocab_32k.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b91ce916",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding: 5chunk [00:20,  2.41s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 99,988,431 tokens to shard_0000.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding: 9chunk [00:36,  3.10s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 99,997,968 tokens to shard_0001.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding: 13chunk [00:51,  4.39s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 99,996,637 tokens to shard_0002.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding: 18chunk [00:57,  2.16s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 99,990,697 tokens to shard_0003.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding: 22chunk [01:12,  2.63s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 99,997,764 tokens to shard_0004.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding: 26chunk [01:26,  3.52s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 99,995,411 tokens to shard_0005.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding: 32chunk [01:37,  2.28s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 99,988,214 tokens to shard_0006.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding: 33chunk [01:37,  2.96s/chunk]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 51,902,468 tokens to shard_0007.bin\n"
     ]
    }
   ],
   "source": [
    "bulk_encode(\n",
    "\tdoc_iter=stream_texts(shards=[0]),\n",
    "\tvocab=vocab,\n",
    "\tvocab_path=VOCAB_PATH,\n",
    "\toutput_dir=OUTPUT_DIR,\n",
    "\tsplit_token=BOS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c103eb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard shape: (99990484,), dtype: uint16\n",
      "First 20 token IDs: [    0    87 10453 13649    59  8865   328   444  2798   415  5218   284\n",
      "   362 12366  3868    33    55    47  1156   308]\n",
      "\n",
      "=== First 200 tokens decoded ===\n",
      "<BOS>Version info: Code for this page was tested in Mplus version 6.12.\n",
      "Zero-inflated poisson regression is used to model count data that has an excess of zero counts. Further, theory suggests that the excess zeros are generated by a separate process from the count values and that the excess zeros can be modeled independently. Thus, the zip model has two parts, a poisson count model and the logit model for predicting excess zeros. You may want to review these Data Analysis Example pages, Poisson Regression and Logit Regression.\n",
      "Please Note: The purpose of this page is to show how to use various data analysis commands. It does not cover all aspects of the research process which researchers are expected to do. In particular, it does not cover data cleaning and verification, verification of assumptions, model diagnostics and potential follow-up analyses.\n",
      "Example 1. School administrators study the attendance behavior of high school juniors over one semester at two schools.\n",
      "\n",
      "BOS id: 0\n",
      "Number of documents in shard: 21450\n",
      "First 5 BOS positions: [   0 6445 6946 7379 7712]\n"
     ]
    }
   ],
   "source": [
    "from toy_transformers.tokenization import _read_shard\n",
    "\n",
    "shard = _read_shard(OUTPUT_DIR / \"shard_0003.bin\")\n",
    "print(f\"Shard shape: {shard.shape}, dtype: {shard.dtype}\")\n",
    "print(f\"First 20 token IDs: {shard[:20]}\")\n",
    "\n",
    "# decode first 200 tokens\n",
    "decoded = vocab.decode(shard[:200].tolist())\n",
    "text = b\"\".join(decoded).decode(\"utf-8\", errors=\"replace\")\n",
    "print(f\"\\n=== First 200 tokens decoded ===\\n{text}\")\n",
    "\n",
    "# check BOS positions\n",
    "bos_id = vocab.token_to_idx[b\"<BOS>\"]\n",
    "bos_positions = (shard == bos_id).nonzero()[0]\n",
    "print(f\"\\nBOS id: {bos_id}\")\n",
    "print(f\"Number of documents in shard: {len(bos_positions)}\")\n",
    "print(f\"First 5 BOS positions: {bos_positions[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "483e76ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "shuffling: 162729doc [00:01, 109999.43doc/s]\n"
     ]
    }
   ],
   "source": [
    "from toy_transformers.tokenization import shuffle_shards\n",
    "\n",
    "shuffle_shards(OUTPUT_DIR, EXPERIMENT_DIR / \"data/shuffled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ef85a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard shape: (94011011,), dtype: uint16\n",
      "First 20 token IDs: [    0  6100  3989   329  9155    12   458 12374   329   427 11973 25620\n",
      "  4601 25303   559  6489   335   263 14354    12]\n",
      "\n",
      "=== First 200 tokens decoded ===\n",
      "<BOS>Not Just for Kids\n",
      "The Hunt for Falling Leaves...\n",
      "Nature's Color on the Ground\n",
      "by Mary Catherine Ball\n",
      "Being a reporter, I am always looking for an adventure. Last week, I found one.\n",
      "I left work to go on a simple journey, but it turned out to be much more.\n",
      "First, I crossed a mud-ridden stream. Then, I came face to face with flying creatures, fighting to get near me. I even endured webmakers spinning my hair into a shiny maze.\n",
      "Where did I go? Into the woods, of course. Why? I wanted to gather some fallen leaves.\n",
      "My luck was good that day. I was able to spy lots of different kinds of leaves lying on the ground. Some were leaves I had never seen. Some were still green, while others were changing to their autumn colors.\n",
      "Have you ever hunted for leaves? I wonder if you know the names of five of the trees that live in your neighborhood? I bet the answer is no.\n",
      "Well, me neither. So I had five of the leaves analyzed. I had found the leaves from an oak, a beech, a sweet gum tree and more.\n",
      "Now, I invite you to make this journey.\n",
      "Narrow body with pointy edges\n",
      "Narrow body with pointy edges\n",
      "May grow berries\n",
      "Good for sap & color\n",
      "3 distinct leafs\n",
      "May grow nuts\n",
      "This is your task...\n",
      "Travel to the deep, dark woods (in the daylight) to\n",
      "\n",
      "BOS id: 0\n",
      "Number of documents in shard: 20232\n",
      "First 5 BOS positions: [    0  1549  7372  9617 13331]\n"
     ]
    }
   ],
   "source": [
    "from toy_transformers.tokenization import _read_shard\n",
    "\n",
    "shard = _read_shard(EXPERIMENT_DIR / \"data/shuffled/shard_0000.bin\")\n",
    "print(f\"Shard shape: {shard.shape}, dtype: {shard.dtype}\")\n",
    "print(f\"First 20 token IDs: {shard[:20]}\")\n",
    "\n",
    "# decode first 200 tokens\n",
    "decoded = vocab.decode(shard[:300].tolist())\n",
    "text = b\"\".join(decoded).decode(\"utf-8\", errors=\"replace\")\n",
    "print(f\"\\n=== First 200 tokens decoded ===\\n{text}\")\n",
    "\n",
    "# check BOS positions\n",
    "bos_id = vocab.token_to_idx[b\"<BOS>\"]\n",
    "bos_positions = (shard == bos_id).nonzero()[0]\n",
    "print(f\"\\nBOS id: {bos_id}\")\n",
    "print(f\"Number of documents in shard: {len(bos_positions)}\")\n",
    "print(f\"First 5 BOS positions: {bos_positions[:5]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
