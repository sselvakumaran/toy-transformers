{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c45d589",
   "metadata": {},
   "source": [
    "## user-end goals\n",
    "- [ ] use fineweb-edu, sample-10B dataset\n",
    "- [ ] join documents with <BOS>\n",
    "- [ ] add max document length (2x context length) - split at reasonable point\n",
    "## model changes\n",
    "- [ ] (maybe) add GQA\n",
    "- [ ] remove bias in linear layers\n",
    "- [ ] document masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1336159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import sys\n",
    "import pyarrow\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_project_info() -> tuple[Path, Path]:\n",
    "  current = Path.cwd().resolve()\n",
    "  root = current\n",
    "  for parent in [current, *current.parents]:\n",
    "    if (parent / \"toy_transformers\").exists():\n",
    "      root = parent\n",
    "      break\n",
    "  return root, current\n",
    "\n",
    "if 'ROOT_DIR' not in globals():\n",
    "\tROOT_DIR, EXPERIMENT_DIR = get_project_info()\n",
    "\tif str(ROOT_DIR) not in sys.path:\n",
    "\t\tsys.path.append(str(ROOT_DIR))\n",
    "\tif Path.cwd() != ROOT_DIR:\n",
    "\t\tos.chdir(ROOT_DIR)\n",
    "\n",
    "DATA_DIR = ROOT_DIR / \"data/fineweb-edu/sample/10BT\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "from toy_transformers.tokenization import create_bpe, bulk_encode, Vocabulary, TokenizationMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "707e4350",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_BYTES = 100 * 1024 * 1024\n",
    "API_URL = \"https://huggingface.co/api/datasets/HuggingFaceFW/fineweb-edu/parquet/sample-10BT/train\"\n",
    "\n",
    "response = requests.get(API_URL)\n",
    "response.raise_for_status()\n",
    "SHARD_URLS = response.json()\n",
    "\n",
    "def download_shard(url, dst: Path):\n",
    "\tif dst.exists():\n",
    "\t\treturn\n",
    "\t\n",
    "\ttmp = dst.with_suffix(\".tmp\")\n",
    "\ttry:\n",
    "\t\twith requests.get(url, stream=True, headers={\"User-Agent\": \"python\"}) as r:\n",
    "\t\t\tr.raise_for_status()\n",
    "\n",
    "\t\t\ttotal = int(r.headers.get(\"Content-Length\", 0))\n",
    "\t\t\twith open(tmp, \"wb\") as f, tqdm(total=total, unit=\"B\", unit_scale=True, desc=dst.name) as bar:\n",
    "\t\t\t\tfor chunk in r.iter_content(chunk_size=1024*1024):\n",
    "\t\t\t\t\tif chunk:\n",
    "\t\t\t\t\t\tf.write(chunk)\n",
    "\t\t\t\t\t\tbar.update(len(chunk))\n",
    "\t\ttmp.rename(dst)\n",
    "\n",
    "\texcept Exception as e:\n",
    "\t\tif tmp.exists():\n",
    "\t\t\ttmp.unlink()\n",
    "\t\traise e\n",
    "\n",
    "def stream_raw_ds(columns=None, shards=None, batch_size_bytes=BATCH_SIZE_BYTES):\n",
    "\tidxs = shards if shards is not None else range(len(SHARD_URLS))\n",
    "\n",
    "\tbatch_tables = []\n",
    "\tbatch_bytes = 0\n",
    "\n",
    "\tfor i in idxs:\n",
    "\t\tdst = DATA_DIR / f\"{i:02d}.parquet\"\n",
    "\t\tdownload_shard(SHARD_URLS[i], dst)\n",
    "\n",
    "\t\tpf = pq.ParquetFile(dst)\n",
    "\n",
    "\t\tfor rg in range(pf.metadata.num_row_groups):\n",
    "\t\t\ttable = pf.read_row_group(rg, columns=columns)\n",
    "\t\t\tbatch_tables.append(table)\n",
    "\t\t\tbatch_bytes += table.nbytes\n",
    "\n",
    "\t\t\tif batch_bytes >= batch_size_bytes:\n",
    "\t\t\t\tyield pyarrow.concat_tables(batch_tables)\n",
    "\t\t\t\tbatch_tables = []\n",
    "\t\t\t\tbatch_bytes = 0\n",
    "\n",
    "\tif batch_tables:\n",
    "\t\tyield pyarrow.concat_tables(batch_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3120960",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1 << 15\n",
    "BOS = \"<BOS>\"\n",
    "SPECIAL_TOKENS = [BOS]\n",
    "\n",
    "VOCAB_PATH = EXPERIMENT_DIR / \"data/vocab_32k.json\"\n",
    "vocab = Vocabulary.load(VOCAB_PATH)\n",
    "OUTPUT_DIR = EXPERIMENT_DIR / \"data/encoded\"\n",
    "\n",
    "def stream_texts(shards=None):\n",
    "\tfor batch in stream_raw_ds(columns=[\"text\"], shards=shards):\n",
    "\t\tyield (BOS + BOS.join(batch[\"text\"].to_pylist())).encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc391b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocessing: 0shard [00:00, ?shard/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01.parquet: 100%|██████████| 2.15G/2.15G [01:12<00:00, 29.7MB/s]\n",
      "preprocessing: 65shard [02:43,  2.51s/shard]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting merging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BPE Training: 100%|██████████| 32511/32511 [01:08<00:00, 476.79it/s] \n"
     ]
    }
   ],
   "source": [
    "vocab = create_bpe(\n",
    "\tdata_iter=stream_texts(shards=[0, 1]),\n",
    "\tvocab_size=VOCAB_SIZE,\n",
    "\tmode=TokenizationMode.BYTES,\n",
    "\tspecial_tokens=SPECIAL_TOKENS\n",
    ")\n",
    "\n",
    "vocab.save(EXPERIMENT_DIR / \"data/vocab_32k.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b91ce916",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding: 5chunk [00:12,  1.50s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 99,983,250 tokens to shard_0000.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding: 9chunk [00:14,  1.49chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 99,999,435 tokens to shard_0001.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding: 13chunk [00:20,  1.66s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 99,999,023 tokens to shard_0002.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding: 18chunk [00:23,  1.14chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 99,999,170 tokens to shard_0003.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding: 22chunk [00:26,  1.40chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 99,981,093 tokens to shard_0004.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding: 26chunk [00:30,  1.13chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 99,999,188 tokens to shard_0005.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding: 31chunk [00:34,  1.32chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 99,998,612 tokens to shard_0006.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding: 33chunk [00:34,  1.04s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 51,858,617 tokens to shard_0007.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bulk_encode(\n",
    "\tdoc_iter=stream_texts(shards=[0]),\n",
    "\tvocab=vocab,\n",
    "\tvocab_path=VOCAB_PATH,\n",
    "\toutput_dir=OUTPUT_DIR,\n",
    "\tsplit_token=BOS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c103eb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard shape: (99999170,), dtype: uint16\n",
      "First 20 token IDs: [    0    83   323  9422  4949   288   477  1187   387  4536    11    46\n",
      " 22609  2385   288  1205  2917   281  1699  8552]\n",
      "\n",
      "=== First 200 tokens decoded ===\n",
      "<BOS>Rotavirus vaccine and intussusception\n",
      "- Inform parents and carers of young infants receiving rotavirus vaccine of the rare risk of intussusception following the vaccine and how to be alert to the signs and symptoms of the condition.\n",
      "- Do not give rotavirus vaccine outside the recommended age limits.\n",
      "- Do not give rotavirus vaccine to a baby with a history of intussusception.\n",
      "- Report any cases of intussusception following rotavirus vaccination through the usual reporting arrangements for adverse events following immunisation in your State and Territory.\n",
      "Risk of intussusception\n",
      "- There is new evidence from Australian and overseas studies suggesting a small increased risk of intussusception in infants following rotavirus vaccination.\n",
      "- The increased risk appears to occur mainly in the first 1- 7 days following the first dose of rotavirus vaccine.\n",
      "- Intussusception is rare, with an annual incidence under 12 months of age in Australia of 80 per 100,000\n",
      "\n",
      "BOS id: 0\n",
      "Number of documents in shard: 21447\n",
      "First 5 BOS positions: [    0  3710  4488 17315 32504]\n"
     ]
    }
   ],
   "source": [
    "from toy_transformers.tokenization import _read_shard\n",
    "\n",
    "shard = _read_shard(OUTPUT_DIR / \"shard_0003.bin\")\n",
    "print(f\"Shard shape: {shard.shape}, dtype: {shard.dtype}\")\n",
    "print(f\"First 20 token IDs: {shard[:20]}\")\n",
    "\n",
    "# decode first 200 tokens\n",
    "decoded = vocab.decode(shard[:200].tolist())\n",
    "text = b\"\".join(decoded).decode(\"utf-8\", errors=\"replace\")\n",
    "print(f\"\\n=== First 200 tokens decoded ===\\n{text}\")\n",
    "\n",
    "# check BOS positions\n",
    "bos_id = vocab.token_to_idx[b\"<BOS>\"]\n",
    "bos_positions = (shard == bos_id).nonzero()[0]\n",
    "print(f\"\\nBOS id: {bos_id}\")\n",
    "print(f\"Number of documents in shard: {len(bos_positions)}\")\n",
    "print(f\"First 5 BOS positions: {bos_positions[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37644cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS id: 0\n",
      "Found 8 shards\n",
      "\n",
      "shard_0000.bin:   99,983,250 tokens, 21,437 docs, starts_with_bos=True\n",
      "shard_0001.bin:   99,999,435 tokens, 21,952 docs, starts_with_bos=True\n",
      "shard_0002.bin:   99,999,023 tokens, 21,885 docs, starts_with_bos=True\n",
      "shard_0003.bin:   99,999,170 tokens, 21,447 docs, starts_with_bos=True\n",
      "shard_0004.bin:   99,981,093 tokens, 21,560 docs, starts_with_bos=True\n",
      "shard_0005.bin:   99,999,188 tokens, 21,780 docs, starts_with_bos=True\n",
      "shard_0006.bin:   99,998,612 tokens, 21,738 docs, starts_with_bos=True\n",
      "shard_0007.bin:   51,858,617 tokens, 10,930 docs, starts_with_bos=True\n",
      "\n",
      "=== Cross-shard boundary check ===\n",
      "All shard boundaries align with document boundaries.\n",
      "\n",
      "=== stream_texts BOS check ===\n",
      "  chunk 0: 107,504,256 bytes, starts_with_BOS=True\n",
      "  chunk 1: 105,618,284 bytes, starts_with_BOS=True\n",
      "  chunk 2: 107,026,664 bytes, starts_with_BOS=True\n",
      "  chunk 3: 105,112,500 bytes, starts_with_BOS=True\n",
      "  chunk 4: 107,668,853 bytes, starts_with_BOS=True\n",
      "  ... (4 more chunks)\n"
     ]
    }
   ],
   "source": [
    "# Cell - Verify document integrity across shards\n",
    "import numpy as np\n",
    "from toy_transformers.tokenization import _read_shard\n",
    "\n",
    "shard_dir = OUTPUT_DIR\n",
    "shard_files = sorted(shard_dir.glob(\"shard_*.bin\"))\n",
    "bos_id = vocab.token_to_idx[b\"<BOS>\"]\n",
    "\n",
    "print(f\"BOS id: {bos_id}\")\n",
    "print(f\"Found {len(shard_files)} shards\\n\")\n",
    "\n",
    "# 1) Check that every shard starts with BOS (except possibly shard 0 \n",
    "#    if the first batch didn't start with BOS)\n",
    "for path in shard_files:\n",
    "    shard = _read_shard(path)\n",
    "    starts_with_bos = shard[0] == bos_id if len(shard) > 0 else False\n",
    "    bos_count = np.sum(shard == bos_id)\n",
    "    print(f\"{path.name}: {len(shard):>12,} tokens, {bos_count:>6,} docs, starts_with_bos={starts_with_bos}\")\n",
    "\n",
    "# 2) Check cross-shard boundaries: load consecutive pairs and verify\n",
    "#    shard N+1 starts with BOS (meaning shard N ended at a document boundary)\n",
    "print(\"\\n=== Cross-shard boundary check ===\")\n",
    "issues = 0\n",
    "for i in range(len(shard_files) - 1):\n",
    "    cur = _read_shard(shard_files[i])\n",
    "    nxt = _read_shard(shard_files[i + 1])\n",
    "    \n",
    "    next_starts_bos = nxt[0] == bos_id if len(nxt) > 0 else True\n",
    "    if not next_starts_bos:\n",
    "        # decode tokens around the boundary to see what happened\n",
    "        tail = vocab.decode(cur[-10:].tolist())\n",
    "        head = vocab.decode(nxt[:10].tolist())\n",
    "        tail_str = b\"\".join(tail).decode(\"utf-8\", errors=\"replace\")\n",
    "        head_str = b\"\".join(head).decode(\"utf-8\", errors=\"replace\")\n",
    "        print(f\"SPLIT at shard {i}/{i+1}:\")\n",
    "        print(f\"  tail: ...{tail_str!r}\")\n",
    "        print(f\"  head: {head_str!r}...\")\n",
    "        issues += 1\n",
    "\n",
    "if issues == 0:\n",
    "    print(\"All shard boundaries align with document boundaries.\")\n",
    "else:\n",
    "    print(f\"\\n{issues} document(s) split across shards!\")\n",
    "\n",
    "# 3) Check stream_texts: verify first batch starts with BOS\n",
    "print(\"\\n=== stream_texts BOS check ===\")\n",
    "for i, chunk in enumerate(stream_texts(shards=[0])):\n",
    "    starts_bos = chunk.startswith(b\"<BOS>\")\n",
    "    # check if it ends mid-document (no trailing BOS) — not necessarily \n",
    "    # an issue since the next chunk should start with BOS\n",
    "    print(f\"  chunk {i}: {len(chunk):,} bytes, starts_with_BOS={starts_bos}\")\n",
    "    if i >= 4:\n",
    "        print(f\"  ... ({i} more chunks)\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
