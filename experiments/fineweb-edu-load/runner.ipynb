{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c45d589",
   "metadata": {},
   "source": [
    "## user-end goals\n",
    "- [ ] use fineweb-edu, sample-10B dataset\n",
    "- [ ] join documents with <BOS>\n",
    "- [ ] add max document length (2x context length) - split at reasonable point\n",
    "## model changes\n",
    "- [ ] (maybe) add GQA\n",
    "- [ ] remove bias in linear layers\n",
    "- [ ] document masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1336159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import sys\n",
    "import pyarrow\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_project_info() -> tuple[Path, Path]:\n",
    "  current = Path.cwd().resolve()\n",
    "  root = current\n",
    "  for parent in [current, *current.parents]:\n",
    "    if (parent / \"toy_transformers\").exists():\n",
    "      root = parent\n",
    "      break\n",
    "  return root, current\n",
    "\n",
    "if 'ROOT_DIR' not in globals():\n",
    "\tROOT_DIR, EXPERIMENT_DIR = get_project_info()\n",
    "\tif str(ROOT_DIR) not in sys.path:\n",
    "\t\tsys.path.append(str(ROOT_DIR))\n",
    "\tif Path.cwd() != ROOT_DIR:\n",
    "\t\tos.chdir(ROOT_DIR)\n",
    "\n",
    "DATA_DIR = ROOT_DIR / \"data/fineweb-edu/sample/10BT\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "from toy_transformers.tokenization import create_bpe, bulk_encode, Vocabulary, TokenizationMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "707e4350",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_BYTES = 100 * 1024 * 1024\n",
    "API_URL = \"https://huggingface.co/api/datasets/HuggingFaceFW/fineweb-edu/parquet/sample-10BT/train\"\n",
    "\n",
    "response = requests.get(API_URL)\n",
    "response.raise_for_status()\n",
    "SHARD_URLS = response.json()\n",
    "\n",
    "def download_shard(url, dst: Path):\n",
    "\tif dst.exists():\n",
    "\t\treturn\n",
    "\t\n",
    "\ttmp = dst.with_suffix(\".tmp\")\n",
    "\ttry:\n",
    "\t\twith requests.get(url, stream=True, headers={\"User-Agent\": \"python\"}) as r:\n",
    "\t\t\tr.raise_for_status()\n",
    "\n",
    "\t\t\ttotal = int(r.headers.get(\"Content-Length\", 0))\n",
    "\t\t\twith open(tmp, \"wb\") as f, tqdm(total=total, unit=\"B\", unit_scale=True, desc=dst.name) as bar:\n",
    "\t\t\t\tfor chunk in r.iter_content(chunk_size=1024*1024):\n",
    "\t\t\t\t\tif chunk:\n",
    "\t\t\t\t\t\tf.write(chunk)\n",
    "\t\t\t\t\t\tbar.update(len(chunk))\n",
    "\t\ttmp.rename(dst)\n",
    "\n",
    "\texcept Exception as e:\n",
    "\t\tif tmp.exists():\n",
    "\t\t\ttmp.unlink()\n",
    "\t\traise e\n",
    "\n",
    "def stream_raw_ds(columns=None, shards=None, batch_size_bytes=BATCH_SIZE_BYTES):\n",
    "\tidxs = shards if shards is not None else range(len(SHARD_URLS))\n",
    "\n",
    "\tbatch_tables = []\n",
    "\tbatch_bytes = 0\n",
    "\n",
    "\tfor i in idxs:\n",
    "\t\tdst = DATA_DIR / f\"{i:02d}.parquet\"\n",
    "\t\tdownload_shard(SHARD_URLS[i], dst)\n",
    "\n",
    "\t\tpf = pq.ParquetFile(dst)\n",
    "\n",
    "\t\tfor rg in range(pf.metadata.num_row_groups):\n",
    "\t\t\ttable = pf.read_row_group(rg, columns=columns)\n",
    "\t\t\tbatch_tables.append(table)\n",
    "\t\t\tbatch_bytes += table.nbytes\n",
    "\n",
    "\t\t\tif batch_bytes >= batch_size_bytes:\n",
    "\t\t\t\tyield pyarrow.concat_tables(batch_tables)\n",
    "\t\t\t\tbatch_tables = []\n",
    "\t\t\t\tbatch_bytes = 0\n",
    "\n",
    "\tif batch_tables:\n",
    "\t\tyield pyarrow.concat_tables(batch_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3120960",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1 << 15\n",
    "BOS = \"<BOS>\"\n",
    "SPECIAL_TOKENS = [BOS]\n",
    "\n",
    "VOCAB_PATH = EXPERIMENT_DIR / \"data/vocab_32k.json\"\n",
    "vocab = Vocabulary.load(VOCAB_PATH)\n",
    "OUTPUT_DIR = EXPERIMENT_DIR / \"data/encoded\"\n",
    "\n",
    "def stream_texts(shards=None):\n",
    "\tfor batch in stream_raw_ds(columns=[\"text\"], shards=shards):\n",
    "\t\tyield (BOS + BOS.join(batch[\"text\"].to_pylist())).encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc391b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocessing: 0shard [00:00, ?shard/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01.parquet: 100%|██████████| 2.15G/2.15G [01:12<00:00, 29.7MB/s]\n",
      "preprocessing: 65shard [02:43,  2.51s/shard]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting merging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BPE Training: 100%|██████████| 32511/32511 [01:08<00:00, 476.79it/s] \n"
     ]
    }
   ],
   "source": [
    "vocab = create_bpe(\n",
    "\tdata_iter=stream_texts(shards=[0, 1]),\n",
    "\tvocab_size=VOCAB_SIZE,\n",
    "\tmode=TokenizationMode.BYTES,\n",
    "\tspecial_tokens=SPECIAL_TOKENS\n",
    ")\n",
    "\n",
    "vocab.save(EXPERIMENT_DIR / \"data/vocab_32k.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b91ce916",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding: 5chunk [00:19,  2.69s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 99,988,208 tokens to shard_0000.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding: 9chunk [00:33,  2.83s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 99,997,731 tokens to shard_0001.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding: 13chunk [00:49,  4.81s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 99,996,422 tokens to shard_0002.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding: 18chunk [00:55,  1.87s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 99,990,484 tokens to shard_0003.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding: 22chunk [01:09,  2.33s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 99,997,550 tokens to shard_0004.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding: 26chunk [01:23,  3.42s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 99,995,174 tokens to shard_0005.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding: 33chunk [01:35,  2.89s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 99,987,983 tokens to shard_0006.bin\n",
      "wrote 51,902,347 tokens to shard_0007.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bulk_encode(\n",
    "\tdoc_iter=stream_texts(shards=[0]),\n",
    "\tvocab=vocab,\n",
    "\tvocab_path=VOCAB_PATH,\n",
    "\toutput_dir=OUTPUT_DIR,\n",
    "\tsplit_token=BOS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c103eb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard shape: (99999170,), dtype: uint16\n",
      "First 20 token IDs: [    0    83   323  9422  4949   288   477  1187   387  4536    11    46\n",
      " 22609  2385   288  1205  2917   281  1699  8552]\n",
      "\n",
      "=== First 200 tokens decoded ===\n",
      "<BOS>Rotavirus vaccine and intussusception\n",
      "- Inform parents and carers of young infants receiving rotavirus vaccine of the rare risk of intussusception following the vaccine and how to be alert to the signs and symptoms of the condition.\n",
      "- Do not give rotavirus vaccine outside the recommended age limits.\n",
      "- Do not give rotavirus vaccine to a baby with a history of intussusception.\n",
      "- Report any cases of intussusception following rotavirus vaccination through the usual reporting arrangements for adverse events following immunisation in your State and Territory.\n",
      "Risk of intussusception\n",
      "- There is new evidence from Australian and overseas studies suggesting a small increased risk of intussusception in infants following rotavirus vaccination.\n",
      "- The increased risk appears to occur mainly in the first 1- 7 days following the first dose of rotavirus vaccine.\n",
      "- Intussusception is rare, with an annual incidence under 12 months of age in Australia of 80 per 100,000\n",
      "\n",
      "BOS id: 0\n",
      "Number of documents in shard: 21447\n",
      "First 5 BOS positions: [    0  3710  4488 17315 32504]\n"
     ]
    }
   ],
   "source": [
    "from toy_transformers.tokenization import _read_shard\n",
    "\n",
    "shard = _read_shard(OUTPUT_DIR / \"shard_0003.bin\")\n",
    "print(f\"Shard shape: {shard.shape}, dtype: {shard.dtype}\")\n",
    "print(f\"First 20 token IDs: {shard[:20]}\")\n",
    "\n",
    "# decode first 200 tokens\n",
    "decoded = vocab.decode(shard[:200].tolist())\n",
    "text = b\"\".join(decoded).decode(\"utf-8\", errors=\"replace\")\n",
    "print(f\"\\n=== First 200 tokens decoded ===\\n{text}\")\n",
    "\n",
    "# check BOS positions\n",
    "bos_id = vocab.token_to_idx[b\"<BOS>\"]\n",
    "bos_positions = (shard == bos_id).nonzero()[0]\n",
    "print(f\"\\nBOS id: {bos_id}\")\n",
    "print(f\"Number of documents in shard: {len(bos_positions)}\")\n",
    "print(f\"First 5 BOS positions: {bos_positions[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "483e76ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "shuffling: 162729doc [00:01, 122486.18doc/s]\n"
     ]
    }
   ],
   "source": [
    "from toy_transformers.tokenization import shuffle_shards\n",
    "\n",
    "shuffle_shards(OUTPUT_DIR, EXPERIMENT_DIR / \"data/shuffled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ef85a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard shape: (94997855,), dtype: uint16\n",
      "First 20 token IDs: [    0   457 17175 11597    11  2030   499   262  2655    45 27705   288\n",
      " 24267   284 11597 31772   383   116  2980    45]\n",
      "\n",
      "=== First 200 tokens decoded ===\n",
      "<BOS>The Independent Jane\n",
      "For all the love, romance and scandal in Jane Austen’s books, what they are really about is freedom and independence. Independence of thought and the freedom to choose.\n",
      "Elizabeth’s refusal of Mr. Collins offer of marriage showed an independence seldom seen in heroines of the day. Her refusal of Mr. Darcy while triggered by anger showed a level of independence that left him shocked and stunned.\n",
      "The freedom she exhibited in finally accepting him in direct defiance of Lady Catherine and knowing her father would disapprove was unusual even for Austen. In her last book Anne Elliot is persuaded to refuse Captain Wentworth at Lady Russel’s insistence.\n",
      "Although Jane played by the rules of the day, all of her writing is infused with how she wanted life to be. She ‘screams’ her outrage at the limitations for women in Emma.\n",
      "When accosted by Mrs. Elton, Jane Fairfax says,\n",
      "“Excuse me, ma’am, but this is by no means my intention; I make no inquiry myself, and should be sorry to have any made by my friends. When I am quite determined as to the time, I am not at all afraid of being long unemployed. There are places in town, offices, where inquiry would soon produce something — offices for the sale, not quite of human flesh, but of human intellect.”\n",
      "“Oh! my dear, human flesh! You quite shock me; if\n",
      "\n",
      "BOS id: 0\n",
      "Number of documents in shard: 20565\n",
      "First 5 BOS positions: [    0  6645  8680 12229 15541]\n"
     ]
    }
   ],
   "source": [
    "from toy_transformers.tokenization import _read_shard\n",
    "\n",
    "shard = _read_shard(EXPERIMENT_DIR / \"data/shuffled/shard_0001.bin\")\n",
    "print(f\"Shard shape: {shard.shape}, dtype: {shard.dtype}\")\n",
    "print(f\"First 20 token IDs: {shard[:20]}\")\n",
    "\n",
    "# decode first 200 tokens\n",
    "decoded = vocab.decode(shard[:300].tolist())\n",
    "text = b\"\".join(decoded).decode(\"utf-8\", errors=\"replace\")\n",
    "print(f\"\\n=== First 200 tokens decoded ===\\n{text}\")\n",
    "\n",
    "# check BOS positions\n",
    "bos_id = vocab.token_to_idx[b\"<BOS>\"]\n",
    "bos_positions = (shard == bos_id).nonzero()[0]\n",
    "print(f\"\\nBOS id: {bos_id}\")\n",
    "print(f\"Number of documents in shard: {len(bos_positions)}\")\n",
    "print(f\"First 5 BOS positions: {bos_positions[:5]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
