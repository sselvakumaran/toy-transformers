{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c45d589",
   "metadata": {},
   "source": [
    "## user-end goals\n",
    "- [ ] use fineweb-edu, sample-10B dataset\n",
    "- [ ] join documents with <BOS>\n",
    "- [ ] add max document length (2x context length) - split at reasonable point\n",
    "## model changes\n",
    "- [ ] (maybe) add GQA\n",
    "- [ ] remove bias in linear layers\n",
    "- [ ] document masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1336159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import sys\n",
    "import pyarrow\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_project_info() -> tuple[Path, Path]:\n",
    "  current = Path.cwd().resolve()\n",
    "  root = current\n",
    "  for parent in [current, *current.parents]:\n",
    "    if (parent / \"toy_transformers\").exists():\n",
    "      root = parent\n",
    "      break\n",
    "  return root, current\n",
    "\n",
    "if 'ROOT_DIR' not in globals():\n",
    "\tROOT_DIR, EXPERIMENT_DIR = get_project_info()\n",
    "\tif str(ROOT_DIR) not in sys.path:\n",
    "\t\tsys.path.append(str(ROOT_DIR))\n",
    "\tif Path.cwd() != ROOT_DIR:\n",
    "\t\tos.chdir(ROOT_DIR)\n",
    "\n",
    "DATA_DIR = ROOT_DIR / \"data/fineweb-edu/sample/10BT\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "from toy_transformers.tokenization import create_bpe, TokenizationMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "707e4350",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_BYTES = 100 * 1024 * 1024\n",
    "API_URL = \"https://huggingface.co/api/datasets/HuggingFaceFW/fineweb-edu/parquet/sample-10BT/train\"\n",
    "\n",
    "response = requests.get(API_URL)\n",
    "response.raise_for_status()\n",
    "SHARD_URLS = response.json()\n",
    "\n",
    "def download_shard(url, dst: Path):\n",
    "\tif dst.exists():\n",
    "\t\treturn\n",
    "\t\n",
    "\ttmp = dst.with_suffix(\".tmp\")\n",
    "\ttry:\n",
    "\t\twith requests.get(url, stream=True, headers={\"User-Agent\": \"python\"}) as r:\n",
    "\t\t\tr.raise_for_status()\n",
    "\n",
    "\t\t\ttotal = int(r.headers.get(\"Content-Length\", 0))\n",
    "\t\t\twith open(tmp, \"wb\") as f, tqdm(total=total, unit=\"B\", unit_scale=True, desc=dst.name) as bar:\n",
    "\t\t\t\tfor chunk in r.iter_content(chunk_size=1024*1024):\n",
    "\t\t\t\t\tif chunk:\n",
    "\t\t\t\t\t\tf.write(chunk)\n",
    "\t\t\t\t\t\tbar.update(len(chunk))\n",
    "\t\ttmp.rename(dst)\n",
    "\n",
    "\texcept Exception as e:\n",
    "\t\tif tmp.exists():\n",
    "\t\t\ttmp.unlink()\n",
    "\t\traise e\n",
    "\n",
    "def stream_raw_ds(columns=None, shards=None, batch_size_bytes=BATCH_SIZE_BYTES):\n",
    "\tidxs = shards if shards is not None else range(len(SHARD_URLS))\n",
    "\n",
    "\tbatch_tables = []\n",
    "\tbatch_bytes = 0\n",
    "\n",
    "\tfor i in idxs:\n",
    "\t\tdst = DATA_DIR / f\"{i:02d}.parquet\"\n",
    "\t\tdownload_shard(SHARD_URLS[i], dst)\n",
    "\n",
    "\t\tpf = pq.ParquetFile(dst)\n",
    "\n",
    "\t\tfor rg in range(pf.metadata.num_row_groups):\n",
    "\t\t\ttable = pf.read_row_group(rg, columns=columns)\n",
    "\t\t\tbatch_tables.append(table)\n",
    "\t\t\tbatch_bytes += table.nbytes\n",
    "\n",
    "\t\t\tif batch_bytes >= batch_size_bytes:\n",
    "\t\t\t\tyield pyarrow.concat_tables(batch_tables)\n",
    "\t\t\t\tbatch_tables = []\n",
    "\t\t\t\tbatch_bytes = 0\n",
    "\n",
    "\tif batch_tables:\n",
    "\t\tyield pyarrow.concat_tables(batch_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19dcf61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02.parquet: 100%|██████████| 2.15G/2.15G [01:09<00:00, 30.8MB/s]\n",
      "03.parquet: 100%|██████████| 2.15G/2.15G [01:28<00:00, 24.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "for batch in stream_raw_ds(columns=[\"text\"], shards=[2, 3]):\n",
    "\tcontinue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fc391b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocessing: 0shard [00:00, ?shard/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01.parquet: 100%|██████████| 2.15G/2.15G [01:12<00:00, 29.7MB/s]\n",
      "preprocessing: 65shard [02:43,  2.51s/shard]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting merging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BPE Training: 100%|██████████| 32511/32511 [01:08<00:00, 476.79it/s] \n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 1 << 15\n",
    "BOS = \"<BOS>\"\n",
    "SPECIAL_TOKENS = [BOS]\n",
    "\n",
    "def stream_texts(shards=None):\n",
    "\tfor batch in stream_raw_ds(columns=[\"text\"], shards=shards):\n",
    "\t\tyield (BOS.join(batch[\"text\"].to_pylist())).encode('utf-8')\n",
    "\n",
    "vocab = create_bpe(\n",
    "\tdata_iter=stream_texts(shards=[0, 1]),\n",
    "\tvocab_size=VOCAB_SIZE,\n",
    "\tmode=TokenizationMode.BYTES,\n",
    "\tspecial_tokens=SPECIAL_TOKENS\n",
    ")\n",
    "vocab.save(EXPERIMENT_DIR / \"data/vocab_32k.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
