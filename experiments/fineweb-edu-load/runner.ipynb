{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c45d589",
   "metadata": {},
   "source": [
    "## user-end goals\n",
    "- [ ] use fineweb-edu, sample-10B dataset\n",
    "- [ ] join documents with <BOS>\n",
    "- [ ] add max document length (2x context length) - split at reasonable point\n",
    "## model changes\n",
    "- [ ] (maybe) add GQA\n",
    "- [ ] remove bias in linear layers\n",
    "- [ ] document masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1336159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import sys\n",
    "import pyarrow\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_project_info() -> tuple[Path, Path]:\n",
    "  current = Path.cwd().resolve()\n",
    "  root = current\n",
    "  for parent in [current, *current.parents]:\n",
    "    if (parent / \"toy_transformers\").exists():\n",
    "      root = parent\n",
    "      break\n",
    "  return root, current\n",
    "\n",
    "if 'ROOT_DIR' not in globals():\n",
    "\tROOT_DIR, EXPERIMENT_DIR = get_project_info()\n",
    "\tif str(ROOT_DIR) not in sys.path:\n",
    "\t\tsys.path.append(str(ROOT_DIR))\n",
    "\tif Path.cwd() != ROOT_DIR:\n",
    "\t\tos.chdir(ROOT_DIR)\n",
    "\n",
    "DATA_DIR = ROOT_DIR / \"data/fineweb-edu/sample/10BT\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "from toy_transformers.tokenization import create_bpe, TokenizationMode, Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "707e4350",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_BYTES = 100 * 1024 * 1024\n",
    "API_URL = \"https://huggingface.co/api/datasets/HuggingFaceFW/fineweb-edu/parquet/sample-10BT/train\"\n",
    "\n",
    "response = requests.get(API_URL)\n",
    "response.raise_for_status()\n",
    "SHARD_URLS = response.json()\n",
    "\n",
    "def download_shard(url, dst: Path):\n",
    "\tif dst.exists():\n",
    "\t\treturn\n",
    "\t\n",
    "\ttmp = dst.with_suffix(\".tmp\")\n",
    "\ttry:\n",
    "\t\twith requests.get(url, stream=True, headers={\"User-Agent\": \"python\"}) as r:\n",
    "\t\t\tr.raise_for_status()\n",
    "\n",
    "\t\t\ttotal = int(r.headers.get(\"Content-Length\", 0))\n",
    "\t\t\twith open(tmp, \"wb\") as f, tqdm(total=total, unit=\"B\", unit_scale=True, desc=dst.name) as bar:\n",
    "\t\t\t\tfor chunk in r.iter_content(chunk_size=1024*1024):\n",
    "\t\t\t\t\tif chunk:\n",
    "\t\t\t\t\t\tf.write(chunk)\n",
    "\t\t\t\t\t\tbar.update(len(chunk))\n",
    "\t\ttmp.rename(dst)\n",
    "\n",
    "\texcept Exception as e:\n",
    "\t\tif tmp.exists():\n",
    "\t\t\ttmp.unlink()\n",
    "\t\traise e\n",
    "\n",
    "def stream_raw_ds(columns=None, shards=None, batch_size_bytes=BATCH_SIZE_BYTES):\n",
    "\tidxs = shards if shards is not None else range(len(SHARD_URLS))\n",
    "\n",
    "\tbatch_tables = []\n",
    "\tbatch_bytes = 0\n",
    "\n",
    "\tfor i in idxs:\n",
    "\t\tdst = DATA_DIR / f\"{i:02d}.parquet\"\n",
    "\t\tdownload_shard(SHARD_URLS[i], dst)\n",
    "\n",
    "\t\tpf = pq.ParquetFile(dst)\n",
    "\n",
    "\t\tfor rg in range(pf.metadata.num_row_groups):\n",
    "\t\t\ttable = pf.read_row_group(rg, columns=columns)\n",
    "\t\t\tbatch_tables.append(table)\n",
    "\t\t\tbatch_bytes += table.nbytes\n",
    "\n",
    "\t\t\tif batch_bytes >= batch_size_bytes:\n",
    "\t\t\t\tyield pyarrow.concat_tables(batch_tables)\n",
    "\t\t\t\tbatch_tables = []\n",
    "\t\t\t\tbatch_bytes = 0\n",
    "\n",
    "\tif batch_tables:\n",
    "\t\tyield pyarrow.concat_tables(batch_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fc391b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocessing: 0shard [00:00, ?shard/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01.parquet: 100%|██████████| 2.15G/2.15G [00:44<00:00, 48.5MB/s]\n",
      "02.parquet: 100%|██████████| 2.15G/2.15G [00:46<00:00, 46.1MB/s]\n",
      "03.parquet: 100%|██████████| 2.15G/2.15G [00:43<00:00, 49.3MB/s]\n",
      "04.parquet: 100%|██████████| 2.15G/2.15G [00:48<00:00, 44.1MB/s]\n",
      "preprocessing: 162shard [07:40,  2.84s/shard]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting merging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BPE Training: 100%|██████████| 32511/32511 [02:27<00:00, 220.66it/s] \n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 1 << 15\n",
    "BOS = \"<BOS>\"\n",
    "SPECIAL_TOKENS = [BOS]\n",
    "\n",
    "def stream_texts(shards=None):\n",
    "\tfor batch in stream_raw_ds(columns=[\"text\"], shards=shards):\n",
    "\t\tyield (BOS.join(batch[\"text\"].to_pylist())).encode('utf-8')\n",
    "\n",
    "vocab = create_bpe(\n",
    "\tdata_iter=stream_texts(shards=[0, 1, 2, 3, 4]),\n",
    "\tvocab_size=VOCAB_SIZE,\n",
    "\tmode=TokenizationMode.BYTES,\n",
    "\tspecial_tokens=SPECIAL_TOKENS\n",
    ")\n",
    "vocab.save(EXPERIMENT_DIR / \"data/vocab_32k.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92cce46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary.load(EXPERIMENT_DIR / \"data/vocab_32k.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "feae5c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Original (first 200 chars) ===\n",
      "<BOS>Water: What You Can Do\n",
      "What You Can Do\n",
      "Provides information on how you can get involved including ways to protect human health and the environment by raising awareness about potential threats to \n",
      "\n",
      "=== Token IDs (first 30) ===\n",
      "[0, 11148, 59, 1759, 1227, 1606, 2474, 11, 1604, 1227, 1606, 2474, 11, 16295, 1718, 1019, 334, 656, 355, 411, 858, 2701, 1221, 1912, 289, 1972, 1157, 818, 288, 262]\n",
      "\n",
      "=== Decoded tokens (first 30) ===\n",
      "  '<BOS>'\n",
      "  'Water'\n",
      "  ':'\n",
      "  ' What'\n",
      "  ' You'\n",
      "  ' Can'\n",
      "  ' Do'\n",
      "  '\\n'\n",
      "  'What'\n",
      "  ' You'\n",
      "  ' Can'\n",
      "  ' Do'\n",
      "  '\\n'\n",
      "  'Prov'\n",
      "  'ides'\n",
      "  ' information'\n",
      "  ' on'\n",
      "  ' how'\n",
      "  ' you'\n",
      "  ' can'\n",
      "  ' get'\n",
      "  ' involved'\n",
      "  ' including'\n",
      "  ' ways'\n",
      "  ' to'\n",
      "  ' protect'\n",
      "  ' human'\n",
      "  ' health'\n",
      "  ' and'\n",
      "  ' the'\n",
      "\n",
      "=== Roundtrip matches: True\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for batch in stream_raw_ds(columns=[\"text\"], shards=[0]):\n",
    "    docs = batch[\"text\"].to_pylist()\n",
    "    doc = random.choice(docs)\n",
    "    break\n",
    "\n",
    "text = \"<BOS>\" + doc[:500]  # first 500 chars to keep output readable\n",
    "encoded = vocab.encode(text.encode(\"utf-8\"))\n",
    "decoded_tokens = vocab.decode(encoded)\n",
    "reconstructed = b\"\".join(decoded_tokens).decode(\"utf-8\")\n",
    "\n",
    "print(f\"\\n=== Original (first 200 chars) ===\\n{text[:200]}\")\n",
    "print(f\"\\n=== Token IDs (first 30) ===\\n{encoded[:30]}\")\n",
    "print(f\"\\n=== Decoded tokens (first 30) ===\")\n",
    "for t in decoded_tokens[:30]:\n",
    "    print(f\"  {t.decode('utf-8', errors='replace')!r}\")\n",
    "print(f\"\\n=== Roundtrip matches: {reconstructed == text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
