{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Run02: GPT-v1 with Full Artifact Chain\n",
    "\n",
    "This experiment follows the workflow from gpt-v1-w-tokenizer but uses the full artifact chain:\n",
    "1. RawDataset - Store raw text with compression\n",
    "2. Vocabulary - BPE tokenizer (256 tokens)\n",
    "3. TokenizedData - Tokenized tensor with artifact references\n",
    "4. TrainingRun - Model training with checkpoints\n",
    "5. Text Generation Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def get_project_info() -> Path:\n",
    "  current = Path.cwd().resolve()\n",
    "  root = current\n",
    "  for parent in [current, *current.parents]:\n",
    "    if (parent / \"toy_transformers\").exists():\n",
    "      root = parent\n",
    "      break\n",
    "  return root, current\n",
    "\n",
    "if 'ROOT_DIR' not in globals():\n",
    "\tROOT_DIR, EXPERIMENT_DIR = get_project_info()\n",
    "\tif str(ROOT_DIR) not in sys.path:\n",
    "\t\tsys.path.append(str(ROOT_DIR))\n",
    "\tif Path.cwd() != ROOT_DIR:\n",
    "\t\tos.chdir(ROOT_DIR)\n",
    "\n",
    "print(f\"Root: {ROOT_DIR}\")\n",
    "print(f\"Experiment: {EXPERIMENT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toy_transformers.data import RawDataset, register_raw_dataset\n",
    "from toy_transformers.data import tokenization\n",
    "from toy_transformers.data.bpe import Vocabulary\n",
    "from toy_transformers.training.training_run import TrainingRun\n",
    "from toy_transformers.configs import (\n",
    "    DataConfig, GPTv1Config, OptimizerConfig,\n",
    "    AdamWConfig, ReduceLROnPlateauConfig\n",
    ")\n",
    "from toy_transformers.utilities import io\n",
    "from toy_transformers.utilities.reproducibility import set_all_seeds\n",
    "\n",
    "print(\"Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artifact-chain-header",
   "metadata": {},
   "source": [
    "## Stage 1: Create RawDataset Artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-raw-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artifact paths\n",
    "ARTIFACTS_DIR = EXPERIMENT_DIR / \"artifacts\"\n",
    "RAW_DATA_PATH = ARTIFACTS_DIR / \"raw_dataset\"\n",
    "VOCAB_PATH = ARTIFACTS_DIR / \"vocab256\"\n",
    "TOKENIZED_PATH = ARTIFACTS_DIR / \"tokenized_data\"\n",
    "CHECKPOINT_PATH = ARTIFACTS_DIR / \"checkpoint\"\n",
    "\n",
    "# Load input text\n",
    "with open(ROOT_DIR / \"data/input.txt\", 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# register raw dataset artifact\n",
    "raw_dataset = register_raw_dataset(raw_text, str(RAW_DATA_PATH))\n",
    "print(f\"RawDataset created: {raw_dataset.artifact_id}\")\n",
    "print(f\"Text length: {len(raw_dataset.text):,} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocab-header",
   "metadata": {},
   "source": [
    "## Stage 2: Create Vocabulary Artifact (BPE, 256 tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-vocab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary from raw dataset (256 tokens)\n",
    "vocab = tokenization.create_vocabulary_from_raw(\n",
    "    str(RAW_DATA_PATH),\n",
    "    str(VOCAB_PATH),\n",
    "    vocab_size=256\n",
    ")\n",
    "print(f\"Vocabulary created: {vocab.artifact_id}\")\n",
    "print(f\"Vocab size: {len(vocab.tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenized-header",
   "metadata": {},
   "source": [
    "## Stage 3: Create TokenizedData Artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-tokenized",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize using vocabulary\n",
    "tokenized_data = tokenization.tokenize_with_vocabulary(\n",
    "    str(VOCAB_PATH),\n",
    "    str(RAW_DATA_PATH),\n",
    "    str(TOKENIZED_PATH)\n",
    ")\n",
    "print(f\"TokenizedData created: {tokenized_data.artifact_id}\")\n",
    "print(f\"Tokens: {len(tokenized_data.data):,}\")\n",
    "print(f\"Linked vocab: {tokenized_data.vocab_id}\")\n",
    "print(f\"Linked raw: {tokenized_data.raw_dataset_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## Stage 4: Setup Model and Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_all_seeds(42, deterministic=True)\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# Model config (similar to gpt-v1-w-tokenizer)\n",
    "model_config = {\n",
    "    \"n_heads\": 6,\n",
    "    \"n_embed\": 288,\n",
    "    \"n_layers\": 6,\n",
    "    \"dropout\": 0.2\n",
    "}\n",
    "\n",
    "# Data config\n",
    "data_config = DataConfig(\n",
    "    vocab_size=256,\n",
    "    block_size=256,\n",
    "    batch_size=16,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Optimizer config with scheduler\n",
    "optimizer_config = OptimizerConfig(\n",
    "    optimizer_type=\"adamw\",\n",
    "    optimizer_params=AdamWConfig(lr=3e-4, weight_decay=0.01),\n",
    "    scheduler=ReduceLROnPlateauConfig(\n",
    "        mode=\"min\",\n",
    "        factor=0.1,\n",
    "        patience=10\n",
    "    ),\n",
    "    device=device,\n",
    "    seed=42,\n",
    "    max_epochs=1\n",
    ")\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Model: gpt-v1, n_embed={model_config['n_embed']}, n_layers={model_config['n_layers']}\")\n",
    "print(f\"Data: vocab_size={data_config.vocab_size}, block_size={data_config.block_size}, batch_size={data_config.batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-run-header",
   "metadata": {},
   "source": [
    "## Stage 5: Create TrainingRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-training-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training run\n",
    "training_run = TrainingRun(\n",
    "    model_name=\"gpt-v1\",\n",
    "    model_config=model_config,\n",
    "    data_config=data_config,\n",
    "    optimizer_config=optimizer_config,\n",
    "    processed_dataset_id=tokenized_data.artifact_id\n",
    ")\n",
    "\n",
    "# Set dataset hash for verification\n",
    "training_run.set_dataset_hash(tokenized_data)\n",
    "\n",
    "# Create model and optimizer\n",
    "model = training_run.create_model().to(device)\n",
    "optimizer = training_run.create_optimizer(model)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"TrainingRun created\")\n",
    "print(f\"Model parameters: {num_params:,}\")\n",
    "print(f\"Dataset hash: {training_run.dataset_hash}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-header",
   "metadata": {},
   "source": [
    "## Stage 6: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-loop",
   "metadata": {},
   "outputs": [],
   "source": "from torch.amp import autocast\nfrom tqdm import tqdm\n\ntorch.set_float32_matmul_precision(\"medium\")\nmodel.train()\n\nnum_steps = 4000\nsave_every = 1000\nlog_every = 50\n\ndataloader = training_run.create_dataloader(tokenized_data, epoch=0)\n\nfor step in tqdm(range(num_steps), desc=\"Training\"):\n    try:\n        x, y = next(dataloader)\n    except StopIteration:\n        training_run.epoch += 1\n        training_run.batches_completed = 0\n        dataloader = training_run.create_dataloader(tokenized_data, training_run.epoch)\n        x, y = next(dataloader)\n\n    x, y = x.to(device), y.to(device)\n\n    with autocast(device_type=device, dtype=torch.float16):\n        logits, loss = model(x, y)\n\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.optimizer.step()  # Step underlying optimizer only\n\n    training_run.step += 1\n    training_run.batches_completed += 1\n\n    train_loss = loss.item()\n    training_run.log_step(train_loss=train_loss, lr=optimizer.get_lr())\n\n    # Log and step scheduler periodically (matching original pattern)\n    if step % log_every == 0:\n        if optimizer.scheduler is not None:\n            optimizer.scheduler.step(train_loss)\n        print(f\"Step {step}: loss={train_loss:.4f}, lr={optimizer.get_lr():.2e}\")\n\n    # Save checkpoint\n    if step > 0 and step % save_every == 0:\n        training_run.save(str(CHECKPOINT_PATH), model, optimizer)\n        print(f\"Checkpoint saved at step {step}\")\n\n# Final save\ntraining_run.save(str(CHECKPOINT_PATH), model, optimizer)\nprint(f\"Training complete! Final loss: {train_loss:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "generation-header",
   "metadata": {},
   "source": [
    "## Stage 7: Text Generation Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "text-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocabulary for decoding\n",
    "vocab_data = io.load(str(VOCAB_PATH))\n",
    "vocab_obj = Vocabulary.from_state_dict(vocab_data)\n",
    "\n",
    "# Generate text\n",
    "model.eval()\n",
    "seed_text = \"The \"\n",
    "seed_tokens = vocab_obj.encode(seed_text)\n",
    "idx = torch.tensor([seed_tokens], dtype=torch.long, device=device)\n",
    "\n",
    "print(f\"Seed: '{seed_text}'\")\n",
    "print(f\"Generating 200 tokens...\\n\")\n",
    "\n",
    "print(seed_text, end=\"\", flush=True)\n",
    "with torch.no_grad():\n",
    "    for token in model.generate(idx, max_new_tokens=200):\n",
    "        print(vocab_obj.decode([token.item()])[0], end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Summary: Artifact Chain\n",
    "\n",
    "This experiment created the following artifacts:\n",
    "- `artifacts/raw_dataset/` - RawDataset (compressed text)\n",
    "- `artifacts/vocab256/` - Vocabulary (BPE with 256 tokens)\n",
    "- `artifacts/tokenized_data/` - TokenizedData (with vocab and raw_dataset references)\n",
    "- `artifacts/checkpoint/` - TrainingRun (model weights, optimizer state, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Artifact Chain Summary:\")\n",
    "print(f\"  RawDataset: {raw_dataset.artifact_id}\")\n",
    "print(f\"  Vocabulary: {vocab.artifact_id}\")\n",
    "print(f\"  TokenizedData: {tokenized_data.artifact_id}\")\n",
    "print(f\"    -> vocab_id: {tokenized_data.vocab_id}\")\n",
    "print(f\"    -> raw_dataset_id: {tokenized_data.raw_dataset_id}\")\n",
    "print(f\"  TrainingRun: step={training_run.step}, epoch={training_run.epoch}\")\n",
    "print(f\"    -> processed_dataset_id: {training_run.processed_dataset_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}