{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2597617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch \n",
    "from pathlib import Path\n",
    "\n",
    "def get_project_info() -> Path:\n",
    "  current = Path.cwd().resolve()\n",
    "  root = current\n",
    "  for parent in [current, *current.parents]:\n",
    "    if (parent / \"toy_transformers\").exists():\n",
    "      root = parent\n",
    "      break\n",
    "  return root, current\n",
    "\n",
    "if 'ROOT_DIR' not in globals():\n",
    "\tROOT_DIR, EXPERIMENT_DIR = get_project_info()\n",
    "\tif str(ROOT_DIR) not in sys.path:\n",
    "\t\tsys.path.append(str(ROOT_DIR))\n",
    "\tif Path.cwd() != ROOT_DIR:\n",
    "\t\tos.chdir(ROOT_DIR)\n",
    "\n",
    "from toy_transformers.models import gptv3\n",
    "from toy_transformers import tokenization\n",
    "from toy_transformers import checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca1310df",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 256\n",
    "BATCH_SIZE = 16\n",
    "MODE = tokenization.TokenizationMode.STR\n",
    "DEVICE = \"mps\"\n",
    "\n",
    "config = gptv3.GPTv3Config(\n",
    "\tvocab_size=VOCAB_SIZE,\n",
    "\tblock_size=256,\n",
    "\tdevice=DEVICE,\n",
    "\tn_heads=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23787b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = EXPERIMENT_DIR / f\"vocab_{VOCAB_SIZE}.json\"\n",
    "raw_data_path = ROOT_DIR / \"data/gutenberg/freud-interpretation-of-dreams.txt\"\n",
    "\n",
    "if not vocab_path.exists():\n",
    "\traw_data = open(raw_data_path, \"r\")\n",
    "\tvocab = tokenization.create_bpe(\n",
    "\t\traw_data, \n",
    "\t\tVOCAB_SIZE, MODE\n",
    "\t)\n",
    "\tvocab.save(vocab_path)\n",
    "else:\n",
    "\tvocab = tokenization.Vocabulary.load(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbcae302",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(\n",
    "\tvocab.encode(open(raw_data_path, \"r\").read()),\n",
    "\tdtype=torch.long\n",
    ").to(device=DEVICE)\n",
    "\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "block_size, batch_size = config.block_size, BATCH_SIZE\n",
    "def get_batch(split):\n",
    "  data = train_data if split == 'train' else val_data\n",
    "  idxs = torch.randint(len(data) - block_size, (batch_size,), device=DEVICE)\n",
    "  x = torch.stack([data[i:i+block_size] for i in idxs])\n",
    "  y = torch.stack([data[i+1:i+block_size+1] for i in idxs])\n",
    "  return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_val_loss(model):\n",
    "  model.eval()\n",
    "  X, Y = get_batch(\"val\")\n",
    "  _, loss = model(X, Y)\n",
    "  model.train()\n",
    "  return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfc13aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "m = gptv3.LanguageModel(config).to(device=DEVICE)\n",
    "m.compile()\n",
    "\n",
    "optimizer = m.get_optimizer(weight_decay=0.1, lr=3e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "  optimizer,\n",
    "  mode='min',\n",
    "  factor=0.1,\n",
    "  patience=10\n",
    ")\n",
    "\n",
    "from torch.amp import autocast\n",
    "\n",
    "@torch.compile(fullgraph=False)\n",
    "def opt_step():\n",
    "\toptimizer.step()\n",
    "\n",
    "\n",
    "TOTAL_STEPS = 5000\n",
    "CKPT_DIR: Path = EXPERIMENT_DIR / \"checkpoints\"\n",
    "TRAINING_CONFIG = {\n",
    "\t\"lr\": 3e-4, \n",
    "\t\"batch_size\": BATCH_SIZE, \"vocab_path\": str(vocab_path)\n",
    "}\n",
    "\n",
    "metrics = []\n",
    "step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a2e7c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0219 15:40:30.080000 71146 torch/_logging/_internal.py:1154] [1/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 3.9244933128356934 4.1749444007873535\n",
      "100 3.5931828022003174 3.8514957427978516\n",
      "150 3.1167306900024414 3.548898220062256\n",
      "200 2.8385024070739746 3.3778066635131836\n",
      "250 2.767003059387207 3.2359979152679443\n",
      "300 2.7373557090759277 3.358879804611206\n",
      "350 2.5502915382385254 3.1376328468322754\n",
      "400 2.378495693206787 2.8449530601501465\n",
      "450 2.4831795692443848 2.952868700027466\n",
      "500 2.1727793216705322 2.9904608726501465\n",
      "550 2.2676777839660645 2.911193609237671\n",
      "600 2.2127254009246826 2.8323745727539062\n",
      "650 2.201648712158203 2.9746642112731934\n",
      "700 2.1552319526672363 2.7987470626831055\n",
      "750 2.0808825492858887 2.7265307903289795\n",
      "800 2.0988922119140625 2.929091453552246\n",
      "850 1.9184622764587402 2.814074993133545\n",
      "900 2.0258474349975586 2.9404420852661133\n",
      "950 1.911578893661499 2.9324519634246826\n",
      "1000 1.9286599159240723 3.0614914894104004\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m step < TOTAL_STEPS:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \txb, yb = \u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \t\u001b[38;5;28;01mwith\u001b[39;00m autocast(device_type=DEVICE, dtype=torch.bfloat16):\n\u001b[32m      4\u001b[39m \t\tlogits, loss = m(xb, yb)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mget_batch\u001b[39m\u001b[34m(split)\u001b[39m\n\u001b[32m     12\u001b[39m data = train_data \u001b[38;5;28;01mif\u001b[39;00m split == \u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m val_data\n\u001b[32m     13\u001b[39m idxs = torch.randint(\u001b[38;5;28mlen\u001b[39m(data) - block_size, (batch_size,), device=DEVICE)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m x = torch.stack(\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m+\u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midxs\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m     15\u001b[39m y = torch.stack([data[i+\u001b[32m1\u001b[39m:i+block_size+\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idxs])\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x, y\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     12\u001b[39m data = train_data \u001b[38;5;28;01mif\u001b[39;00m split == \u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m val_data\n\u001b[32m     13\u001b[39m idxs = torch.randint(\u001b[38;5;28mlen\u001b[39m(data) - block_size, (batch_size,), device=DEVICE)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m x = torch.stack([data[i:i+block_size] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idxs])\n\u001b[32m     15\u001b[39m y = torch.stack([data[i+\u001b[32m1\u001b[39m:i+block_size+\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idxs])\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x, y\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "while step < TOTAL_STEPS:\n",
    "\txb, yb = get_batch('train')\n",
    "\twith autocast(device_type=DEVICE, dtype=torch.bfloat16):\n",
    "\t\tlogits, loss = m(xb, yb)\n",
    "\toptimizer.zero_grad(set_to_none=True)\n",
    "\tloss.backward()\n",
    "\ttorch.nn.utils.clip_grad_norm_(m.parameters(), 1.0)\n",
    "\topt_step()\n",
    "\n",
    "\tif step % 10 != 0: \n",
    "\t\tstep += 1\n",
    "\t\tcontinue\n",
    "\n",
    "\trow = {\"step\": step, \"train_loss\": loss.item()}\n",
    "\tif step % 50 == 0:\n",
    "\t\trow[\"val_loss\"] = estimate_val_loss(m)\n",
    "\t\tscheduler.step(row[\"val_loss\"])\n",
    "\t\tprint(step, row.get(\"train_loss\"), row.get(\"val_loss\"))\n",
    "\tmetrics.append(row)\n",
    "\t\n",
    "\t# if step % 1000 == 0:\n",
    "\t# \tcheckpoint.save(\n",
    "\t# \t\tCKPT_DIR / f\"step-{step}\",\n",
    "\t# \t\tm, config, TRAINING_CONFIG, metrics, optimizer=optimizer, scheduler=scheduler\n",
    "\t# \t)\n",
    "\t# \tprint(f\"saved /checkpoints/step-{step}\")\n",
    "\n",
    "\tstep += 1\n",
    "\n",
    "# checkpoint.save(\n",
    "# \tCKPT_DIR / \"final\", \n",
    "# \tm, config, TRAINING_CONFIG, \n",
    "# \tmetrics, \n",
    "# \toptimizer=optimizer, scheduler=scheduler\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de620972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 36, 126, 145, 129,  47]], device='mps:0')\n",
      "The mind  is"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sselva890@cable.comcast.com/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_dynamo/utils.py:3421: UserWarning: record_context_cpp is not support on non-linux non-x86_64 platforms (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/profiler/unwind/unwind.cpp:12.)\n",
      "  return node.target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most mammore and there was of althoughn forsuch a father has beenfarised from the familymay soothing but why they findcloghtes back upon the child grateful neurosis I sway to exching the woreus as theyage I keep after these indreams within"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(idx)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mThe mind \u001b[39m\u001b[33m\"\u001b[39m, end=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m\t\u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflush\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:59\u001b[39m, in \u001b[36m_wrap_generator.<locals>.generator_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     56\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     57\u001b[39m             \u001b[38;5;66;03m# Pass the last request to the generator and get its response\u001b[39;00m\n\u001b[32m     58\u001b[39m             \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m                 response = gen.send(request)\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# We let the exceptions raised above by the generator's `.throw` or\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# `.send` methods bubble up to our caller, except for StopIteration\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# The generator informed us that it is done: take whatever its\u001b[39;00m\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# returned value (if any) was and indicate that we're done too\u001b[39;00m\n\u001b[32m     66\u001b[39m     \u001b[38;5;66;03m# by returning it (see docs for python's return-statement).\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/toy_transformers/models/gptv3.py:201\u001b[39m, in \u001b[36mLanguageModel.generate\u001b[39m\u001b[34m(self, seed, max_new_tokens, temperature, topk)\u001b[39m\n\u001b[32m    199\u001b[39m device_type = device.type\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autocast(device_type=device_type, dtype=torch.float16):\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m   logits, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m logits = logits[:, -\u001b[32m1\u001b[39m, :].float()\n\u001b[32m    203\u001b[39m probs = F.softmax(logits / temperature, dim = -\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1771\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1769\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m   1770\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1771\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compiled_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1773\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:736\u001b[39m, in \u001b[36m_TorchDynamoContext.__call__.<locals>.compile_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    733\u001b[39m _maybe_set_eval_frame(_callback_from_stance(callback))\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    738\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config.verbose:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/toy_transformers/models/gptv3.py:157\u001b[39m, in \u001b[36mLanguageModel.forward\u001b[39m\u001b[34m(self, idx, targets)\u001b[39m\n\u001b[32m    155\u001b[39m     x = torch.utils.checkpoint.checkpoint(block, x)\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m   x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m x = \u001b[38;5;28mself\u001b[39m.ln(x)\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: x = x[:, [-\u001b[32m1\u001b[39m], :]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/nn/modules/container.py:244\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/toy_transformers/models/gptv3.py:109\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor):\n\u001b[32m    108\u001b[39m   \u001b[38;5;66;03m# residual connections\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m   attn_add = \u001b[38;5;28mself\u001b[39m.attn(\u001b[38;5;28mself\u001b[39m.norm1(x))\n\u001b[32m    110\u001b[39m   x = torch.add(x, attn_add)\n\u001b[32m    111\u001b[39m   mlp_add = \u001b[38;5;28mself\u001b[39m.mlp(\u001b[38;5;28mself\u001b[39m.norm2(x))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/toy_transformers/models/gptv3.py:109\u001b[39m, in \u001b[36mtorch_dynamo_resume_in_forward_at_109\u001b[39m\u001b[34m(___stack0, ___stack1, self, x)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor):\n\u001b[32m    108\u001b[39m   \u001b[38;5;66;03m# residual connections\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m   attn_add = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m   x = torch.add(x, attn_add)\n\u001b[32m    111\u001b[39m   mlp_add = \u001b[38;5;28mself\u001b[39m.mlp(\u001b[38;5;28mself\u001b[39m.norm2(x))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/toy_transformers/models/gptv3.py:80\u001b[39m, in \u001b[36mCausalSelfAttention.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     77\u001b[39m q, k = \u001b[38;5;28mself\u001b[39m.rope(q, k)\n\u001b[32m     78\u001b[39m v = v_joined.view(B, T, \u001b[38;5;28mself\u001b[39m.n_heads, n_head_embed).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m y = F.scaled_dot_product_attention(q, k, v, is_causal=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     81\u001b[39m y_joined = y.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).contiguous().view(B, T, \u001b[38;5;28mself\u001b[39m.n_embed)\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.proj(y_joined)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/toy_transformers/models/gptv3.py:80\u001b[39m, in \u001b[36mtorch_dynamo_resume_in_forward_at_80\u001b[39m\u001b[34m(___stack0, self, B, T)\u001b[39m\n\u001b[32m     77\u001b[39m q, k = \u001b[38;5;28mself\u001b[39m.rope(q, k)\n\u001b[32m     78\u001b[39m v = v_joined.view(B, T, \u001b[38;5;28mself\u001b[39m.n_heads, n_head_embed).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m y = F.scaled_dot_product_attention(q, k, v, is_causal=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     81\u001b[39m y_joined = y.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).contiguous().view(B, T, \u001b[38;5;28mself\u001b[39m.n_embed)\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.proj(y_joined)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:929\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    927\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m.callback))\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m929\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    931\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:1241\u001b[39m, in \u001b[36maot_module_simplified.<locals>.forward\u001b[39m\u001b[34m(*runtime_args)\u001b[39m\n\u001b[32m   1239\u001b[39m full_args.extend(params_flat)\n\u001b[32m   1240\u001b[39m full_args.extend(runtime_args)\n\u001b[32m-> \u001b[39m\u001b[32m1241\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:384\u001b[39m, in \u001b[36m_create_runtime_wrapper.<locals>.runtime_wrapper\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    382\u001b[39m         torch._C._set_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    383\u001b[39m     record_runtime_wrapper_prologue_exit(cm)\n\u001b[32m--> \u001b[39m\u001b[32m384\u001b[39m     all_outs = \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompiled_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteal_args\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    386\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    388\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m grad_enabled:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001b[39m, in \u001b[36mcall_func_at_runtime_with_args\u001b[39m\u001b[34m(f, args, steal_args, disable_amp)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(f, \u001b[33m\"\u001b[39m\u001b[33m_boxed_call\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m         out = normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    128\u001b[39m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[32m    129\u001b[39m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[32m    130\u001b[39m         warnings.warn(\n\u001b[32m    131\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt take boxed arguments. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:750\u001b[39m, in \u001b[36mEffectTokensWrapper.post_compile.<locals>.inner_fn\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    747\u001b[39m     args = [*([\u001b[38;5;28;01mNone\u001b[39;00m] * num_tokens), *args]\n\u001b[32m    748\u001b[39m     old_args.clear()\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m outs = \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[38;5;66;03m# Inductor cache DummyModule can return None\u001b[39;00m\n\u001b[32m    753\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m outs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:556\u001b[39m, in \u001b[36mFunctionalizedRngRuntimeWrapper.post_compile.<locals>.wrapper\u001b[39m\u001b[34m(runtime_args)\u001b[39m\n\u001b[32m    549\u001b[39m     out = \u001b[38;5;28mself\u001b[39m._functionalized_rng_runtime_epilogue(\n\u001b[32m    550\u001b[39m         runtime_metadata,\n\u001b[32m    551\u001b[39m         out,\n\u001b[32m    552\u001b[39m         \u001b[38;5;66;03m# TODO: this won't be right for the backward when we convert the call_compiled_backward to use the wrapper\u001b[39;00m\n\u001b[32m    553\u001b[39m         runtime_metadata.num_forward_returns,\n\u001b[32m    554\u001b[39m     )\n\u001b[32m    555\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mruntime_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_inductor/output_code.py:584\u001b[39m, in \u001b[36mCompiledFxGraph.__call__\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_callable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    583\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcurrent_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    586\u001b[39m     get_runtime_metrics_context().finish()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_inductor/utils.py:2716\u001b[39m, in \u001b[36malign_inputs_from_check_idxs.<locals>.run\u001b[39m\u001b[34m(new_inputs)\u001b[39m\n\u001b[32m   2712\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(new_inputs: \u001b[38;5;28mlist\u001b[39m[InputType]) -> Any:\n\u001b[32m   2713\u001b[39m     old_tensors, new_tensors = copy_misaligned_inputs(\n\u001b[32m   2714\u001b[39m         new_inputs, inputs_to_check, mutated_input_idxs\n\u001b[32m   2715\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2716\u001b[39m     out = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2718\u001b[39m     \u001b[38;5;66;03m# If a mutated tensor was cloned to be aligned, we need to reflect back the mutation to the\u001b[39;00m\n\u001b[32m   2719\u001b[39m     \u001b[38;5;66;03m# original tensor.\u001b[39;00m\n\u001b[32m   2720\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(old_tensors):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/folders/2k/7nzhgb9s67b36s310p79c0_00000gp/T/torchinductor_sselva890@cable.comcast.com/eu/ceuzlcpq6drfgvyduuouuxkbxnxlpwv24uaaq77m5mgscsikeols.py:98\u001b[39m, in \u001b[36mcall\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m     96\u001b[39m buf3 = empty_strided((s52, \u001b[32m288\u001b[39m), (\u001b[32m288\u001b[39m, \u001b[32m1\u001b[39m), device=\u001b[33m'\u001b[39m\u001b[33mmps\u001b[39m\u001b[33m'\u001b[39m, dtype=torch.float16)\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# Topologically Sorted Source Nodes: [linear], Original ATen: [aten._to_copy, aten.addmm]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[43mextern_kernels\u001b[49m\u001b[43m.\u001b[49m\u001b[43maddmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreinterpret_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43ms52\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m288\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m288\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreinterpret_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m288\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m288\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m288\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuf3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (reinterpret_tensor(buf3, (\u001b[32m1\u001b[39m, s52, \u001b[32m288\u001b[39m), (\u001b[32m288\u001b[39m*s52, \u001b[32m288\u001b[39m, \u001b[32m1\u001b[39m), \u001b[32m0\u001b[39m), )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "idx = torch.tensor([vocab.encode(\"The mind \")], dtype=torch.long, device=DEVICE)\n",
    "print(idx)\n",
    "print(\"The mind \", end=\"\", flush=True)\n",
    "for token in m.generate(idx, max_new_tokens=200):\n",
    "\tprint(vocab.decode([token.item()])[0], end=\"\", flush=True)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
