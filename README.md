## model additions
- [X] `gpt-v3`: refined "baseline" model
  - [X] RMSNorm
  - [X] remove dropout
  - [X] RoPE
  - [X] ReLU^2 activation
  - [X] ~~QK-Norm~~ (not added, unstable at this size)
  - [X] logit soft-capping
- [ ] `gpt-v4`: better adaptation of modern practices (maybe?)
  - [ ] add GQA
  - [ ] remove logit soft-capping (not normal)
  - [ ] remove bias in linear layers
- [ ] `gqa`: Grouped Query Attention
  - simple, mostly to compare with MLA
  - note: add to final gptv
- [ ] `mla`: Multi-head Latent Attention (Deepseek)
  - might need to remove RoPE? implementation is majorly different
- [ ] `moe`: Mixture-of-Experts
  - may need secondary loss to make sure routing is handled well
- [ ] `mtp`: Multi-Token Prediction
- [ ] `swa`: Sliding Window Attention
  - mostly tocompare with nanoGPT speedrun
- [ ] `swiglu`: SwiGLU activation
- [ ] `gelu` GELU activation
- [ ] Add LoRA to token embedding OR head
- [ ] Muon (maybe)
## repo changes
- [ ] improved tokenization algorithms
  - [X] *use merge rank lookup instead of Trie
  - [X] add regular checking / clearing out when 0ed items (Counter, pair_to_words)
  - [ ] add streaming abilities (run on fineweb-edu sample-10b)
  - [ ] add sharding of training data
- [X] add support for cloud compute
  - [X] able to download data (Colab / ssh-like)
  - [X] able to persist work (Colab / ssh-like)
