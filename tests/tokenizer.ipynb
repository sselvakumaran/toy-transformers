{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "552dd715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('../src')))\n",
    "\n",
    "from utilities import tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0349ccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../data/freud/interpretation-of-dreams.txt\"\n",
    "input_file = open(filepath, 'r', encoding='utf-8')\n",
    "raw_text = input_file.read()\n",
    "input_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dcc1ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import text_cleaning\n",
    "text = text_cleaning.basic_cleaning(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "415a5eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding: e , th, s , re, in, t , the , on, d , at, an, y , , , is, f , of, r , o , or, it, io, al, am, as, co, rea, ic, . , ve, in , ing, on , dr, wh, ing , the, de, be, ur, us, and , ati, pr, f the , di, wi, s a, am , tha, pe, ich, whic, that , s o, ich , ut, I , ul, un, thi, we, wa, res, po, al , pa, in the , s i, Th, s th, all, an , pl, , an, int, ke, ation , with, ug, bl, ate, ons, ugh, this , by, . T, , w, with , \n",
      "| |!|&|(|)|,|, |, an|, w|-|.|. |. T|0|1|2|3|4|5|6|7|8|9|:|;|=|?|A|B|C|D|E|F|G|H|I|I |J|K|L|M|N|O|P|Q|R|S|T|Th|U|V|W|X|Y|Z|[|]|_|a|al|al |all|am|am |an|an |and |as|at|ate|ati|ation |b|be|bl|by|c|co|d|d |de|di|dr|e|e |f|f |f the |g|h|i|ic|ich|ich |in|in |in the |ing|ing |int|io|is|it|j|k|ke|l|m|n|o|o |of|on|on |ons|or|p|pa|pe|pl|po|pr|q|r|r |re|rea|res|s|s |s a|s i|s o|s th|t|t |th|tha|that |the|the |thi|this |u|ug|ugh|ul|un|ur|us|ut|v|ve|w|wa|we|wh|whic|wi|with|with |x|y|y |z|·|Â|Æ|Ü|à|ä|æ|ç|è|é|ê|î|ó|ô|û|ü|œ|Ψ|έ|υ|–|—|‘|’|“|”\n",
      "192\n"
     ]
    }
   ],
   "source": [
    "td = tokenizer.create_tokenizer(text, num_tokens=200, verbose=True)\n",
    "td = tokenizer.reduce_token_dictionary(td, text)\n",
    "print(\"|\".join(td.token_set))\n",
    "print(len(td.token_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18a9e93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
