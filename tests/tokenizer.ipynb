{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "552dd715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('../src')))\n",
    "\n",
    "from utilities import tokenizer2, tokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0349ccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../data/freud/interpretation-of-dreams.txt\"\n",
    "input_file = open(filepath, 'r', encoding='utf-8')\n",
    "raw_text = input_file.read()\n",
    "input_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dcc1ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import text_cleaning\n",
    "text = text_cleaning.basic_cleaning(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "415a5eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| | a| a | ac| al| an| an | and| and | ar| are | as| as | s| su| th| that| that | the| the | the s| this | though|!|&|(|)|,|, |, and|, and |, the |-|.|.\n",
      "|. |. A|. I|. T|. The |0|1|2|3|4|5|6|7|8|9|:|: |;|; |=|?|A|B|C|D|E|F|G|H|I|I |J|K|L|M|N|O|P|Q|R|S|T|U|V|W|X|Y|[|]|_|a|a |ab|able |ac|ad|ag|ain|al|al |all|am|ame |an|an |and|and |ar|are |as|as |at|at |ation|ation |ations |ay|b|be|be |bec|been |bu|by|by |c|cc|ce|ce |ch|ch |ci|cl|com|con|cons|consci|cont|d|d |de|der|di|dis|dream|dream |e|e |e of |e th|e the |e, |ear|ec|ed|ed |el|ell|em|en|en |ens|ent|ent |ep|er|er |ere|ere |es|es |ess|ess |et|ev|ever|ex|f|fo|for|for |for the |form|fr|from|from |ful|g|gh|h|ha|had |has |have |he |him|his |how|i|ic|id|ien|if|ig|igh|il|im|in|in |in the |ind|ing|ing |ing the |inter|ir|is|is |ish|it|it |its |ity |j|jec|k|k |ke |l|le|le |li|ll|ll |ly|ly |m|ma|man|may |me|mo|mor|my |n|n |ne|no|not |now|o|o |ob|of|of |of the |of the dream|og|ol|om|on|on |on the |one |ong|or|or |os|oth|other |ou|ough|oul|ould |oun|our|our |ous|ous |ow|p|pa|par|per|ph|pl|po|pos|pp|pre|pres|present|pro|psych|psychic|qu|r|ra|re|rea|ream|rec|res|ri|ro|s|s |s, |se|sel|self|si|sion|sion |st|su|sy|sych|t|t |t the |ta|ted |ten|ter|ter |th|ti|tic|tim|tion|tion |to|to |to the |tr|ts |tu|tw|u|uc|ul|un|up|ur|us|us |v|ve |ver|w|wa|was |we |wh|whi|which|which |wi|wish|with|with |wor|x|y|y |z|·|À|Â|Ü|à|ä|æ|ç|è|é|ê|î|ô|ü|œ|Ψ|ς|χ|–|—|‘|’|“|”|” \n",
      "388\n"
     ]
    }
   ],
   "source": [
    "td = tokenizer2.create_tokenizer(text, num_tokens=400)\n",
    "td = tokenizer.reduce_token_dictionary(td, text)\n",
    "print(\"|\".join(td.token_set))\n",
    "print(len(td.token_set))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
