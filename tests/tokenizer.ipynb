{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552dd715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('../src')))\n",
    "\n",
    "from utilities import tokenizer2, tokenizer_old\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0349ccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../data/freud/interpretation-of-dreams.txt\"\n",
    "input_file = open(filepath, 'r', encoding='utf-8')\n",
    "raw_text = input_file.read()\n",
    "input_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dcc1ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import text_cleaning\n",
    "text = text_cleaning.basic_cleaning(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415a5eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| | a| a | a s| ab| about | ac| acc| activ| activity | ad| af| after| ag| again| al| all| all | also | am| an| an | and| and | and the | another | any | app| appear| ar| are | as| as | as a | as the | at| at | at the | of | s| se| sh| she | su| sub| th| that| that | that the | the| the | the dream | the s| the same | their | them| them | there| there | these | they | thin| this | those | though| thought| thought | thoughts| thoughts | through| thus |!|&|(|(_|(p. |)|) |,|, |, a |, and|, and |, and the |, as |, but |, however|, however, |, that |, the |, wh|, which |,” |-|.|.\n",
      "|.\n",
      "I|.\n",
      "The |. |. A|. Bu|. But |. F|. H|. He |. I|. I |. In|. It |. It is |. M|. O|. On|. S|. T|. Th|. The |. This |. W|. We |..|.[|._|.”|.”\n",
      "|.” |0|1|2|3|4|5|6|7|8|9|:|: |: “|;|; |; the |=|?|? |A|B|Bu|C|D|E|F|G|H|I|I |I am|I am |I had |I have |I sh|I was |J|K|L|M|N|O|P|Q|R|S|T|The |U|V|W|X|Y|Z|[|]|] |_|_ |a|a |a s|ab|able |ac|ack |ad|ag|ain|ain |al|al |al s|all|all |ally |aly|alys|alysis |am|ame |an|an |anc|ance|ance |and|and |ang|ans|ant|ant |app|ar|ar |ard|are |ari|ary |as|as |ass|ast |at|at |at the |ate |ated |ation|ation |ation of |ation of the |ation, |ations|ations |au|ay|ay |b|be|be |bec|beca|because |becom|become |been |being |bel|ber|betwe|bl|ble |bo|br|bro|bu|but|but |by|by |by the |c|ca|call|can|can |cannot |cap|cas|case |cc|ce|ce |cens|cep|cer|ces |ch|ch |chang|char|charac|child|ci|cit|cl|co|com|comm|comp|compl|con|concer|condi|connec|cons|consci|conscious|conscious |consi|consider|cont|content|content |continu|contr|cor|cour|cr|cri|cus|d|d |d of |day|day |de|de |dea|dec|dem|den|der|des|di|dif|differen|diffic|direc|dis|dist|do|do |does |does not |dream|dream |dreams|dreams |duc|dur|during the |e|e |e of |e th|e that |e the |e, |ea|ear|eas|ec|ed|ed |ed by |ed by the |ed in |ed the |ed to |ed, |el|element|ell|ely|ely |em|em |ement|ement |emo|emp|ems |en|en |en the |ence |end|ens|ent|ent |enti|ently |ep|er|er |ere|ere |eri|es|es |es of |es the |es, |ese |ess|ess |est|est |et|et |ev|even |ever|every |ex|exam|excit|exper|experien|expl|explan|expres|exual |ey|f|f |fa|fac|fact|fe|feel|fer|fere|feren|ff|ffec|fi|fic|fin|find|fir|first |fl|flu|fo|follow|for|for |for the |fore|fore |form|found |fr|fre|frien|friend |from |from the |fu|ful|ful |fulfil|fur|g|ge|ge |gen|gh|giv|go|gr|grea|great |gu|h|ha|had|had |has |has been |have |he|he |her|her |here |hi|him|him |his |hoo|how|however|hy|i|ic|ical |id|id |ide|ie|ien|if|if |ig|igh|ight|ight |il|ill|im|imp|impres|impression|in|in |in a |in s|in the |in the dream|in the dream |in this |in which |inc|ind|inde|ing|ing |ing s|ing the |ing to |ing, |ings |int|inter|interpret|interpretation |into |into the |inv|ir|ir |is|is |is a |is the |is, |ise |ish|ish |ist|it|it |it is |ite |its |itself |ity|ity |iv|ive |j|jec|just|k|k |ke|ke |king |know|ks |l|la|lat|le|le |lea|lear|leep|less |li|lif|life|life |like |lit|little |ll|ll |llec|llow|lo|long|ly|ly |m|m |ma|made |main|make |man|man |manif|many |mar|mat|materi|material |may |may be |me|me |mean|med|mem|memor|men|ment|ment |might |min|mind|mis|mo|mor|more |most |mp|mu|must |my|my |n|n |nat|ne|nec|ner|ness|ness |nif|nific|no|no |not|not |now|now |num|o|o |ob|objec|occ|occa|occur|of|of |of a |of my |of s|of the |of the dream|of the dream |of this |often |og|ogn|ol|old|old |olog|om|om |ome|ome |omething |on|on |on the |once |one|one |ong|ong |only|only |oo|op|opp|or|or |ord|org|origin|ory |os|ose |ost |oth|other|other |ou|ough|oul|ould |oun|ound|ound |our|our |ous|ous |out|out |out the |over|ow|ow |own|own |p|p. |pa|pain|par|part |pati|patient|pe|pec|per|percep|pers|person|person |ph|pic|pictu|pl|plac|place |po|poin|pon|por|pos|posi|possible |pp|pr|pre|pres|present|press|pro|prob|proc|process|produc|ps |psych|psychic|psychic |psycholog|pu|pur|purpos|qu|ques|r|ra|re|rea|read|ready |real|ream|reas|rec|recogn|red|red |reg|rel|remain|ren|represent|res|respon|resul|ri|ro|rough|s|s |s and |s of |s of the |s the |s to |s, |s, and |sa|same |sc|se|se |secon|sel|self|self |sens|ser|serv|sh|she |show|si|sible |sig|sion|sion |so|so |soci|st|st |sti|str|struc|su|sub|subjec|sur|sy|sych|t|t |t of |t the |t to |ta|tain|tain |tak|take |taken |tat|ted|ted |ten|ten |ter|ter |termin|th|th |the |ther|ther |thing |thor|though|ti|tic|tic |ties |til|tim|time |timul|timuli|ting |tion|tion |tion of |tion of the |tions |tis|tiv|tive |tle |tly |to|to |to a |to be |to s|to the |too|tow|tr|trac|trans|trea|tri|ts |tu|tur|tw|twe|two |u|ual |uc|ud|ue |ul|ult|um|un|unc|unconscious |und|under|up|up |upon |ur|ure|ure |us|us |use |ust|ust |v|val|ve|ve |vel|ver|ver |vers|very |ves |vi|vid|vis|w|w |wa|wak|waken|was |way|way |we |we are |we have |well|were |wh|what |when |whi|which|which |which are |which is |which the |while |who|who |wi|will |wish|wish |wish-|with|with |with the |without |wor|word|would |x|y|y |y the |y, |year|you|young|z|·|É|Ü|à|ä|æ|ç|è|é|ê|ó|ô|û|ü|Œ|œ|έ|υ|–|—|‘|’|’s |“|”|” \n",
      "1013\n"
     ]
    }
   ],
   "source": [
    "td = tokenizer2.create_tokenizer(text, num_tokens=1032)\n",
    "td = tokenizer_old.reduce_token_dictionary(td, text)\n",
    "print(\"|\".join(td.token_set))\n",
    "print(len(td.token_set))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
