{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6afa1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTS\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from torch.amp import autocast\n",
    "\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "\n",
    "sys.path.append(os.path.join(ROOT_DIR, 'src'))\n",
    "sys.path.append(os.path.join(ROOT_DIR, 'src', 'utilities'))\n",
    "\n",
    "from toy_transformers.models import gptv2 as transformer\n",
    "from toy_transformers.utilities import tokenizer\n",
    "from toy_transformers.utilities import tokenization\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ca0317c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.212m\n"
     ]
    }
   ],
   "source": [
    "## CONFIGS\n",
    "TRAIN_BIN = os.path.join(ROOT_DIR, \"data/processed/simplebooks-train.bin\")\n",
    "VAL_BIN = os.path.join(ROOT_DIR, \"data/processed/simplebooks-val.bin\")\n",
    "VOCAB_JSON = os.path.join(ROOT_DIR, \"artifacts/simplebooks_4097.json\")\n",
    "\n",
    "td = tokenization.read_tokenization(VOCAB_JSON)\n",
    "\n",
    "vocab_size = len(td.token_set)\n",
    "device=\"mps\"\n",
    "\n",
    "config = transformer.GPTv2Config(\n",
    "\tvocab_size=vocab_size,\n",
    "\tdevice=device,\n",
    "\tbatch_size = 256\n",
    ")\n",
    "m = transformer.LanguageModel(config)\n",
    "print(m.get_num_parameters(as_str=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d529b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD DATA\n",
    "train_data: torch.Tensor = torch.load(TRAIN_BIN, map_location='cpu')\n",
    "val_data: torch.Tensor = torch.load(VAL_BIN, map_location='cpu')\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "\tdef __init__(self, data_tensor: torch.Tensor, block_size: int):\n",
    "\t\tself.data = data_tensor\n",
    "\t\tself.block_size = block_size\n",
    "\t\tself.n_chunks = (len(data_tensor) - 1) // block_size\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.n_chunks\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tchunk = self.data[idx:idx+self.block_size+1]\n",
    "\t\tx = chunk[:-1]\n",
    "\t\ty = chunk[1:]\n",
    "\t\treturn x, y\n",
    "\n",
    "train_dataset = TextDataset(train_data, config.block_size)\n",
    "val_dataset = TextDataset(val_data, config.block_size)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "\ttrain_dataset, \n",
    "\tbatch_size=config.batch_size,\n",
    "\tshuffle=True,\n",
    "\tdrop_last=True\n",
    "#\tnum_workers=2,\n",
    "#\tpin_memory=True,\n",
    "#\tpin_memory_device=device\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "\tval_dataset, \n",
    "\tbatch_size=config.batch_size,\n",
    "\tshuffle=True,\n",
    "\tdrop_last=True\n",
    "#\tnum_workers=2,\n",
    "#\tpin_memory=True,\n",
    "#\tpin_memory_device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cb0c609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5357"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7fd7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEFINE MODEL, OPTIMIZER, ETC.\n",
    "\n",
    "# note step = number of batches to process in total / optimizer steps\n",
    "\n",
    "NUM_EPOCHS = 2\n",
    "VIRTUAL_BATCH_SIZE = 256\n",
    "GRAD_ACCUMULATION_STEPS = max(1, VIRTUAL_BATCH_SIZE // config.batch_size)\n",
    "TOTAL_STEPS = len(train_loader) * NUM_EPOCHS // GRAD_ACCUMULATION_STEPS\n",
    "WARMUP_STEPS = min(int(TOTAL_STEPS * 0.4), 2000)\n",
    "\n",
    "MAX_LR = 3e-5\n",
    "MIN_LR = MAX_LR * 0.1\n",
    "\n",
    "VAL_INTERVAL = 100\n",
    "LOG_INTERVAL = 2\n",
    "\n",
    "def make_val_loss_function(val_loader):\n",
    "\tval_iterator = iter(val_loader)\n",
    "\n",
    "\t@torch.no_grad()\n",
    "\tdef estimate_val_loss(model):\n",
    "\t\tmodel.eval()\n",
    "\t\tnonlocal val_iterator\n",
    "\t\ttry:\n",
    "\t\t\tX, Y = next(val_iterator)\n",
    "\t\texcept StopIteration:\n",
    "\t\t\tval_iterator = iter(val_loader)\n",
    "\t\t\tX, Y = next(val_iterator)\n",
    "\t\tX = X.to(device, non_blocking=True)\n",
    "\t\tY = Y.to(device, non_blocking=True)\n",
    "\n",
    "\t\t_, loss = model(X, Y)\n",
    "\t\treturn loss.item()\n",
    "\treturn estimate_val_loss\n",
    "\n",
    "estimate_val_loss = make_val_loss_function(val_loader)\n",
    "\n",
    "encode, decode = tokenizer.get_encoder(td), tokenizer.get_decoder(td)\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "m = transformer.LanguageModel(config).to(device=device)\n",
    "m.compile()\n",
    "\n",
    "optimizer = m.get_optimizer(weight_decay=0.01, lr=MAX_LR)\n",
    "warmup_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "\toptimizer, start_factor=0.01, total_iters=WARMUP_STEPS\n",
    ")\n",
    "decay_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "\toptimizer, T_max=TOTAL_STEPS - WARMUP_STEPS, eta_min=MIN_LR\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "\toptimizer, [warmup_scheduler, decay_scheduler], milestones=[WARMUP_STEPS]\n",
    ")\n",
    "\n",
    "@torch.compile(fullgraph=False)\n",
    "def opt_step():\n",
    "\toptimizer.step()\n",
    "\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9389b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9063bca53bde4fc790889a0ebc2fa06d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:  38%|###7      | 2016/5357 [00:00<?, ?step/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02020/05357] (3.00e-05) T: 4.58603\n",
      "[02030/05357] (3.00e-05) T: 4.66307\n",
      "[02040/05357] (3.00e-05) T: 4.57842\n",
      "[02050/05357] (3.00e-05) T: 4.59427 V: 4.42516\n",
      "[02060/05357] (3.00e-05) T: 4.53868\n",
      "[02070/05357] (3.00e-05) T: 4.59590\n",
      "[02080/05357] (3.00e-05) T: 4.53785\n",
      "[02090/05357] (2.99e-05) T: 4.59186\n",
      "[02100/05357] (2.99e-05) T: 4.55039 V: 4.36301\n",
      "[02110/05357] (2.99e-05) T: 4.57180\n",
      "[02120/05357] (2.99e-05) T: 4.56467\n",
      "[02130/05357] (2.99e-05) T: 4.52510\n",
      "[02140/05357] (2.99e-05) T: 4.49567\n",
      "[02150/05357] (2.99e-05) T: 4.53179 V: 4.32782\n",
      "[02160/05357] (2.98e-05) T: 4.56876\n",
      "[02170/05357] (2.98e-05) T: 4.55194\n",
      "[02180/05357] (2.98e-05) T: 4.48882\n",
      "[02190/05357] (2.98e-05) T: 4.53559\n",
      "[02200/05357] (2.98e-05) T: 4.52208 V: 4.32778\n",
      "[02210/05357] (2.97e-05) T: 4.54362\n",
      "[02220/05357] (2.97e-05) T: 4.48990\n",
      "[02230/05357] (2.97e-05) T: 4.44832\n",
      "[02240/05357] (2.96e-05) T: 4.47610\n",
      "[02250/05357] (2.96e-05) T: 4.46781 V: 4.32290\n",
      "[02260/05357] (2.96e-05) T: 4.47944\n",
      "[02270/05357] (2.96e-05) T: 4.47015\n",
      "[02280/05357] (2.95e-05) T: 4.50928\n",
      "[02290/05357] (2.95e-05) T: 4.41714\n",
      "[02300/05357] (2.95e-05) T: 4.47903 V: 4.31920\n",
      "[02310/05357] (2.94e-05) T: 4.46409\n",
      "[02320/05357] (2.94e-05) T: 4.45633\n",
      "[02330/05357] (2.93e-05) T: 4.43849\n",
      "[02340/05357] (2.93e-05) T: 4.45185\n",
      "[02350/05357] (2.93e-05) T: 4.45649 V: 4.27901\n",
      "[02360/05357] (2.92e-05) T: 4.45354\n",
      "[02370/05357] (2.92e-05) T: 4.42117\n",
      "[02380/05357] (2.91e-05) T: 4.45987\n",
      "[02390/05357] (2.91e-05) T: 4.44261\n",
      "[02400/05357] (2.90e-05) T: 4.39177 V: 4.34381\n",
      "[02410/05357] (2.90e-05) T: 4.41053\n",
      "[02420/05357] (2.90e-05) T: 4.40800\n",
      "[02430/05357] (2.89e-05) T: 4.40131\n",
      "[02440/05357] (2.89e-05) T: 4.38067\n",
      "training interrupted\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\tpbar = tqdm(\n",
    "\t\ttotal=TOTAL_STEPS,\n",
    "\t\tinitial=step,\n",
    "\t\tdesc=\"training\",\n",
    "\t\tunit=\"step\",\n",
    "\t\tposition=0,\n",
    "\t\tleave=True\n",
    "\t)\n",
    "\n",
    "\tfor epoch in range(NUM_EPOCHS):\n",
    "\t\t# pbar.write(f\"--- starting epoch {epoch + 1}/{NUM_EPOCHS} ---\")\n",
    "\n",
    "\t\tfor virtual_batch_i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "\t\t\tx_batch = x_batch.to(device, non_blocking=True)\n",
    "\t\t\ty_batch = y_batch.to(device, non_blocking=True)\n",
    "\n",
    "\t\t\tm.train()\n",
    "\t\t\twith autocast(device_type=device, dtype=torch.float16):\n",
    "\t\t\t\tlogits, loss = m(x_batch, y_batch)\n",
    "\t\t\t\tloss /= GRAD_ACCUMULATION_STEPS\n",
    "\t\t\t\n",
    "\t\t\tloss.backward()\n",
    "\n",
    "\t\t\tif (virtual_batch_i + 1) % GRAD_ACCUMULATION_STEPS == 0:\n",
    "\t\t\t\ttorch.nn.utils.clip_grad_norm_(m.parameters(), 1.0)\n",
    "\t\t\t\topt_step()\n",
    "\t\t\t\toptimizer.zero_grad(set_to_none=True)\n",
    "\t\t\t\tscheduler.step()\n",
    "\t\t\t\ttrain_loss = loss.item() * GRAD_ACCUMULATION_STEPS\n",
    "\t\t\t\tpbar.set_postfix(train_loss=f\"{train_loss:01.05f}\")\n",
    "\n",
    "\t\t\t\tif step % VAL_INTERVAL == 0:\n",
    "\t\t\t\t\tlr = scheduler.get_last_lr()[0]\n",
    "\t\t\t\t\tval_loss = estimate_val_loss(m)\n",
    "\t\t\t\t\tpbar.write(f\"[{step:05d}/{TOTAL_STEPS:05d}] ({lr:.2e}) T: {train_loss:01.05f} V: {val_loss:01.05f}\")\n",
    "\t\t\t\telif step % LOG_INTERVAL == 0:\n",
    "\t\t\t\t\tlr = scheduler.get_last_lr()[0]\n",
    "\t\t\t\t\tpbar.write(f\"[{step:05d}/{TOTAL_STEPS:05d}] ({lr:.2e}) T: {train_loss:01.05f}\")\n",
    "\n",
    "\t\t\t\tstep += 1\n",
    "\t\t\t\tpbar.update(1)\n",
    "\t\t\t\t\n",
    "\t\t\t\tif step >= TOTAL_STEPS: break\n",
    "\t\tif step >= TOTAL_STEPS: break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "\tpbar.write(\"training interrupted\")\n",
    "finally:\n",
    "\tpbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113c3ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sselva890@cable.comcast.com/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_dynamo/utils.py:3421: UserWarning: record_context_cpp is not support on non-linux non-x86_64 platforms (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/profiler/unwind/unwind.cpp:12.)\n",
      "  return node.target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", gaught shovel braon the young remvedifiith young in where looking upon and had foot to left rested board thatmlyigners room .\n",
      "\n",
      "Fourcending zion places declared .\n",
      "\n",
      "\" Here , directed the"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m idx = torch.tensor([encode(SEED)], dtype=torch.long, device=device)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(SEED, end=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m400\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m\t\u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m\t\u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflush\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:59\u001b[39m, in \u001b[36m_wrap_generator.<locals>.generator_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     56\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     57\u001b[39m             \u001b[38;5;66;03m# Pass the last request to the generator and get its response\u001b[39;00m\n\u001b[32m     58\u001b[39m             \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m                 response = gen.send(request)\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# We let the exceptions raised above by the generator's `.throw` or\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# `.send` methods bubble up to our caller, except for StopIteration\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# The generator informed us that it is done: take whatever its\u001b[39;00m\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# returned value (if any) was and indicate that we're done too\u001b[39;00m\n\u001b[32m     66\u001b[39m     \u001b[38;5;66;03m# by returning it (see docs for python's return-statement).\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/src/models/gptv2.py:169\u001b[39m, in \u001b[36mLanguageModel.generate\u001b[39m\u001b[34m(self, seed, max_new_tokens, temperature, topk)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m    168\u001b[39m \t\u001b[38;5;28;01mwith\u001b[39;00m torch.autocast(device_type=device, dtype=torch.bfloat16):\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \t\tlogits, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m \tlogits = logits[:, -\u001b[32m1\u001b[39m, :]\n\u001b[32m    171\u001b[39m \tprobs = F.softmax(logits / temperature, dim = -\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1771\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1769\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m   1770\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1771\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compiled_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1773\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:736\u001b[39m, in \u001b[36m_TorchDynamoContext.__call__.<locals>.compile_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    733\u001b[39m _maybe_set_eval_frame(_callback_from_stance(callback))\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    738\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config.verbose:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/src/models/gptv2.py:127\u001b[39m, in \u001b[36mLanguageModel.forward\u001b[39m\u001b[34m(self, idx, targets)\u001b[39m\n\u001b[32m    125\u001b[39m \t\tx = torch.utils.checkpoint.checkpoint(block, x)\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \tx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m x = \u001b[38;5;28mself\u001b[39m.ln(x)\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/nn/modules/container.py:244\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/src/models/gptv2.py:80\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor):\n\u001b[32m     79\u001b[39m \t\u001b[38;5;66;03m# residual connections\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \tattn_add = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \tx = torch.add(x, attn_add)\n\u001b[32m     82\u001b[39m \tmlp_add = \u001b[38;5;28mself\u001b[39m.mlp(\u001b[38;5;28mself\u001b[39m.norm2(x))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/src/models/gptv2.py:47\u001b[39m, in \u001b[36mCausalSelfAttention.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     45\u001b[39m k = k_joined.view(B, T, \u001b[38;5;28mself\u001b[39m.n_heads, n_head_embed).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m     46\u001b[39m v = v_joined.view(B, T, \u001b[38;5;28mself\u001b[39m.n_heads, n_head_embed).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m y = F.scaled_dot_product_attention(q, k, v, dropout_p=\u001b[38;5;28mself\u001b[39m.dropout \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0.0\u001b[39m, is_causal=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     48\u001b[39m y_joined = y.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).contiguous().view(B, T, \u001b[38;5;28mself\u001b[39m.n_embed)\n\u001b[32m     49\u001b[39m y_proj = \u001b[38;5;28mself\u001b[39m.proj_dp(\u001b[38;5;28mself\u001b[39m.proj(y_joined))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/src/models/gptv2.py:47\u001b[39m, in \u001b[36mtorch_dynamo_resume_in_forward_at_47\u001b[39m\u001b[34m(___stack0, self, B, T)\u001b[39m\n\u001b[32m     45\u001b[39m k = k_joined.view(B, T, \u001b[38;5;28mself\u001b[39m.n_heads, n_head_embed).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m     46\u001b[39m v = v_joined.view(B, T, \u001b[38;5;28mself\u001b[39m.n_heads, n_head_embed).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m y = F.scaled_dot_product_attention(q, k, v, dropout_p=\u001b[38;5;28mself\u001b[39m.dropout \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0.0\u001b[39m, is_causal=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     48\u001b[39m y_joined = y.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).contiguous().view(B, T, \u001b[38;5;28mself\u001b[39m.n_embed)\n\u001b[32m     49\u001b[39m y_proj = \u001b[38;5;28mself\u001b[39m.proj_dp(\u001b[38;5;28mself\u001b[39m.proj(y_joined))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:929\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    927\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m.callback))\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m929\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    931\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:1241\u001b[39m, in \u001b[36maot_module_simplified.<locals>.forward\u001b[39m\u001b[34m(*runtime_args)\u001b[39m\n\u001b[32m   1239\u001b[39m full_args.extend(params_flat)\n\u001b[32m   1240\u001b[39m full_args.extend(runtime_args)\n\u001b[32m-> \u001b[39m\u001b[32m1241\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:384\u001b[39m, in \u001b[36m_create_runtime_wrapper.<locals>.runtime_wrapper\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    382\u001b[39m         torch._C._set_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    383\u001b[39m     record_runtime_wrapper_prologue_exit(cm)\n\u001b[32m--> \u001b[39m\u001b[32m384\u001b[39m     all_outs = \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompiled_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteal_args\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    386\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    388\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m grad_enabled:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001b[39m, in \u001b[36mcall_func_at_runtime_with_args\u001b[39m\u001b[34m(f, args, steal_args, disable_amp)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(f, \u001b[33m\"\u001b[39m\u001b[33m_boxed_call\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m         out = normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    128\u001b[39m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[32m    129\u001b[39m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[32m    130\u001b[39m         warnings.warn(\n\u001b[32m    131\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt take boxed arguments. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:556\u001b[39m, in \u001b[36mFunctionalizedRngRuntimeWrapper.post_compile.<locals>.wrapper\u001b[39m\u001b[34m(runtime_args)\u001b[39m\n\u001b[32m    549\u001b[39m     out = \u001b[38;5;28mself\u001b[39m._functionalized_rng_runtime_epilogue(\n\u001b[32m    550\u001b[39m         runtime_metadata,\n\u001b[32m    551\u001b[39m         out,\n\u001b[32m    552\u001b[39m         \u001b[38;5;66;03m# TODO: this won't be right for the backward when we convert the call_compiled_backward to use the wrapper\u001b[39;00m\n\u001b[32m    553\u001b[39m         runtime_metadata.num_forward_returns,\n\u001b[32m    554\u001b[39m     )\n\u001b[32m    555\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mruntime_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_inductor/output_code.py:584\u001b[39m, in \u001b[36mCompiledFxGraph.__call__\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_callable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    583\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcurrent_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    586\u001b[39m     get_runtime_metrics_context().finish()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/apps/toy-transformers/venv/lib/python3.11/site-packages/torch/_inductor/utils.py:2716\u001b[39m, in \u001b[36malign_inputs_from_check_idxs.<locals>.run\u001b[39m\u001b[34m(new_inputs)\u001b[39m\n\u001b[32m   2712\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(new_inputs: \u001b[38;5;28mlist\u001b[39m[InputType]) -> Any:\n\u001b[32m   2713\u001b[39m     old_tensors, new_tensors = copy_misaligned_inputs(\n\u001b[32m   2714\u001b[39m         new_inputs, inputs_to_check, mutated_input_idxs\n\u001b[32m   2715\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2716\u001b[39m     out = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2718\u001b[39m     \u001b[38;5;66;03m# If a mutated tensor was cloned to be aligned, we need to reflect back the mutation to the\u001b[39;00m\n\u001b[32m   2719\u001b[39m     \u001b[38;5;66;03m# original tensor.\u001b[39;00m\n\u001b[32m   2720\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(old_tensors):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/folders/2k/7nzhgb9s67b36s310p79c0_00000gp/T/torchinductor_sselva890@cable.comcast.com/ws/cwsayfhzm2fcyywwjgv6tul7ipni5tsrfk3tkp5rbjzpizhaoaxd.py:100\u001b[39m, in \u001b[36mcall\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m     98\u001b[39m buf3 = empty_strided((s52, \u001b[32m288\u001b[39m), (\u001b[32m288\u001b[39m, \u001b[32m1\u001b[39m), device=\u001b[33m'\u001b[39m\u001b[33mmps\u001b[39m\u001b[33m'\u001b[39m, dtype=torch.bfloat16)\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# Topologically Sorted Source Nodes: [linear], Original ATen: [aten._to_copy, aten.addmm]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m \u001b[43mextern_kernels\u001b[49m\u001b[43m.\u001b[49m\u001b[43maddmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreinterpret_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43ms52\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m288\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m288\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreinterpret_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m288\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m288\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m288\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuf3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (reinterpret_tensor(buf3, (\u001b[32m1\u001b[39m, s52, \u001b[32m288\u001b[39m), (\u001b[32m288\u001b[39m*s52, \u001b[32m288\u001b[39m, \u001b[32m1\u001b[39m), \u001b[32m0\u001b[39m), )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "SEED = \"The \"\n",
    "idx = torch.tensor([encode(SEED)], dtype=torch.long, device=device)\n",
    "print(SEED, end=\"\", flush=True)\n",
    "for token in m.generate(idx, max_new_tokens=400):\n",
    "\tv = token.item()\n",
    "\tprint(decode([v])[0], end=\"\", flush=True)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
